"""
Train Qwen2.5-7B model on oracle labels using Unsloth.

This script fine-tunes Qwen2.5-7B-Instruct on tech deployment oracle labels
with optimizations for class imbalance.
"""

import json
import torch
from pathlib import Path
from datasets import Dataset
from transformers import TrainingArguments
from unsloth import FastLanguageModel
from trl import SFTTrainer
import argparse


def load_training_data(train_file: Path, val_file: Path):
    """Load training and validation data from JSONL files."""
    def load_jsonl(file_path):
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        return data

    train_data = load_jsonl(train_file)
    val_data = load_jsonl(val_file)

    return train_data, val_data


def format_prompt_completion(example):
    """Format example as single text for SFTTrainer."""
    # Unsloth SFTTrainer expects a single text field with prompt + completion
    text = f"{example['prompt']}\n\nASSISTANT: {example['completion']}"
    return {'text': text}


def main():
    parser = argparse.ArgumentParser(description='Train Qwen2.5-7B on tech deployment labels')
    parser.add_argument('--train-file', required=True, help='Training data JSONL file')
    parser.add_argument('--val-file', required=True, help='Validation data JSONL file')
    parser.add_argument('--output-dir', required=True, help='Output directory for model')
    parser.add_argument('--max-seq-length', type=int, default=2048, help='Maximum sequence length')
    parser.add_argument('--lora-rank', type=int, default=16, help='LoRA rank')
    parser.add_argument('--batch-size', type=int, default=4, help='Per-device batch size')
    parser.add_argument('--gradient-accumulation', type=int, default=4, help='Gradient accumulation steps')
    parser.add_argument('--learning-rate', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')
    parser.add_argument('--warmup-steps', type=int, default=100, help='Warmup steps')
    parser.add_argument('--eval-steps', type=int, default=200, help='Evaluation frequency')
    parser.add_argument('--save-steps', type=int, default=200, help='Save checkpoint frequency')

    args = parser.parse_args()

    print('='*60)
    print('TECH DEPLOYMENT MODEL TRAINING')
    print('='*60)

    # Load data
    print(f'\n1. Loading training data...')
    print(f'   Train: {args.train_file}')
    print(f'   Val:   {args.val_file}')

    train_data, val_data = load_training_data(
        Path(args.train_file),
        Path(args.val_file)
    )

    print(f'   Loaded {len(train_data):,} train examples')
    print(f'   Loaded {len(val_data):,} val examples')

    # Convert to HuggingFace Dataset
    print(f'\n2. Converting to HuggingFace Dataset format...')
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)

    # Format for SFTTrainer
    train_dataset = train_dataset.map(format_prompt_completion)
    val_dataset = val_dataset.map(format_prompt_completion)

    # Load model
    print(f'\n3. Loading Qwen2.5-7B-Instruct with 4-bit quantization...')
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Qwen2.5-7B-Instruct",
        max_seq_length=args.max_seq_length,
        dtype=None,  # Auto-detect optimal dtype
        load_in_4bit=True  # 4-bit quantization for memory efficiency
    )

    print(f'   Model loaded: {model.config._name_or_path}')
    print(f'   Max seq length: {args.max_seq_length}')

    # Add LoRA adapters
    print(f'\n4. Adding LoRA adapters (rank={args.lora_rank})...')
    model = FastLanguageModel.get_peft_model(
        model,
        r=args.lora_rank,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
        lora_alpha=args.lora_rank,
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing=True,
        random_state=42,
    )

    # Training arguments
    print(f'\n5. Configuring training...')
    effective_batch_size = args.batch_size * args.gradient_accumulation
    print(f'   Batch size: {args.batch_size} (effective: {effective_batch_size})')
    print(f'   Learning rate: {args.learning_rate}')
    print(f'   Epochs: {args.epochs}')
    print(f'   Warmup steps: {args.warmup_steps}')

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        logging_steps=50,
        eval_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        optim="adamw_8bit",
        weight_decay=0.01,
        max_grad_norm=1.0,
        lr_scheduler_type="cosine",
        seed=42,
        report_to="none",  # Disable wandb/tensorboard
    )

    # Create trainer
    print(f'\n6. Initializing SFTTrainer...')
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        args=training_args,
        max_seq_length=args.max_seq_length,
        dataset_text_field="text",
        packing=False,  # Don't pack multiple examples together
    )

    # Train
    print(f'\n7. Starting training...')
    print('='*60)
    trainer.train()

    # Save final model
    print(f'\n8. Saving final model...')
    final_model_dir = Path(args.output_dir) / 'final'
    model.save_pretrained(final_model_dir)
    tokenizer.save_pretrained(final_model_dir)

    print(f'\n{"="*60}')
    print('TRAINING COMPLETE!')
    print(f'{"="*60}')
    print(f'\nModel saved to: {final_model_dir}')
    print(f'\nNext steps:')
    print(f'  1. Evaluate model on validation set')
    print(f'  2. Run inference on full dataset')
    print(f'  3. Compare distilled model vs oracle')


if __name__ == '__main__':
    main()
