# Training configuration for Uplifting Content Filter v1.0
# Based on filters/uplifting/v1/config.yaml

filter:
  path: "filters/uplifting/v1"
  name: "uplifting"
  version: "1.0"

model:
  name: "Qwen/Qwen2.5-7B"
  # Alternative smaller models for testing:
  # - "Qwen/Qwen2.5-1.5B" (faster, less accurate)
  # - "Qwen/Qwen2.5-3B" (balanced)
  max_length: 512

training:
  epochs: 3
  batch_size: 8
  learning_rate: 2.0e-5
  warmup_steps: 500
  gradient_accumulation_steps: 1

  # Data split ratios
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  seed: 42

optimization:
  optimizer: "adamw"
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler: "linear_warmup"

# Expected performance targets (from filter config)
targets:
  accuracy_vs_oracle: "90-95%"
  inference_time: "20-50ms"
  mae_threshold: 1.0  # Maximum acceptable MAE per dimension

# Hardware requirements
hardware:
  min_gpu_memory: "16GB"  # For 7B model
  recommended_gpu: "RTX 4090, A100, or similar"
  cpu_fallback: true  # Allow CPU training (very slow)

# Output paths (relative to project root)
paths:
  data_dir: "datasets/uplifting_ground_truth_v1_splits"
  output_dir: "inference/deployed/uplifting_v1"
  checkpoints_dir: "training/checkpoints/uplifting_v1"
