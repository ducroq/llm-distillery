{"id":"science_arxiv_cs_e71cc4ec0e77","title":"Compositional Zero","content":"arXiv:2510.11106v1 Announce Type: new Abstract: Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision that enables models to recognize unseen combinations of known attributes and objects during inference, addressing the combinatorial challenge of requiring training data for every possible composition. This is particularly challenging because the visual appearance of primitives is highly contextual; for example, ``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars differ significantly from ``wet'' cats. Effectively modeling this contextuality and the inherent compositionality is crucial for robust compositional zero-shot recognition. This paper presents, to our knowledge, the first comprehensive survey specifically focused on Compositional Zero-Shot Learning. We systematically review the state-of-the-art CZSL methods, introducing a taxonomy grounded in disentanglement, with four families of approaches: no explicit disentanglement, textual disentanglement, visual disentanglement, and cross-modal disentanglement. We provide a detailed comparative analysis of these methods, highlighting their core advantages and limitations in different problem settings, such as closed-world and open-world CZSL. Finally, we identify the most significant open challenges and outline promising future research directions. This survey aims to serve as a foundational resource to guide and inspire further advancements in this fascinating and important field. Papers studied in this survey with their official code are available on our github: https://github.com/ans92/Compositional-Zero-Shot-Learning","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11106","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.878563","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan","raw_content_length":1646,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["Compositional Zero-Shot Learning"],"persons":[],"locations":[],"monetary":[]},"char_count":1645,"language_detected":"en","key_concepts":{"key_phrases":["Compositional Zero","arXiv251011106v1 Announce Type","new Abstract","Compositional Zero-Shot Learning","CZSL","a critical task","computer vision","models","unseen combinations","known attributes"],"filter_categories":{"ai_ml":["computer vision"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Compositional Zero":2.0,"arXiv251011106v1 Announce Type":1.0,"new Abstract":1.0,"Compositional Zero-Shot Learning":1.0,"CZSL":1.0,"a critical task":1.0,"computer vision":1.0,"models":1.0,"unseen combinations":1.0,"known attributes":1.0}},"age_hours":2.7564817275,"is_recent":true,"quality_score":1.0,"sentiment_score":6.1235,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2247,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8237,"joy":0.0075,"surprise":0.0839,"sadness":0.0062,"fear":0.0347,"anger":0.0179,"disgust":0.0261},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper is a survey of compositional zero-shot learning methods, which aims to improve AI's ability to recognize unseen combinations of attributes and objects. While this could potentially contribute to sustainability by improving the efficiency of resource management or environmental monitoring in the future, it is currently in the basic research stage with no concrete deployments or measurable outcomes. The technical credibility is relatively high due to the peer-reviewed nature of the survey and the availability of code on GitHub.","key_impact_metrics":[],"technology_tags":["computer vision","zero-shot learning","artificial intelligence"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:51:08.960995Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_75ea7b5dac56","title":"A Vision for Access Control in LLM","content":"arXiv:2510.11108v1 Announce Type: new Abstract: The autonomy and contextual complexity of LLM-based agents render traditional access control (AC) mechanisms insufficient. Static, rule-based systems designed for predictable environments are fundamentally ill-equipped to manage the dynamic information flows inherent in agentic interactions. This position paper argues for a paradigm shift from binary access control to a more sophisticated model of information governance, positing that the core challenge is not merely about permission, but about governing the flow of information. We introduce Agent Access Control (AAC), a novel framework that reframes AC as a dynamic, context-aware process of information flow governance. AAC operates on two core modules: (1) multi-dimensional contextual evaluation, which assesses not just identity but also relationships, scenarios, and norms; and (2) adaptive response formulation, which moves beyond simple allow/deny decisions to shape information through redaction, summarization, and paraphrasing. This vision, powered by a dedicated AC reasoning engine, aims to bridge the gap between human-like nuanced judgment and scalable Al safety, proposing a new conceptual lens for future research in trustworthy agent design.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11108","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.879334","language":"en","tags":["computer-science","csma","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":172,"author":"Xinfeng Li, Dong Huang, Jie Li, Hongyi Cai, Zhenhong Zhou, Wei Dong, XiaoFeng Wang, Yang Liu","raw_content_length":1265,"priority":7,"update_frequency":1,"reading_time_minutes":0.86,"robust_parsing_used":true,"entities":{"organizations":["AAC","Vision for Access Control in LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1264,"language_detected":"en","key_concepts":{"key_phrases":["A Vision","Access Control","LLM","arXiv251011108v1 Announce Type","new Abstract","The autonomy","contextual complexity","LLM-based agents","traditional access control AC mechanisms","Static rule-based systems"],"filter_categories":{"ai_ml":["LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Vision":2.0,"Access Control":2.0,"LLM":2.0,"arXiv251011108v1 Announce Type":1.0,"new Abstract":1.0,"The autonomy":1.0,"contextual complexity":1.0,"LLM-based agents":1.0,"traditional access control AC mechanisms":1.0,"Static rule-based systems":1.0}},"age_hours":2.756511203888889,"is_recent":true,"quality_score":1.0,"sentiment_score":8.5445,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7089,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.5861,"joy":0.0147,"surprise":0.0494,"sadness":0.046,"fear":0.1967,"anger":0.0615,"disgust":0.0456},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel access control framework (AAC) for LLMs, but it remains a conceptual vision without concrete deployments or measured outcomes. The technical credibility is moderate due to the academic nature of the paper, but the lack of deployment readiness and economic viability limits its current sustainability impact. It's vaporware at this stage.","key_impact_metrics":[],"technology_tags":["Agent Access Control","LLM Security","Information Governance"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:51:11.956071Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_56057d84b554","title":"Graph Neural Network","content":"arXiv:2510.11109v1 Announce Type: new Abstract: The increase of bandwidth-intensive applications in sixth-generation (6G) wireless networks, such as real-time volumetric streaming and multi-sensory extended reality, demands intelligent multicast routing solutions capable of delivering differentiated quality-of-service (QoS) at scale. Traditional shortest-path and multicast routing algorithms are either computationally prohibitive or structurally rigid, and they often fail to support heterogeneous user demands, leading to suboptimal resource utilization. Neural network-based approaches, while offering improved inference speed, typically lack topological generalization and scalability. To address these limitations, this paper presents a graph neural network (GNN)-based multicast routing framework that jointly minimizes total transmission cost and supports user-specific video quality requirements. The routing problem is formulated as a constrained minimum-flow optimization task, and a reinforcement learning algorithm is developed to sequentially construct efficient multicast trees by reusing paths and adapting to network dynamics. A graph attention network (GAT) is employed as the encoder to extract context-aware node embeddings, while a long short-term memory (LSTM) module models the sequential dependencies in routing decisions. Extensive simulations demonstrate that the proposed method closely approximates optimal dynamic programming-based solutions while significantly reducing computational complexity. The results also confirm strong generalization to large-scale and dynamic network topologies, highlighting the method's potential for real-time deployment in 6G multimedia delivery scenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11109","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.879900","language":"en","tags":["computer-science","cslg","preprints","csni","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":213,"author":"Xiucheng Wang, Zien Wang, Nan Cheng, Wenchao Xu, Wei Quan, Xuemin Shen","raw_content_length":1782,"priority":7,"update_frequency":1,"reading_time_minutes":1.065,"robust_parsing_used":true,"entities":{"organizations":["Graph Neural Network arXiv:2510.11109v1 Announce Type"],"persons":["QoS"],"locations":[],"monetary":[]},"char_count":1781,"language_detected":"en","key_concepts":{"key_phrases":["Graph Neural Network","new Abstract","The increase","bandwidth-intensive applications","real-time volumetric streaming","multi-sensory extended reality","intelligent multicast routing solutions","service","scale","Traditional shortest-path"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Graph Neural Network":2.0,"new Abstract":1.0,"The increase":1.0,"bandwidth-intensive applications":1.0,"real-time volumetric streaming":1.0,"multi-sensory extended reality":1.0,"intelligent multicast routing solutions":1.0,"service":1.0,"scale":1.0,"Traditional shortest-path":1.0}},"age_hours":2.7565256211111113,"is_recent":true,"quality_score":1.0,"sentiment_score":8.715,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.743,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.702,"joy":0.0031,"surprise":0.0341,"sadness":0.1117,"fear":0.0191,"anger":0.0916,"disgust":0.0384},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a GNN-based multicast routing framework that minimizes transmission cost and supports user-specific video quality requirements. The method approximates optimal dynamic programming-based solutions while significantly reducing computational complexity. While simulations show strong generalization to large-scale and dynamic network topologies, it is still in the applied research stage with no deployed units.","key_impact_metrics":["Computational complexity reduction","Approximation of optimal dynamic programming solutions"],"technology_tags":["Graph Neural Network","Multicast Routing"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:51:36.628918Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c5f762d70061","title":"Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few","content":"arXiv:2510.11115v1 Announce Type: new Abstract: Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11115","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.881093","language":"en","tags":["computer-science","preprints","cscv","csmm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":188,"author":"Hao Tang, Shengfeng He, Jing Qin","raw_content_length":1496,"priority":7,"update_frequency":1,"reading_time_minutes":0.94,"robust_parsing_used":true,"entities":{"organizations":["FSL","Connecting Giants","CLIP"],"persons":["Knowledge Transfer"],"locations":[],"monetary":[]},"char_count":1495,"language_detected":"en","key_concepts":{"key_phrases":["Synergistic Knowledge Transfer","Connecting Giants","Large Multimodal Models","arXiv251011115v1 Announce Type","new Abstract","Few-shot learning","FSL","the challenge","novel classes","limited training samples"],"filter_categories":{"ai_ml":["limited training samples"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Synergistic Knowledge Transfer":3.0,"Connecting Giants":2.0,"Large Multimodal Models":2.0,"arXiv251011115v1 Announce Type":1.0,"new Abstract":1.0,"Few-shot learning":1.0,"FSL":1.0,"the challenge":1.0,"novel classes":1.0,"limited training samples":1.0}},"age_hours":2.7565695447222223,"is_recent":true,"quality_score":1.0,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8813,"joy":0.0087,"surprise":0.026,"sadness":0.012,"fear":0.0137,"anger":0.0326,"disgust":0.0258},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel framework for few-shot learning using large multimodal models. While it demonstrates improved performance on FSL datasets, it is still in the basic research phase with no deployed technology or measurable real-world impact on sustainability. The potential climate impact is theoretical, as it's unclear how this AI advancement directly translates to emissions reductions or other sustainability benefits.","key_impact_metrics":["Significantly outperforms current state-of-the-art methods on four FSL datasets"],"technology_tags":["Few-shot learning","Large Multimodal Models","Knowledge Transfer","AI"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:51:40.461666Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e13505e89967","title":"Demystifying Numerosity in Diffusion Models -","content":"arXiv:2510.11117v1 Announce Type: new Abstract: Numerosity remains a challenge for state-of-the-art text-to-image generation models like FLUX and GPT-4o, which often fail to accurately follow counting instructions in text prompts. In this paper, we aim to study a fundamental yet often overlooked question: Can diffusion models inherently generate the correct number of objects specified by a textual prompt simply by scaling up the dataset and model size? To enable rigorous and reproducible evaluation, we construct a clean synthetic numerosity benchmark comprising two complementary datasets: GrayCount250 for controlled scaling studies, and NaturalCount6 featuring complex naturalistic scenes. Second, we empirically show that the scaling hypothesis does not hold: larger models and datasets alone fail to improve counting accuracy on our benchmark. Our analysis identifies a key reason: diffusion models tend to rely heavily on the noise initialization rather than the explicit numerosity specified in the prompt. We observe that noise priors exhibit biases toward specific object counts. In addition, we propose an effective strategy for controlling numerosity by injecting count-aware layout information into the noise prior. Our method achieves significant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and on NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization across settings.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11117","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.881909","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":198,"author":"Yaqi Zhao, Xiaochen Wang, Li Dong, Wentao Zhang, Yuhui Yuan","raw_content_length":1429,"priority":7,"update_frequency":1,"reading_time_minutes":0.99,"robust_parsing_used":true,"entities":{"organizations":["Diffusion Models - arXiv:2510.11117v1 Announce"],"persons":[],"locations":[],"monetary":[]},"char_count":1428,"language_detected":"en","key_concepts":{"key_phrases":["Numerosity","Diffusion Models","arXiv251011117v1 Announce Type","new Abstract","a challenge","the-art","image","FLUX","GPT-4o","which"],"filter_categories":{"ai_ml":["GPT-4o"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Numerosity":3.0,"Diffusion Models":2.0,"arXiv251011117v1 Announce Type":1.0,"new Abstract":1.0,"a challenge":1.0,"the-art":1.0,"image":1.0,"FLUX":1.0,"GPT-4o":1.0,"which":1.0}},"age_hours":2.7565999883333334,"is_recent":true,"quality_score":1.0,"sentiment_score":4.36,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.128,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8283,"joy":0.0046,"surprise":0.0437,"sadness":0.0404,"fear":0.0367,"anger":0.0231,"disgust":0.0231},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents research on improving the accuracy of text-to-image generation models, specifically in counting objects. The concrete action is the development of a method for controlling numerosity by injecting count-aware layout information, improving accuracy on GrayCount250 from 20.0% to 85.3% and on NaturalCount6 from 74.8% to 86.3%. This is currently at the basic research stage, with no clear path to economic viability or large-scale deployment, and the climate impact is indirect at best.","key_impact_metrics":["Accuracy on GrayCount250 improved by 65.3%","Accuracy on NaturalCount6 improved by 11.5%"],"technology_tags":["diffusion models","text-to-image generation","machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:51:45.592372Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4c87acc3567e","title":"Improving AI Efficiency in Data Centres by Power Dynamic Response","content":"arXiv:2510.11119v1 Announce Type: new Abstract: The steady growth of artificial intelligence (AI) has accelerated in the recent years, facilitated by the development of sophisticated models such as large language models and foundation models. Ensuring robust and reliable power infrastructures is fundamental to take advantage of the full potential of AI. However, AI data centres are extremely hungry for power, putting the problem of their power management in the spotlight, especially with respect to their impact on environment and sustainable development. In this work, we investigate the capacity and limits of solutions based on an innovative approach for the power management of AI data centres, i.e., making part of the input power as dynamic as the power used for data-computing functions. The performance of passive and active devices are quantified and compared in terms of computational gain, energy efficiency, reduction of capital expenditure, and management costs by analysing power trends from multiple data platforms worldwide. This strategy, which identifies a paradigm shift in the AI data centre power management, has the potential to strongly improve the sustainability of AI hyperscalers, enhancing their footprint on environmental, financial, and societal fields.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11119","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.882441","language":"en","tags":["computer-science","csar","csai","preprints","csdc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":188,"author":"Andrea Marinoni, Sai Shivareddy, Pietro Lio', Weisi Lin, Erik Cambria, Clare Grey","raw_content_length":1288,"priority":7,"update_frequency":1,"reading_time_minutes":0.94,"robust_parsing_used":true,"entities":{"organizations":["Data Centres","Improving AI Efficiency"],"persons":[],"locations":[],"monetary":[]},"char_count":1287,"language_detected":"en","key_concepts":{"key_phrases":["AI Efficiency","Data Centres","Power Dynamic Response","arXiv251011119v1 Announce Type","new Abstract","The steady growth","artificial intelligence","the recent years","the development","sophisticated models"],"filter_categories":{"ai_ml":["AI Efficiency","artificial intelligence"],"engineering":["the development"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"AI Efficiency":2.0,"Data Centres":2.0,"Power Dynamic Response":2.0,"arXiv251011119v1 Announce Type":1.0,"new Abstract":1.0,"The steady growth":1.0,"artificial intelligence":1.0,"the recent years":1.0,"the development":1.0,"sophisticated models":1.0}},"age_hours":2.7566145858333333,"is_recent":true,"quality_score":1.0,"sentiment_score":9.792000000000002,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9584,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8183,"joy":0.0104,"surprise":0.0504,"sadness":0.0148,"fear":0.0147,"anger":0.0773,"disgust":0.0141},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":6,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article investigates a power management approach for AI data centers, focusing on dynamic power input. It quantifies performance in terms of computational gain, energy efficiency, and cost reduction using power trends from multiple data platforms. However, it's still in the research phase with no deployed units or independent verification, making it vaporware.","key_impact_metrics":["computational gain","energy efficiency"],"technology_tags":["AI power management","dynamic power input"],"sdg_alignment":[7,9,13],"analyzed_at":"2025-10-29T11:51:48.878478Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_12a51cf6a31c","title":"Refining Hybrid Genetic Search for CVRP via Reinforcement Learning","content":"arXiv:2510.11121v1 Announce Type: new Abstract: While large language models (LLMs) are increasingly used as automated heuristic designers for vehicle routing problems (VRPs), current state-of-the-art methods predominantly rely on prompting massive, general-purpose models like GPT-4. This work challenges that paradigm by demonstrating that a smaller, specialized LLM, when meticulously fine-tuned, can generate components that surpass expert-crafted heuristics within advanced solvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for Fine-Tuning a small LLM to generate high-performance crossover operators for the Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP). Our method employs a multi-tiered, curriculum-based reward function that progressively guides the LLM to master generating first compilable, then executable, and finally, superior-performing operators that exceed human expert designs. This is coupled with an operator caching mechanism that discourages plagiarism and promotes diversity during training. Comprehensive experiments show that our fine-tuned LLM produces crossover operators which significantly outperform the expert-designed ones in HGS. The performance advantage remains consistent, generalizing from small-scale instances to large-scale problems with up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading neuro-combinatorial baselines, prompt-based methods, and commercial LLMs such as GPT-4o and GPT-4o-mini.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11121","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.882895","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":190,"author":"Rongjie Zhu, Cong Zhang, Zhiguang Cao","raw_content_length":1507,"priority":7,"update_frequency":1,"reading_time_minutes":0.95,"robust_parsing_used":true,"entities":{"organizations":["the Hybrid Genetic Search","RFTHGS","HGS","LLM","CVRP","Reinforcement Learning arXiv:2510.11121v1"],"persons":["GPT-4"],"locations":[],"monetary":[]},"char_count":1506,"language_detected":"en","key_concepts":{"key_phrases":["Refining Hybrid Genetic Search","CVRP","Reinforcement Learning","new Abstract","large language models","LLMs","automated heuristic designers","vehicle routing problems","VRPs","the-art"],"filter_categories":{"ai_ml":["Reinforcement Learning","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Refining Hybrid Genetic Search":2.0,"CVRP":2.0,"Reinforcement Learning":2.0,"new Abstract":1.0,"large language models":1.0,"LLMs":1.0,"automated heuristic designers":1.0,"vehicle routing problems":1.0,"VRPs":1.0,"the-art":1.0}},"age_hours":2.7566302691666666,"is_recent":true,"quality_score":1.0,"sentiment_score":5.1290000000000004,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8273,"joy":0.0256,"surprise":0.1133,"sadness":0.0034,"fear":0.0067,"anger":0.0197,"disgust":0.004},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel reinforcement learning framework (RFTHGS) for fine-tuning a small LLM to generate high-performance crossover operators for the Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP). This could lead to more efficient vehicle routing, potentially reducing fuel consumption and emissions. However, it is still in the applied research phase with no deployed units or customer contracts, making economic viability and deployment readiness low.","key_impact_metrics":["Performance advantage over expert-designed operators","Generalization to large-scale problems with up to 1000 nodes"],"technology_tags":["Reinforcement Learning","Large Language Models","Vehicle Routing Optimization"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-29T11:51:52.316152Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b9b23da5f424","title":"DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval","content":"arXiv:2510.11122v1 Announce Type: new Abstract: Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11122","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.883319","language":"en","tags":["csir","research","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":241,"author":"Tingqiao Xu, Shaowei Yao, Chenhe Dong, Yiming Jin, Zerui Huang, Dan Ou, Haihong Tang","raw_content_length":1781,"priority":7,"update_frequency":1,"reading_time_minutes":1.205,"robust_parsing_used":true,"entities":{"organizations":["DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for","UGC","Group Relative Policy Optimization"],"persons":["DyKnow-RAG"],"locations":[],"monetary":[]},"char_count":1780,"language_detected":"en","key_concepts":{"key_phrases":["DyKnow-RAG Dynamic Knowledge Utilization Reinforcement Framework","Noisy Retrieval","arXiv251011122v1 Announce Type","new Abstract","query-item relevance","e-commerce ranking","parametric LLM coverage","External context","reviews","attribute encyclopedias"],"filter_categories":{"ai_ml":["parametric LLM coverage"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"DyKnow-RAG Dynamic Knowledge Utilization Reinforcement Framework":2.0,"Noisy Retrieval":2.0,"arXiv251011122v1 Announce Type":1.0,"new Abstract":1.0,"query-item relevance":1.0,"e-commerce ranking":1.0,"parametric LLM coverage":1.0,"External context":1.0,"reviews":1.0,"attribute encyclopedias":1.0}},"age_hours":2.756647056666667,"is_recent":true,"quality_score":1.0,"sentiment_score":2.9905000000000004,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4019,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8929,"joy":0.0062,"surprise":0.0254,"sadness":0.005,"fear":0.0107,"anger":0.0304,"disgust":0.0294},"emotion_method":"local"},"sustainability_analysis":{"content_type":"technology_deployment","innovation_stage":"commercial","climate_impact_potential":3,"technical_credibility":7,"economic_viability":6,"deployment_readiness":7,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":true,"has_metrics":true,"has_peer_review":true,"has_deployment":true},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"The article describes a deployed AI system (DyKnow-RAG) within Taobao's e-commerce platform, improving relevance ranking. The system is in production, serving live traffic and delivering consistent lifts on GSB, Query Goodrate, and Item Goodrate in A/B testing, indicating real-world impact. While not directly climate-related, improved e-commerce efficiency could indirectly reduce energy consumption associated with online shopping.","key_impact_metrics":["GSB lift","Query Goodrate lift","Item Goodrate lift"],"technology_tags":["AI","Machine Learning","Relevance Ranking","E-commerce"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T11:52:06.618389Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_701a026e3f9d","title":"Visible Light Communication for Vehicular Networks: A Tutorial","content":"arXiv:2510.11123v1 Announce Type: new Abstract: The advent of the fifth-generation technology promises to bring about more vertical applications and emerging services that include vehicular networks and intelligent transportation systems (ITSs). To achieve their vision of real-time and safetyapplications, vehicular networks rely on short-range to medium-range communications. One emerging technology that aims to provide reliability and high-data rate in short-range communications is the visible light communications (VLC). Due to its remarkable advantages, some studies have recently investigated the integration of VLC in vehicular networks and ITSs. Despite their attractive features, such networks also face several implementation issues. This paper provides an extended tutorial on the implementation of VLC-based vehicular networks. To begin with, we present the implementation characteristics of these systems and discuss some related issues. The underlying system considers a general structure with transmitters, channels, and receivers based on photodetectors and cameras, as well as standardization efforts and types of topologies. In addition, we discuss the impact of the sun and artificial light sources, flickering, dimming, throughput enhancement, uplink security, and mobility on practical implementation. Finally, we highlight some key challenges and potential solutions and provide some directions for future research investigations that could constitute an advancement toward the development of commercial VLC-based vehicular systems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11123","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.883804","language":"en","tags":["eesssy","cssy","preprints","csni","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":205,"author":"Pedro E. G\\'oria Silva, Eduardo S. Lima, Jules M. Moualeu, Mohamed Korium, Pedro H. J. Nardelli","raw_content_length":1557,"priority":7,"update_frequency":1,"reading_time_minutes":1.025,"robust_parsing_used":true,"entities":{"organizations":["Visible Light Communication for Vehicular Networks: A Tutorial arXiv:2510.11123v1 Announce Type","VLC"],"persons":[],"locations":[],"monetary":[]},"char_count":1556,"language_detected":"en","key_concepts":{"key_phrases":["Visible Light Communication","Vehicular Networks","A Tutorial","vehicular networks","arXiv251011123v1 Announce Type","new Abstract","The advent","the fifth-generation technology","more vertical applications","emerging services"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Visible Light Communication":2.0,"Vehicular Networks":2.0,"A Tutorial":2.0,"vehicular networks":2.0,"arXiv251011123v1 Announce Type":1.0,"new Abstract":1.0,"The advent":1.0,"the fifth-generation technology":1.0,"more vertical applications":1.0,"emerging services":1.0}},"age_hours":2.7566619769444443,"is_recent":true,"quality_score":1.0,"sentiment_score":8.825000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.765,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8331,"joy":0.0751,"surprise":0.0611,"sadness":0.0035,"fear":0.0142,"anger":0.0095,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper is a tutorial on VLC for vehicular networks, focusing on implementation characteristics and challenges. It's primarily theoretical, discussing potential solutions but lacking concrete deployments or measured outcomes. The potential climate impact is indirect, relying on the assumption that improved vehicular networks will lead to more efficient transportation and reduced emissions, but this is not quantified.","key_impact_metrics":[],"technology_tags":["Visible Light Communication","Vehicular Networks","Intelligent Transportation Systems"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:52:23.589071Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9099581792dc","title":"Perturbation Self-Supervised Representations for Cross","content":"arXiv:2510.11124v1 Announce Type: new Abstract: Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice's timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language. However, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies-formant shifting and speaker anonymization-on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker-Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11124","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.884218","language":"en","tags":["computer-science","research","cssd","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Cheng Gong, Chunyu Qiang, Tianrui Wang, Yu Jiang, Yuheng Lu, Ruihao Jing, Xiaoxiao Miao, Xiaolei Zhang, Longbiao Wang, Jianwu Dang","raw_content_length":1597,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["SSL","EMM","TTS"],"persons":["arXiv:2510.11124v1 Announce Type"],"locations":[],"monetary":[]},"char_count":1596,"language_detected":"en","key_concepts":{"key_phrases":["Perturbation Self-Supervised Representations","Cross","speech","emotion","timbre","arXiv251011124v1 Announce Type","new Abstract","Cross-lingual emotional text","TTS","one language"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Perturbation Self-Supervised Representations":2.0,"Cross":2.0,"speech":2.0,"emotion":2.0,"timbre":2.0,"arXiv251011124v1 Announce Type":1.0,"new Abstract":1.0,"Cross-lingual emotional text":1.0,"TTS":1.0,"one language":1.0}},"age_hours":2.7566774625000003,"is_recent":true,"quality_score":1.0,"sentiment_score":7.6335,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5267,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.898,"joy":0.0158,"surprise":0.0438,"sadness":0.0078,"fear":0.0113,"anger":0.0161,"disgust":0.0071},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for cross-lingual emotional text-to-speech synthesis. While the research is technically sound and includes multi-metric evaluations, it is in the early stages of development and lacks concrete actions or measurable outcomes related to sustainability. The technology itself does not directly address climate change or environmental issues.","key_impact_metrics":["Naturalness score","Emotion transferability score"],"technology_tags":["Text-to-speech","Cross-lingual learning","Self-supervised learning"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:52:38.554994Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a337b715995a","title":"Lightweight Facial Landmark Detection in Thermal Images via Multi","content":"arXiv:2510.11128v1 Announce Type: new Abstract: Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the student's learned representations by feeding them back into the frozen teacher's prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11128","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.884658","language":"en","tags":["computer-science","cslg","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":213,"author":"Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani","raw_content_length":1629,"priority":7,"update_frequency":1,"reading_time_minutes":1.065,"robust_parsing_used":true,"entities":{"organizations":["Lightweight Facial Landmark Detection","Facial Landmark Detection","Multi-Level Cross-Modal Knowledge Distillation","RGB","FLD","Multi arXiv:2510.11128v1 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1628,"language_detected":"en","key_concepts":{"key_phrases":["Lightweight Facial Landmark Detection","Thermal Images","Multi","arXiv251011128v1 Announce Type","new Abstract","Facial Landmark Detection","FLD","thermal imagery","applications","challenging lighting conditions"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Lightweight Facial Landmark Detection":2.0,"Thermal Images":2.0,"Multi":2.0,"arXiv251011128v1 Announce Type":1.0,"new Abstract":1.0,"Facial Landmark Detection":1.0,"FLD":1.0,"thermal imagery":1.0,"applications":1.0,"challenging lighting conditions":1.0}},"age_hours":2.75669294,"is_recent":true,"quality_score":1.0,"sentiment_score":9.01,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.802,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6031,"joy":0.0073,"surprise":0.0286,"sadness":0.1552,"fear":0.0697,"anger":0.0614,"disgust":0.0747},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the efficiency of facial landmark detection in thermal images, which could have indirect sustainability benefits by reducing computational power needed for applications like security or monitoring. The article presents a novel method (MLCM-KD) and shows improved performance on benchmarks, but it's still in the research phase with no deployed units or concrete economic viability data. The impact on climate is indirect and difficult to quantify at this stage.","key_impact_metrics":["drastically reducing computational overhead"],"technology_tags":["facial landmark detection","thermal imaging","knowledge distillation","machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:52:49.279981Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f59635ba9590","title":"Test","content":"arXiv:2502.20795v3 Announce Type: replace Abstract: Aligning Large Language Models (LLMs) with human preferences through finetuning is resource-intensive, motivating lightweight alternatives at test time. We address test-time alignment through the lens of sequential decision making, a perspective that reveals two fundamental challenges. When actions are defined at the token level, as in guided decoding, alignment suffers from the curse of horizon. Conversely, when actions are at the response level, as in traditional iterative refinement, the curse of dimensionality emerges. To resolve this trade-off, we draw inspiration from Model Predictive Control (MPC) in control theory to propose Textual Model Predictive Control (TMPC), a novel predictive planning framework adapted for aligning LLMs at inference time. A key limitation of standard MPC is its reliance on predefined, hard segment boundaries, which are often absent in text generation. TMPC overcomes this by introducing two principles inspired by hierarchical reinforcement learning: (1) Hindsight Subgoal Identification, where TMPC analyzes generation subgoals to retrospectively identify high-reward intermediate outputs as subgoals. This allows the framework to discover meaningful, task-specific planning steps (e.g., a sentence in machine translation or a bug fix in code generation.). (2) Subgoal-Conditioned Re-Generation, where these identified subgoals are used to guide subsequent planning iterations. By conditioning on these proven, high-quality subgoals, TMPC ensures stable improvement by building upon previously validated successes. TMPC is evaluated on three tasks with distinct segmentation properties: discourse-level translation, long-form response generation, and program synthesis. The results demonstrate that TMPC consistently improves performance, highlighting the generality.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.20795","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.136256","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":245,"author":"Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Guo-Xun Ko, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh","raw_content_length":1866,"priority":7,"update_frequency":1,"reading_time_minutes":1.225,"robust_parsing_used":true,"entities":{"organizations":["TMPC","MPC","Model Predictive Control"],"persons":[],"locations":[],"monetary":[]},"char_count":1865,"language_detected":"en","key_concepts":{"key_phrases":["Test","actions","arXiv250220795v3","Announce Type","Abstract","Large Language Models","LLMs","human preferences","finetuning","lightweight alternatives"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Test":2.0,"actions":2.0,"arXiv250220795v3":1.0,"Announce Type":1.0,"Abstract":1.0,"Large Language Models":1.0,"LLMs":1.0,"human preferences":1.0,"finetuning":1.0,"lightweight alternatives":1.0}},"age_hours":2.7649942555555556,"is_recent":true,"quality_score":0.7,"sentiment_score":2.6165,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4767,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8395,"joy":0.0087,"surprise":0.0159,"sadness":0.0448,"fear":0.0177,"anger":0.0196,"disgust":0.0538},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel method (TMPC) for improving LLM alignment at inference time, drawing inspiration from Model Predictive Control. It is currently at the research stage, with no deployed units or real-world data. The potential climate impact is indirect, as better LLMs could potentially improve efficiency in various sectors, but this is not quantified.","key_impact_metrics":[],"technology_tags":["Large Language Models","Model Predictive Control","Artificial Intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:52:58.170433Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ddc3f2100311","title":"CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense","content":"arXiv:2510.11137v1 Announce Type: new Abstract: Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11137","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.886255","language":"en","tags":["preprints","research","computer-science","cscr","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":161,"author":"Yang Zhuochen, Fok Kar Wai, Thing Vrizlynn","raw_content_length":1180,"priority":7,"update_frequency":1,"reading_time_minutes":0.805,"robust_parsing_used":true,"entities":{"organizations":["Extraction and Defense","LLM","Dynamic Loss"],"persons":["Common Loss","Consistent Soft Prompt"],"locations":["Pythia"],"monetary":[]},"char_count":1179,"language_detected":"en","key_concepts":{"key_phrases":["CoSPED","Defense","Consistent Soft Prompt Targeted Data Extraction","arXiv251011137v1 Announce Type","new Abstract","Large language models","widespread attention","their potential security vulnerabilities","especially privacy leakage","data extraction risks"],"filter_categories":{"ai_ml":["Large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CoSPED":3.0,"Defense":3.0,"Consistent Soft Prompt Targeted Data Extraction":2.0,"arXiv251011137v1 Announce Type":1.0,"new Abstract":1.0,"Large language models":1.0,"widespread attention":1.0,"their potential security vulnerabilities":1.0,"especially privacy leakage":1.0,"data extraction risks":1.0}},"age_hours":2.7567538541666665,"is_recent":true,"quality_score":1.0,"sentiment_score":7.859499999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5719,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8857,"joy":0.009,"surprise":0.0396,"sadness":0.0063,"fear":0.0309,"anger":0.019,"disgust":0.0095},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on identifying and mitigating privacy vulnerabilities in large language models. While it doesn't directly address climate change, improving data security could indirectly support sustainability efforts by protecting sensitive environmental data or preventing misuse of AI for harmful purposes. The research is in an early stage, with experiments conducted but no real-world deployment.","key_impact_metrics":["extraction rate of 65.2% at a 50-token prefix comparison","reduction in the extraction rate to 1.6% through Rank-One Model Editing"],"technology_tags":["large language models","data extraction","privacy defense","soft prompt tuning"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T11:53:02.036074Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_0fcbcd4f2862","title":"What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times","content":"arXiv:2510.11138v1 Announce Type: new Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming the practice of software engineering by enabling the development of \\emph{FMware} -- applications and infrastructures built around these models. FMware systems now support tasks such as code generation, natural-language interaction, knowledge integration, and multi-modal content creation, underscoring their disruptive impact on current software engineering workflows. However, the design, implementation, and evolution of FMware present significant new challenges, particularly across cloud-based and on-premise platforms where goals, processes, and tools often diverge from those of traditional software development. To our knowledge, this is the first large-scale analysis of FMware development across both cloud-based platforms and open-source repositories. We empirically investigate the FMware ecosystem through three focus areas: (1) the most common application domains of FMware, (2) the key challenges developers encounter, and (3) the types of issues that demand the greatest effort to resolve. Our analysis draws on data from GitHub repositories and from leading FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings reveal a strong focus on education, content creation, and business strategy, alongside persistent technical challenges in memory management, dependency handling, and tokenizer configuration. On GitHub, bug reports and core functionality issues are the most frequently reported problems, while code review, similarity search, and prompt template design are the most time-consuming to resolve. By uncovering developer practices and pain points, this study points to opportunities to improve FMware tools, workflows, and community support, and provides actionable insights to help guide the future of FMware development.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11138","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.886696","language":"en","tags":["preprints","research","computer-science","csse","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":252,"author":"Zitao Wang, Zhimin Zhao, Michael W. Godfrey","raw_content_length":1901,"priority":7,"update_frequency":1,"reading_time_minutes":1.26,"robust_parsing_used":true,"entities":{"organizations":["OpenAI","FMware","Foundation Models","GPT"],"persons":[],"locations":[],"monetary":[]},"char_count":1896,"language_detected":"en","key_concepts":{"key_phrases":["What","FMware Development","An Empirical Study","Developer Challenges","Resolution Times","arXiv251011138v1 Announce Type","new Abstract","Foundation Models","FMs","OpenAIs GPT"],"filter_categories":{"engineering":["FMware Development"],"research_academic":["An Empirical Study"],"ai_ml":["OpenAIs GPT"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"What":2.0,"FMware Development":2.0,"An Empirical Study":2.0,"Developer Challenges":2.0,"Resolution Times":2.0,"arXiv251011138v1 Announce Type":1.0,"new Abstract":1.0,"Foundation Models":1.0,"FMs":1.0,"OpenAIs GPT":1.0}},"age_hours":2.756768597222222,"is_recent":true,"quality_score":1.0,"sentiment_score":7.2940000000000005,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4588,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7538,"joy":0.0455,"surprise":0.0999,"sadness":0.0095,"fear":0.0682,"anger":0.0157,"disgust":0.0075},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper analyzes challenges in FMware development, focusing on developer pain points and resolution times. While it doesn't directly address climate change, improving software development efficiency could indirectly reduce energy consumption in data centers. The research is based on data from GitHub and FMware platforms, but lacks specific metrics related to environmental impact.","key_impact_metrics":[],"technology_tags":["Foundation Models","FMware","Software Engineering"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:53:05.186052Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c77326fc603e","title":"DUAL: Learning Diverse Kernels for Aggregated Two","content":"arXiv:2510.11140v1 Announce Type: new Abstract: To adapt kernel two-sample and independence testing to complex structured data, aggregation of multiple kernels is frequently employed to boost testing power compared to single-kernel tests. However, we observe a phenomenon that directly maximizing multiple kernel-based statistics may result in highly similar kernels that capture highly overlapping information, limiting the effectiveness of aggregation. To address this, we propose an aggregated statistic that explicitly incorporates kernel diversity based on the covariance between different kernels. Moreover, we identify a fundamental challenge: a trade-off between the diversity among kernels and the test power of individual kernels, i.e., the selected kernels should be both effective and diverse. This motivates a testing framework with selection inference, which leverages information from the training phase to select kernels with strong individual performance from the learned diverse kernel pool. We provide rigorous theoretical statements and proofs to show the consistency on the test power and control of Type-I error, along with asymptotic analysis of the proposed statistics. Lastly, we conducted extensive empirical experiments demonstrating the superior performance of our proposed approach across various benchmarks for both two-sample and independence testing.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11140","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.887097","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":187,"author":"Zhijian Zhou, Xunye Tian, Liuhua Peng, Chao Lei, Antonin Schrab, Danica J. Sutherland, Feng Liu","raw_content_length":1383,"priority":7,"update_frequency":1,"reading_time_minutes":0.935,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1382,"language_detected":"en","key_concepts":{"key_phrases":["DUAL","Diverse Kernels","Aggregated Two","aggregation","arXiv251011140v1","Announce Type","new Abstract","kernel two-sample and independence testing","complex structured data","multiple kernels"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"DUAL":2.0,"Diverse Kernels":2.0,"Aggregated Two":2.0,"aggregation":2.0,"arXiv251011140v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"kernel two-sample and independence testing":1.0,"complex structured data":1.0,"multiple kernels":1.0}},"age_hours":2.7567829305555556,"is_recent":true,"quality_score":0.7,"sentiment_score":7.009499999999999,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4019,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8933,"joy":0.0079,"surprise":0.0573,"sadness":0.0063,"fear":0.0063,"anger":0.0182,"disgust":0.0107},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel method for kernel aggregation in machine learning to improve two-sample and independence testing. While the research has theoretical backing and shows superior performance in experiments, it is still in the early stages of development with no deployed applications or quantified environmental benefits. The potential for climate impact is indirect and speculative at this stage.","key_impact_metrics":[],"technology_tags":["machine learning","kernel methods","statistical testing"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:53:08.300172Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a87f540cd953","title":"A Comprehensive Forecasting","content":"arXiv:2510.11141v1 Announce Type: new Abstract: Time series anomaly detection is critical for modern digital infrastructures, yet existing methods lack systematic cross-domain evaluation. We present a comprehensive forecasting-based framework unifying classical methods (Holt-Winters, SARIMA) with deep learning architectures (LSTM, Informer) under a common residual-based detection interface. Our modular pipeline integrates preprocessing (normalization, STL decomposition), four forecasting models, four detection methods, and dual evaluation through forecasting metrics (MAE, RMSE, PCC) and detection metrics (Precision, Recall, F1, AUC). We conduct the first complete evaluation on the Numenta Anomaly Benchmark (58 datasets, 7 categories) with 232 model training runs and 464 detection evaluations achieving 100\\% success rate. LSTM achieves best performance (F1: 0.688, ranking first or second on 81\\% of datasets) with exceptional correlation on complex patterns (PCC: 0.999). Informer provides competitive accuracy (F1: 0.683) with 30\\% faster training. Classical methods achieve perfect predictions on simple synthetic data with 60 lower cost but show 2-3 worse F1-scores on real-world datasets. Forecasting quality dominates detection performance: differences between detection methods (F1: 0.621-0.688) are smaller than between forecasting models (F1: 0.344-0.688). Our findings provide evidence-based guidance: use LSTM for complex patterns, Informer for efficiency-critical deployments, and classical methods for simple periodic data with resource constraints. The complete implementation and results establish baselines for future forecasting-based anomaly detection research.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11141","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.887685","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":212,"author":"Mohammad Karami, Mostafa Jalali, Fatemeh Ghassemi","raw_content_length":1691,"priority":7,"update_frequency":1,"reading_time_minutes":1.06,"robust_parsing_used":true,"entities":{"organizations":["MAE","Holt-Winters","STL","the Numenta Anomaly Benchmark","RMSE","PCC"],"persons":["SARIMA"],"locations":[],"monetary":[]},"char_count":1690,"language_detected":"en","key_concepts":{"key_phrases":["A Comprehensive Forecasting","arXiv251011141v1 Announce Type","new Abstract","Time series anomaly detection","modern digital infrastructures","existing methods","systematic cross-domain evaluation","a comprehensive forecasting-based framework","classical methods","Holt-Winters"],"filter_categories":{"ai_ml":["systematic cross-domain evaluation"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Comprehensive Forecasting":2.0,"arXiv251011141v1 Announce Type":1.0,"new Abstract":1.0,"Time series anomaly detection":1.0,"modern digital infrastructures":1.0,"existing methods":1.0,"systematic cross-domain evaluation":1.0,"a comprehensive forecasting-based framework":1.0,"classical methods":1.0,"Holt-Winters":1.0}},"age_hours":2.7567980319444447,"is_recent":true,"quality_score":1.0,"sentiment_score":4.2345,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9008,"joy":0.0099,"surprise":0.0531,"sadness":0.0081,"fear":0.012,"anger":0.0105,"disgust":0.0055},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a forecasting-based framework for anomaly detection in time series data, evaluating different models on the Numenta Anomaly Benchmark. While it identifies potentially more efficient and accurate methods (LSTM, Informer), it remains in the applied research stage with no clear deployment or economic viability demonstrated. The impact on climate is indirect, as it could improve the efficiency of systems that impact climate, but this is not directly measured or quantified.","key_impact_metrics":["F1: 0.688","PCC: 0.999"],"technology_tags":["Time series analysis","Anomaly detection","LSTM","Informer"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:53:11.408928Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f1661ba9272e","title":"Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay","content":"arXiv:2510.11142v1 Announce Type: new Abstract: Sperm DNA fragmentation (SDF) is a critical parameter in male fertility assessment that conventional semen analysis fails to evaluate. This study presents the validation of a novel artificial intelligence (AI) tool designed to detect SDF through digital analysis of phase contrast microscopy images, using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) assay as the gold standard reference. Utilising the established link between sperm morphology and DNA integrity, the present work proposes a morphology assisted ensemble AI model that combines image processing techniques with state-of-the-art transformer based machine learning models (GC-ViT) for the prediction of DNA fragmentation in sperm from phase contrast images. The ensemble model is benchmarked against a pure transformer `vision' model as well as a `morphology-only` model. Promising results show the proposed framework is able to achieve sensitivity of 60\\% and specificity of 75\\%. This non-destructive methodology represents a significant advancement in reproductive medicine by enabling real-time sperm selection based on DNA integrity for clinical diagnostic and therapeutic applications.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11142","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.888089","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":168,"author":"Byron Alexander Jacobs, Aqeel Morris, Ifthakaar Shaik, Frando Lin","raw_content_length":1232,"priority":7,"update_frequency":1,"reading_time_minutes":0.84,"robust_parsing_used":true,"entities":{"organizations":["GC-ViT","SDF","TUNEL"],"persons":["dUTP nick"],"locations":[],"monetary":[]},"char_count":1231,"language_detected":"en","key_concepts":{"key_phrases":["Validation","an Artificial Intelligence Tool","the Detection","Sperm DNA Fragmentation","the TUNEL","Situ Hybridization Assay","SDF","Announce Type","new Abstract","Sperm DNA fragmentation"],"filter_categories":{"ai_ml":["an Artificial Intelligence Tool"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Validation":2.0,"an Artificial Intelligence Tool":2.0,"the Detection":2.0,"Sperm DNA Fragmentation":2.0,"the TUNEL":2.0,"Situ Hybridization Assay":2.0,"SDF":2.0,"Announce Type":1.0,"new Abstract":1.0,"Sperm DNA fragmentation":1.0}},"age_hours":2.7568133208333334,"is_recent":true,"quality_score":1.0,"sentiment_score":7.6335,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5267,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7953,"joy":0.0203,"surprise":0.0806,"sadness":0.0126,"fear":0.0317,"anger":0.0297,"disgust":0.0297},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes the validation of an AI tool for sperm DNA fragmentation detection. While it presents promising results with a sensitivity of 60% and specificity of 75%, it is still in the early stages of development and lacks deployment data. The impact on climate and broader sustainability goals is minimal at this stage.","key_impact_metrics":["sensitivity of 60%","specificity of 75%"],"technology_tags":["artificial intelligence","image processing","machine learning","sperm DNA fragmentation"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:53:14.794916Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a610554559bc","title":"Spec","content":"arXiv:2510.11143v1 Announce Type: new Abstract: The rapid expansion of scientific data has widened the gap between analytical capability and research intent. Existing AI-based analysis tools, ranging from AutoML frameworks to agentic research assistants, either favor automation over transparency or depend on manual scripting that hinders scalability and reproducibility. We present ARIA (Automated Research Intelligence Assistant), a spec-driven, human-in-the-loop framework for automated and interpretable data analysis. ARIA integrates six interoperable layers, namely Command, Context, Code, Data, Orchestration, and AI Module, within a document-centric workflow that unifies human reasoning and machine execution. Through natural-language specifications, researchers define analytical goals while ARIA autonomously generates executable code, validates computations, and produces transparent documentation. Beyond achieving high predictive accuracy, ARIA can rapidly identify optimal feature sets and select suitable models, minimizing redundant tuning and repetitive experimentation. In the Boston Housing case, ARIA discovered 25 key features and determined XGBoost as the best performing model (R square = 0.93) with minimal overfitting. Evaluations across heterogeneous domains demonstrate ARIA's strong performance, interpretability, and efficiency compared with state-of-the-art systems. By combining AI for research and AI for science principles within a spec-driven architecture, ARIA establishes a new paradigm for transparent, collaborative, and reproducible scientific discovery.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11143","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.888515","language":"en","tags":["computer-science","csai","preprints","cshc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Chuke Chen, Biao Luo, Nan Li, Boxiang Wang, Hang Yang, Jing Guo, Ming Xu","raw_content_length":1596,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["Command","Data, Orchestration","AI Module"],"persons":["ARIA"],"locations":["ARIA"],"monetary":[]},"char_count":1595,"language_detected":"en","key_concepts":{"key_phrases":["Spec","arXiv251011143v1 Announce Type","new Abstract","The rapid expansion","scientific data","the gap","analytical capability","research intent","Existing AI-based analysis tools","AutoML frameworks"],"filter_categories":{"research_academic":["research intent"],"ai_ml":["Existing AI-based analysis tools"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Spec":2.0,"arXiv251011143v1 Announce Type":1.0,"new Abstract":1.0,"The rapid expansion":1.0,"scientific data":1.0,"the gap":1.0,"analytical capability":1.0,"research intent":1.0,"Existing AI-based analysis tools":1.0,"AutoML frameworks":1.0}},"age_hours":2.7568279686111112,"is_recent":true,"quality_score":0.7,"sentiment_score":8.5015,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7003,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9335,"joy":0.0071,"surprise":0.0327,"sadness":0.0042,"fear":0.005,"anger":0.0109,"disgust":0.0066},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"ARIA is a framework for automated data analysis that aims to improve efficiency and transparency in scientific research. While it has the potential to accelerate the discovery of sustainable solutions, its direct climate impact is currently theoretical. The system is at the applied research stage, with demonstrated performance on a specific case study (Boston Housing) but no real-world deployments yet.","key_impact_metrics":["R square = 0.93"],"technology_tags":["AI","Data Analysis","Machine Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:53:18.347981Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_dca71429a198","title":"$How^{2}$: How to learn from procedural How","content":"arXiv:2510.11144v1 Announce Type: new Abstract: An agent facing a planning problem can use answers to how-to questions to reduce uncertainty and fill knowledge gaps, helping it solve both current and future tasks. However, their open ended nature, where valid answers to \"How do I X?\" range from executable actions to high-level descriptions of X's sub-goals, makes them challenging for AI agents to ask, and for AI experts to answer, in ways that support efficient planning. We introduce $How^{2}$, a memory agent framework that enables agents to ask how-to questions, store the answers, and reuse them for lifelong learning in interactive environments. We evaluate our approach in Plancraft, a Minecraft crafting environment, where agents must complete an assembly task by manipulating inventory items. Using teacher models that answer at varying levels of abstraction, from executable action sequences to high-level subgoal descriptions, we show that lifelong learning agents benefit most from answers that are abstracted and decoupled from the current state. $How^{2}$ offers a way for LLM-based agents to improve their planning capabilities over time by asking questions in interactive environments.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11144","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.888924","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":179,"author":"Gautier Dagan, Frank Keller, Alex Lascarides","raw_content_length":1205,"priority":7,"update_frequency":1,"reading_time_minutes":0.895,"robust_parsing_used":true,"entities":{"organizations":["Plancraft"],"persons":[],"locations":[],"monetary":[]},"char_count":1204,"language_detected":"en","key_concepts":{"key_phrases":["How2","arXiv251011144v1 Announce Type","new Abstract","An agent","a planning problem","answers","questions","uncertainty","knowledge gaps","both current and future tasks"],"filter_categories":{"ai_ml":["uncertainty"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"How2":3.0,"arXiv251011144v1 Announce Type":1.0,"new Abstract":1.0,"An agent":1.0,"a planning problem":1.0,"answers":1.0,"questions":1.0,"uncertainty":1.0,"knowledge gaps":1.0,"both current and future tasks":1.0}},"age_hours":2.7568443058333334,"is_recent":true,"quality_score":1.0,"sentiment_score":8.062000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6124,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8081,"joy":0.0176,"surprise":0.0319,"sadness":0.0052,"fear":0.112,"anger":0.021,"disgust":0.0041},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel AI framework ($How^{2}$) for lifelong learning in interactive environments. While the framework could potentially lead to more efficient resource utilization in various applications (e.g., manufacturing, logistics), it is currently in the basic research phase with no concrete deployments or measurable environmental outcomes. The paper does mention using a Minecraft crafting environment for evaluation, providing some metrics on learning efficiency.","key_impact_metrics":["Learning efficiency in Plancraft environment"],"technology_tags":["Artificial Intelligence","Lifelong Learning","Planning Algorithms"],"sdg_alignment":[4,9,12],"analyzed_at":"2025-10-29T11:53:21.531622Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c5d3d0d58713","title":"TypePilot: Leveraging the Scala Type System for Secure LLM","content":"arXiv:2510.11151v1 Announce Type: new Abstract: Large language Models (LLMs) have shown remarkable proficiency in code generation tasks across various programming languages. However, their outputs often contain subtle but critical vulnerabilities, posing significant risks when deployed in security-sensitive or mission-critical systems. This paper introduces TypePilot, an agentic AI framework designed to enhance the security and robustness of LLM-generated code by leveraging strongly typed and verifiable languages, using Scala as a representative example. We evaluate the effectiveness of our approach in two settings: formal verification with the Stainless framework and general-purpose secure code generation. Our experiments with leading open-source LLMs reveal that while direct code generation often fails to enforce safety constraints, just as naive prompting for more secure code, our type-focused agentic pipeline substantially mitigates input validation and injection vulnerabilities. The results demonstrate the potential of structured, type-guided LLM workflows to improve the SotA of the trustworthiness of automated code generation in high-assurance domains.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11151","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.889316","language":"en","tags":["computer-science","preprints","cscl","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":153,"author":"Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic","raw_content_length":1177,"priority":7,"update_frequency":1,"reading_time_minutes":0.765,"robust_parsing_used":true,"entities":{"organizations":["TypePilot","LLM"],"persons":["Scala","Stainless"],"locations":[],"monetary":[]},"char_count":1176,"language_detected":"en","key_concepts":{"key_phrases":["TypePilot","the Scala Type System","Secure LLM","arXiv251011151v1 Announce Type","new Abstract","Large language Models","LLMs","remarkable proficiency","code generation tasks","various programming languages"],"filter_categories":{"ai_ml":["Secure LLM","Large language Models"],"engineering":["various programming languages"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"TypePilot":3.0,"the Scala Type System":2.0,"Secure LLM":2.0,"arXiv251011151v1 Announce Type":1.0,"new Abstract":1.0,"Large language Models":1.0,"LLMs":1.0,"remarkable proficiency":1.0,"code generation tasks":1.0,"various programming languages":1.0}},"age_hours":2.756858670277778,"is_recent":true,"quality_score":1.0,"sentiment_score":6.0115,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6088,"joy":0.0329,"surprise":0.0245,"sadness":0.0095,"fear":0.2878,"anger":0.0283,"disgust":0.0083},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to improving the security of LLM-generated code using type systems, demonstrating mitigation of input validation and injection vulnerabilities. However, it is still in the early stages of development (pilot program) and lacks concrete deployment data or economic viability analysis. The impact on climate is indirect, as it improves the reliability of code that *could* be used in climate-related applications.","key_impact_metrics":["Mitigates input validation vulnerabilities","Mitigates injection vulnerabilities"],"technology_tags":["LLM Security","Type Systems","Secure Code Generation"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:53:28.949749Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ff7d91cf23a4","title":"A GPU-Accelerated Matrix-Free FAS Multigrid Solver for Navier","content":"arXiv:2510.11152v1 Announce Type: new Abstract: We develop a matrix-free Full Approximation Storage (FAS) multigrid solver based on staggered finite differences and implemented on GPU in MATLAB. To enhance performance, intermediate variables are reused, and an X-shape Multi-Color Gauss-Seidel (X-MCGS) smoother is introduced, which eliminates conditional branching by partitioning the grid into four submatrices. Restriction and prolongation operators are also GPU-accelerated. Convergence tests verify robustness and accuracy, while benchmarks show substantial speedups: for the 2D heat equation on an $8192^2$ grid, the RTX~4090 achieves $61\\times$ over a single-core CPU, and in 3D at $512^3$, $46\\times$. A memory-efficient implementation of first- and second-order projection schemes reduces GPU-resident variables from 12/15 to 8, lowering memory footprint and improving performance by 20--30%, enabling $512^3$ Navier-Stokes simulations on a single GPU. Grain growth on a $512^2$ grid accommodates up to $q=1189$ (2D) and $q=123$ (3D) orientations, reproducing expected scaling laws. Coupled with Cahn-Hilliard equations, air-water two-bubble coalescence is simulated on a $256\\times 256\\times 1024$ grid, agreeing with experimental observations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11152","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.889729","language":"en","tags":["computer-science","preprints","csna","mathna","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":164,"author":"Jiale Meng, Shuqi Tang, Steven M. Wise, Zhenlin Guo","raw_content_length":1255,"priority":7,"update_frequency":1,"reading_time_minutes":0.82,"robust_parsing_used":true,"entities":{"organizations":["FAS","CPU","GPU","Matrix-Free FAS Multigrid Solver","MATLAB","Multi-Color Gauss-Seidel","Navier"],"persons":["Navier-Stokes"],"locations":["Convergence"],"monetary":["61\\times$","3$, $","8192","46\\times$.","512"]},"char_count":1254,"language_detected":"en","key_concepts":{"key_phrases":["A GPU-Accelerated Matrix-Free FAS Multigrid Solver","Navier","arXiv251011152v1 Announce Type","new Abstract","staggered finite differences","GPU","MATLAB","performance","intermediate variables","MCGS"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"A GPU-Accelerated Matrix-Free FAS Multigrid Solver":2.0,"Navier":2.0,"arXiv251011152v1 Announce Type":1.0,"new Abstract":1.0,"staggered finite differences":1.0,"GPU":1.0,"MATLAB":1.0,"performance":1.0,"intermediate variables":1.0,"MCGS":1.0}},"age_hours":2.756875206388889,"is_recent":true,"quality_score":1.0,"sentiment_score":3.634,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2732,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9051,"joy":0.0243,"surprise":0.049,"sadness":0.0046,"fear":0.0026,"anger":0.0086,"disgust":0.0059},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a GPU-accelerated solver for Navier-Stokes equations, demonstrating significant speedups compared to CPU implementations. While the solver itself doesn't directly reduce GHG emissions, it improves the efficiency of simulations used in various fields, including climate modeling and engineering design, potentially leading to more efficient designs and resource utilization. The work is peer-reviewed and includes performance metrics, but it remains in the applied research stage with no real-world deployments yet.","key_impact_metrics":["61x speedup over single-core CPU","20-30% performance improvement"],"technology_tags":["GPU computing","Computational Fluid Dynamics","Multigrid solver"],"sdg_alignment":[7,9],"analyzed_at":"2025-10-29T11:53:32.123520Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4490c1b8ccea","title":"One Size Does Not Fit All: Exploring Variable Thresholds for Distance","content":"arXiv:2510.11160v1 Announce Type: new Abstract: Distance-based unsupervised text classification is a method within text classification that leverages the semantic similarity between a label and a text to determine label relevance. This method provides numerous benefits, including fast inference and adaptability to expanding label sets, as opposed to zero-shot, few-shot, and fine-tuned neural networks that require re-training in such cases. In multi-label distance-based classification and information retrieval algorithms, thresholds are required to determine whether a text instance is \"similar\" to a label or query. Similarity between a text and label is determined in a dense embedding space, usually generated by state-of-the-art sentence encoders. Multi-label classification complicates matters, as a text instance can have multiple true labels, unlike in multi-class or binary classification, where each instance is assigned only one label. We expand upon previous literature on this underexplored topic by thoroughly examining and evaluating the ability of sentence encoders to perform distance-based classification. First, we perform an exploratory study to verify whether the semantic relationships between texts and labels vary across models, datasets, and label sets by conducting experiments on a diverse collection of realistic multi-label text classification (MLTC) datasets. We find that similarity distributions show statistically significant differences across models, datasets and even label sets. We propose a novel method for optimizing label-specific thresholds using a validation set. Our label-specific thresholding method achieves an average improvement of 46% over normalized 0.5 thresholding and outperforms uniform thresholding approaches from previous work by an average of 14%. Additionally, the method demonstrates strong performance even with limited labeled examples.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11160","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.890161","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":256,"author":"Jens Van Nooten, Andriy Kosar, Guy De Pauw, Walter Daelemans","raw_content_length":1904,"priority":7,"update_frequency":1,"reading_time_minutes":1.28,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1903,"language_detected":"en","key_concepts":{"key_phrases":["One Size","All","Variable Thresholds","Distance","arXiv251011160v1","Announce Type","new Abstract","Distance-based unsupervised text classification","a method","text classification"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"One Size":2.0,"All":2.0,"Variable Thresholds":2.0,"Distance":2.0,"arXiv251011160v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Distance-based unsupervised text classification":1.0,"a method":1.0,"text classification":1.0}},"age_hours":2.756891331944445,"is_recent":true,"quality_score":0.7,"sentiment_score":5.6274999999999995,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1255,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8989,"joy":0.0195,"surprise":0.0567,"sadness":0.0058,"fear":0.0052,"anger":0.0098,"disgust":0.0042},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method for optimizing label-specific thresholds in distance-based text classification, achieving a 46% improvement over normalized thresholding. While promising, this is still in the applied research phase with no deployed units or customer contracts. The impact on climate change is indirect, potentially improving the efficiency of information retrieval related to climate solutions.","key_impact_metrics":["46% improvement over normalized 0.5 thresholding","14% outperforms uniform thresholding"],"technology_tags":["text classification","machine learning","natural language processing"],"sdg_alignment":[9,13],"analyzed_at":"2025-10-29T11:53:35.077723Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_547b68797d82","title":"Emergence of hybrid computational dynamics through reinforcement learning","content":"arXiv:2510.11162v1 Announce Type: new Abstract: Understanding how learning algorithms shape the computational strategies that emerge in neural networks remains a fundamental challenge in machine intelligence. While network architectures receive extensive attention, the role of the learning paradigm itself in determining emergent dynamics remains largely unexplored. Here we demonstrate that reinforcement learning (RL) and supervised learning (SL) drive recurrent neural networks (RNNs) toward fundamentally different computational solutions when trained on identical decision-making tasks. Through systematic dynamical systems analysis, we reveal that RL spontaneously discovers hybrid attractor architectures, combining stable fixed-point attractors for decision maintenance with quasi-periodic attractors for flexible evidence integration. This contrasts sharply with SL, which converges almost exclusively to simpler fixed-point-only solutions. We further show that RL sculpts functionally balanced neural populations through a powerful form of implicit regularization -- a structural signature that enhances robustness and is conspicuously absent in the more heterogeneous solutions found by SL-trained networks. The prevalence of these complex dynamics in RL is controllably modulated by weight initialization and correlates strongly with performance gains, particularly as task complexity increases. Our results establish the learning algorithm as a primary determinant of emergent computation, revealing how reward-based optimization autonomously discovers sophisticated dynamical mechanisms that are less accessible to direct gradient-based optimization. These findings provide both mechanistic insights into neural computation and actionable principles for designing adaptive AI systems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11162","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.890601","language":"en","tags":["cslg","nlinao","preprints","research","q-bionc","csne","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":222,"author":"Roman A. Kononov, Nikita A. Pospelov, Konstantin V. Anokhin, Vladimir V. Nekorkin, Oleg V. Maslennikov","raw_content_length":1800,"priority":7,"update_frequency":1,"reading_time_minutes":1.11,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1799,"language_detected":"en","key_concepts":{"key_phrases":["Emergence","hybrid computational dynamics","reinforcement learning","arXiv251011162v1 Announce Type","new Abstract","algorithms","the computational strategies","neural networks","a fundamental challenge","machine intelligence"],"filter_categories":{"ai_ml":["reinforcement learning","algorithms","neural networks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Emergence":2.0,"hybrid computational dynamics":2.0,"reinforcement learning":2.0,"arXiv251011162v1 Announce Type":1.0,"new Abstract":1.0,"algorithms":1.0,"the computational strategies":1.0,"neural networks":1.0,"a fundamental challenge":1.0,"machine intelligence":1.0}},"age_hours":2.756904876944444,"is_recent":true,"quality_score":1.0,"sentiment_score":9.3125,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8625,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8386,"joy":0.0269,"surprise":0.0767,"sadness":0.0075,"fear":0.0195,"anger":0.0222,"disgust":0.0085},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This article describes research into reinforcement learning and its impact on neural network dynamics. While the research is technically credible and innovative, it's at a very early stage (basic research) and doesn't have any direct, measurable climate impact or economic viability at this point. The potential for future applications in areas like energy optimization is there, but currently theoretical.","key_impact_metrics":[],"technology_tags":["Reinforcement Learning","Neural Networks","Computational Dynamics"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:53:38.915298Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c858b6668f81","title":"Poseidon: A OneGraph Engine","content":"arXiv:2510.11166v1 Announce Type: new Abstract: We present the Poseidon engine behind the Neptune Analytics graph database service. Customers interact with Poseidon using the declarative openCypher query language, which enables requests that seamlessly combine traditional querying paradigms (such as graph pattern matching, variable length paths, aggregation) with algorithm invocations and has been syntactically extended to facilitate OneGraph interoperability, such as the disambiguation between globally unique IRIs (as exposed via RDF) vs. local identifiers (as encountered in LPG data). Poseidon supports a broad range of graph workloads, from simple transactions, to top-k beam search algorithms on dynamic graphs, to whole graph analytics requiring multiple full passes over the data. For example, real-time fraud detection, like many other use cases, needs to reflect current committed state of the dynamic graph. If a users cell phone is compromised, then all newer actions by that user become immediately suspect. To address such dynamic graph use cases, Poseidon combines state-of-the-art transaction processing with novel graph data indexing, including lock-free maintenance of adjacency lists, secondary succinct indices, partitioned heaps for data tuple storage with uniform placement, and innovative statistics for cost-based query optimization. The Poseidon engine uses a logical log for durability, enabling rapid evolution of in-memory data structures. Bulk data loads achieve more than 10 million property values per second on many data sets while simple transactions can execute in under 20ms against the storage engine.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11166","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.891385","language":"en","tags":["computer-science","research","csdb","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":230,"author":"Brad Bebee, \\\"Umit V. \\c{C}ataly\\\"urek, Olaf Hartig, Ankesh Khandelwal, Simone Rondelli, Michael Schmidt, Lefteris Sidirourgos, Bryan Thompson","raw_content_length":1643,"priority":7,"update_frequency":1,"reading_time_minutes":1.15,"robust_parsing_used":true,"entities":{"organizations":["RDF","LPG data","OneGraph","Neptune Analytics","OneGraph Engine","Poseidon"],"persons":["Poseidon"],"locations":[],"monetary":[]},"char_count":1642,"language_detected":"en","key_concepts":{"key_phrases":["Poseidon","A OneGraph Engine","arXiv251011166v1 Announce Type","new Abstract","the Poseidon engine","the Neptune Analytics graph database service","Customers","the declarative","openCypher query language","which"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Poseidon":3.0,"A OneGraph Engine":2.0,"arXiv251011166v1 Announce Type":1.0,"new Abstract":1.0,"the Poseidon engine":1.0,"the Neptune Analytics graph database service":1.0,"Customers":1.0,"the declarative":1.0,"openCypher query language":1.0,"which":1.0}},"age_hours":2.7569350183333334,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8982,"joy":0.0419,"surprise":0.0457,"sadness":0.0031,"fear":0.0029,"anger":0.0051,"disgust":0.0031},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":4,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a new graph database engine, Poseidon, and its capabilities. While it mentions performance metrics like 10 million property values per second for bulk data loads and 20ms transaction times, it doesn't directly link to reduced GHG emissions or climate adaptation. It's still in the applied research phase, with no evidence of real-world deployment or customer contracts.","key_impact_metrics":["10 million property values per second","20ms transaction times"],"technology_tags":["graph database","data indexing","query optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:53:42.316282Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b7bc11ce5f89","title":"Bridging Gaps in Hate Speech Detection: Meta","content":"arXiv:2510.11167v1 Announce Type: new Abstract: Hate speech poses a serious threat to social cohesion and individual well-being, particularly on social media, where it spreads rapidly. While research on hate speech detection has progressed, it remains largely focused on English, resulting in limited resources and benchmarks for low-resource languages. Moreover, many of these languages have multiple linguistic varieties, a factor often overlooked in current approaches. At the same time, large language models require substantial amounts of data to perform reliably, a requirement that low-resource languages often cannot meet. In this work, we address these gaps by compiling a meta-collection of hate speech datasets for European Spanish, standardised with unified labels and metadata. This collection is based on a systematic analysis and integration of existing resources, aiming to bridge the data gap and support more consistent and scalable hate speech detection. We extended this collection by translating it into European Portuguese and into a Galician standard that is more convergent with Spanish and another Galician variant that is more convergent with Portuguese, creating aligned multilingual corpora. Using these resources, we establish new benchmarks for hate speech detection in Iberian languages. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, providing baseline results for future research. Moreover, we perform a cross-lingual analysis with our target languages. Our findings underscore the importance of multilingual and variety-aware approaches in hate speech detection and offer a foundation for improved benchmarking in underrepresented European languages.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11167","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.891855","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":241,"author":"Paloma Piot, Jos\\'e Ramom Pichel Campos, Javier Parapar","raw_content_length":1739,"priority":7,"update_frequency":1,"reading_time_minutes":1.205,"robust_parsing_used":true,"entities":{"organizations":["Meta arXiv:2510.11167v1"],"persons":["metadata"],"locations":[],"monetary":[]},"char_count":1738,"language_detected":"en","key_concepts":{"key_phrases":["Gaps","Hate Speech Detection","arXiv251011167v1","Announce Type","new Abstract","Hate speech","a serious threat","social cohesion","individual well-being","social media"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Gaps":2.0,"Hate Speech Detection":2.0,"arXiv251011167v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Hate speech":1.0,"a serious threat":1.0,"social cohesion":1.0,"individual well-being":1.0,"social media":1.0}},"age_hours":2.7569501813888886,"is_recent":true,"quality_score":1.0,"sentiment_score":0.32550000000000023,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.9349,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.3674,"joy":0.0059,"surprise":0.0177,"sadness":0.0442,"fear":0.4095,"anger":0.1142,"disgust":0.041},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":5,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes the creation of a meta-collection of hate speech datasets for Iberian languages and benchmarks for hate speech detection. While this doesn't directly impact climate change, it addresses social cohesion and well-being, which are important for a just transition. The research is in the applied research phase, with the creation of datasets and benchmarks but no deployment of a functional system.","key_impact_metrics":["New benchmarks for hate speech detection in Iberian languages","Aligned multilingual corpora created"],"technology_tags":["Natural Language Processing","Hate Speech Detection"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:53:45.329420Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_30f1ad7f8f39","title":"ELMO: Efficiency via Low","content":"arXiv:2510.11168v1 Announce Type: new Abstract: Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11168","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.892257","language":"en","tags":["computer-science","cslg","preprints","cscl","csir","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":206,"author":"Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar","raw_content_length":1445,"priority":7,"update_frequency":1,"reading_time_minutes":1.03,"robust_parsing_used":true,"entities":{"organizations":["ELMO"],"persons":[],"locations":["Kahan"],"monetary":[]},"char_count":1444,"language_detected":"en","key_concepts":{"key_phrases":["ELMO","Efficiency","Low","arXiv251011168v1 Announce Type","new Abstract","Large output spaces","Extreme multilabel classification","XMC","a setting","product"],"filter_categories":{"hydrogen_energy":["product"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"ELMO":2.0,"Efficiency":2.0,"Low":2.0,"arXiv251011168v1 Announce Type":1.0,"new Abstract":1.0,"Large output spaces":1.0,"Extreme multilabel classification":1.0,"XMC":1.0,"a setting":1.0,"product":1.0}},"age_hours":2.756966416944444,"is_recent":true,"quality_score":1.0,"sentiment_score":5.258000000000001,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0516,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9019,"joy":0.0123,"surprise":0.0628,"sadness":0.0044,"fear":0.0031,"anger":0.0096,"disgust":0.0059},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new low-precision training framework (ELMO) for extreme multilabel classification models, demonstrating a significant reduction in GPU memory usage (6.6 GiB vs 39.7 GiB) without compromising accuracy. This could lead to more efficient and accessible AI development, indirectly reducing energy consumption associated with training large models. The research is peer-reviewed, increasing its credibility, but it's still in the applied research stage with no deployment data.","key_impact_metrics":["GPU memory usage reduction: 39.7 GiB to 6.6 GiB"],"technology_tags":["low-precision training","extreme multilabel classification","BFloat16","Float8"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T11:53:49.792815Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_720db3b44d31","title":"EAGER: Entropy","content":"arXiv:2510.11170v1 Announce Type: new Abstract: With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt. This enables exploration of different reasoning paths toward the correct solution, however, allocates the same compute budget for each prompt. Grounded on the assumption that different prompts carry different degrees of complexity, and thus different computation needs, we propose EAGer, a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and concurrently improve overall performance. EAGer allows branching to multiple reasoning paths only in the presence of high-entropy tokens, and then reallocates the saved compute budget to the instances where exploration of alternative paths is most needed. We find that across multiple open-source models on complex reasoning benchmarks such as AIME 2025, EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, EAGer generates up to 65% fewer tokens (hence saving compute) and achieves up to 37% improvement in Pass@k compared to the Full Parallel Sampling.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11170","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.892820","language":"en","tags":["computer-science","cslg","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet \\\"Ust\\\"un","raw_content_length":1399,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":["EAGer"],"persons":[],"locations":[],"monetary":[]},"char_count":1398,"language_detected":"en","key_concepts":{"key_phrases":["EAGER Entropy","new Abstract","the rise","reasoning language models","test-time scaling methods","a paradigm","model performance","substantial computation","multiple candidate sequences","the same prompt"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"EAGER Entropy":2.0,"new Abstract":1.0,"the rise":1.0,"reasoning language models":1.0,"test-time scaling methods":1.0,"a paradigm":1.0,"model performance":1.0,"substantial computation":1.0,"multiple candidate sequences":1.0,"the same prompt":1.0}},"age_hours":2.7569807783333333,"is_recent":true,"quality_score":1.0,"sentiment_score":9.379999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.876,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8981,"joy":0.0084,"surprise":0.0667,"sadness":0.0054,"fear":0.0056,"anger":0.0102,"disgust":0.0057},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel, training-free method (EAGer) to reduce computation in reasoning language models by reallocating compute budget based on token-wise entropy. It achieves up to 65% fewer tokens generated and up to 37% improvement in Pass@k compared to full parallel sampling. This is currently in the research phase, with no deployed units or real-world data, but the potential for reducing energy consumption in large language models is significant.","key_impact_metrics":["65% fewer tokens generated","37% improvement in Pass@k"],"technology_tags":["Artificial Intelligence","Energy Efficiency","Language Models"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-29T11:53:53.594828Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c76d9552a43a","title":"Multiview Manifold Evidential Fusion for PolSAR Image Classification","content":"arXiv:2510.11171v1 Announce Type: new Abstract: Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their extracted multi-features - such as scattering angle, entropy, texture, and boundary descriptors - provide complementary and physically interpretable information for image classification. Traditional fusion strategies typically concatenate these features or employ deep learning networks to combine them. However, the covariance matrices and multi-features, as two complementary views, lie on different manifolds with distinct geometric structures. Existing fusion methods also overlook the varying importance of different views and ignore uncertainty, often leading to unreliable predictions. To address these issues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to effectively fuse these two views. It gives a new framework to integrate PolSAR manifold learning and evidence fusion into a unified architecture. Specifically, covariance matrices are represented on the Hermitian Positive Definite (HPD) manifold, while multi-features are modeled on the Grassmann manifold. Two different kernel metric learning networks are constructed to learn their manifold representations. Subsequently, a trusted multiview evidence fusion, replacing the conventional softmax classifier, estimates belief mass and quantifies the uncertainty of each view from the learned deep features. Finally, a Dempster-Shafer theory-based fusion strategy combines evidence, enabling a more reliable and interpretable classification. Extensive experiments on three real-world PolSAR datasets demonstrate that the proposed method consistently outperforms existing approaches in accuracy, robustness, and interpretability.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11171","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.893230","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":219,"author":"Junfei Shi, Haojia Zhang, Haiyan Jin, Junhuai Li, Xiaogang Song, Yuanfan Guo, Haonan Su, Weisi Lin","raw_content_length":1740,"priority":7,"update_frequency":1,"reading_time_minutes":1.095,"robust_parsing_used":true,"entities":{"organizations":["Multiview Manifold Evidential Fusion for PolSAR Image Classification arXiv:2510.11171v1 Announce Type:","Multiview Manifold Evidential Fusion"],"persons":[],"locations":[],"monetary":[]},"char_count":1739,"language_detected":"en","key_concepts":{"key_phrases":["Multiview Manifold Evidential Fusion","PolSAR Image Classification","features","arXiv251011171v1 Announce Type","new Abstract","PolSAR","their extracted multi","angle","entropy","texture"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Multiview Manifold Evidential Fusion":2.0,"PolSAR Image Classification":2.0,"features":2.0,"arXiv251011171v1 Announce Type":1.0,"new Abstract":1.0,"PolSAR":1.0,"their extracted multi":1.0,"angle":1.0,"entropy":1.0,"texture":1.0}},"age_hours":2.7569947150000003,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8828,"joy":0.0133,"surprise":0.0183,"sadness":0.0067,"fear":0.0296,"anger":0.0331,"disgust":0.0163},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a new method (MMEFnet) for PolSAR image classification, which could potentially improve the accuracy and interpretability of remote sensing data. The method is validated on real-world PolSAR datasets and demonstrates improved accuracy compared to existing approaches. However, it is still in the research phase with no clear path to deployment or economic viability, and the climate impact is indirect (improved monitoring).","key_impact_metrics":["accuracy improvement","robustness improvement"],"technology_tags":["PolSAR","image classification","manifold learning","evidence fusion"],"sdg_alignment":[9,13,15],"analyzed_at":"2025-10-29T11:53:56.716100Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_0ddf929573b0","title":"CoPRS: Learning Positional Prior from Chain","content":"arXiv:2510.11173v1 Announce Type: new Abstract: Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above prior state of the art across both validation and test partitions. Extensive experiments reveal that the quality of the heatmap strongly influences the resulting mask quality, supporting a consistent association between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and predicting masks more precisely. Code, checkpoints and logs are released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11173","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.893671","language":"en","tags":["computer-science","preprints","cscv","csmm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":229,"author":"Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang","raw_content_length":1699,"priority":7,"update_frequency":1,"reading_time_minutes":1.145,"robust_parsing_used":true,"entities":{"organizations":["Learning Positional Prior","Chain arXiv:2510.11173v1 Announce Type"],"persons":[],"locations":["CoPRS"],"monetary":[]},"char_count":1698,"language_detected":"en","key_concepts":{"key_phrases":["CoPRS","Chain","Announce Type","new Abstract","Existing works","reasoning segmentation","hidden features","a language model","a mask decoder","positions"],"filter_categories":{"ai_ml":["Chain"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CoPRS":3.0,"Chain":2.0,"Announce Type":1.0,"new Abstract":1.0,"Existing works":1.0,"reasoning segmentation":1.0,"hidden features":1.0,"a language model":1.0,"a mask decoder":1.0,"positions":1.0}},"age_hours":2.7570093622222225,"is_recent":true,"quality_score":1.0,"sentiment_score":6.0115,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.898,"joy":0.0086,"surprise":0.0501,"sadness":0.0088,"fear":0.0077,"anger":0.0183,"disgust":0.0085},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the interpretability and accuracy of reasoning segmentation models using a Multi-modal Chain-of-Thought approach. While the research demonstrates improved performance on benchmark datasets, it is currently in the basic research stage with no clear connection to direct climate impact or deployment. The code and checkpoints are released, suggesting potential for further development, but it remains a concept with no deployed technology.","key_impact_metrics":["Performance at or above prior state of the art across both validation and test partitions"],"technology_tags":["Multi-modal Chain-of-Thought","Positional Perception Model","Segmentation"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:54:00.004984Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4513e3378dcd","title":"Reliable Cross","content":"arXiv:2510.11175v1 Announce Type: new Abstract: Cross-modal alignment is an important multi-modal task, aiming to bridge the semantic gap between different modalities. The most reliable fundamention for achieving this objective lies in the semantic consistency between matched pairs. Conventional methods implicitly assume embeddings contain solely semantic information, ignoring the impact of non-semantic information during alignment, which inevitably leads to information bias or even loss. These non-semantic information primarily manifest as stylistic variations in the data, which we formally define as style information. An intuitive approach is to separate style from semantics, aligning only the semantic information. However, most existing methods distinguish them based on feature columns, which cannot represent the complex coupling relationship between semantic and style information. In this paper, we propose PICO, a novel framework for suppressing style interference during embedding interaction. Specifically, we quantify the probability of each feature column representing semantic information, and regard it as the weight during the embedding interaction. To ensure the reliability of the semantic probability, we propose a prototype iterative construction method. The key operation of this method is a performance feedback-based weighting function, and we have theoretically proven that the function can assign higher weight to prototypes that bring higher performance improvements. Extensive experiments on various benchmarks and model backbones demonstrate the superiority of PICO, outperforming state-of-the-art methods by 5.2\\%-14.1\\%.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11175","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.894087","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":219,"author":"Xiang Ma, Litian Xu, Lexin Fang, Caiming Zhang, Lizhen Cui","raw_content_length":1660,"priority":7,"update_frequency":1,"reading_time_minutes":1.095,"robust_parsing_used":true,"entities":{"organizations":["Reliable Cross arXiv:2510.11175v1 Announce Type: new Abstract","PICO"],"persons":[],"locations":[],"monetary":[]},"char_count":1659,"language_detected":"en","key_concepts":{"key_phrases":["Reliable Cross","arXiv251011175v1 Announce Type","new Abstract","Cross-modal alignment","an important multi-modal task","the semantic gap","different modalities","The most reliable fundamention","this objective","the semantic consistency"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Reliable Cross":2.0,"arXiv251011175v1 Announce Type":1.0,"new Abstract":1.0,"Cross-modal alignment":1.0,"an important multi-modal task":1.0,"the semantic gap":1.0,"different modalities":1.0,"The most reliable fundamention":1.0,"this objective":1.0,"the semantic consistency":1.0}},"age_hours":2.7570234375,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2469999999999999,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7506,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9088,"joy":0.0057,"surprise":0.0097,"sadness":0.0084,"fear":0.03,"anger":0.0183,"disgust":0.0191},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel framework (PICO) for improving cross-modal alignment by suppressing style interference. The concrete action is the proposed prototype iterative construction method and performance feedback-based weighting function. The evidence supporting the claims is the experimental results showing a 5.2%-14.1% improvement over state-of-the-art methods, but it is still in the research phase with no deployment.","key_impact_metrics":["5.2%-14.1% performance improvement"],"technology_tags":["cross-modal alignment","machine learning","artificial intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:03.594537Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_327cf1d7bf26","title":"G2L:From Giga-Scale to Cancer","content":"arXiv:2510.11176v1 Announce Type: new Abstract: Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11176","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.894510","language":"en","tags":["computer-science","csai","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":219,"author":"Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin","raw_content_length":1634,"priority":7,"update_frequency":1,"reading_time_minutes":1.095,"robust_parsing_used":true,"entities":{"organizations":["Giga-Scale to Cancer arXiv:2510.11176v1 Announce Type"],"persons":[],"locations":[],"monetary":["only $15\\%$"]},"char_count":1633,"language_detected":"en","key_concepts":{"key_phrases":["Giga-Scale","Cancer","cancer types","arXiv251011176v1 Announce Type","new Abstract","Recent studies","pathology foundation models","training data","their performance","giga-scale foundation models"],"filter_categories":{"ai_ml":["training data"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Giga-Scale":2.0,"Cancer":2.0,"cancer types":2.0,"arXiv251011176v1 Announce Type":1.0,"new Abstract":1.0,"Recent studies":1.0,"pathology foundation models":1.0,"training data":1.0,"their performance":1.0,"giga-scale foundation models":1.0}},"age_hours":2.7570383741666666,"is_recent":true,"quality_score":1.0,"sentiment_score":0.5964999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8807,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8342,"joy":0.0217,"surprise":0.0626,"sadness":0.0164,"fear":0.0397,"anger":0.0206,"disgust":0.005},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel knowledge distillation strategy (G2L) to reduce the computational cost of large-scale pathology foundation models. While it shows improved performance and robustness compared to other models, it is still in the applied research stage with no mention of real-world deployment. The impact on climate is indirect, through potential energy savings from reduced computational demands.","key_impact_metrics":["15% reduction in parameters","1K pathology slides used"],"technology_tags":["knowledge distillation","pathology foundation models","machine learning"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:54:07.509848Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9ae9fe71a130","title":"BLEnD","content":"arXiv:2510.11178v1 Announce Type: new Abstract: As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant (Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11178","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.894914","language":"en","tags":["computer-science","cscy","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee","raw_content_length":1565,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":["VQA","MCQ"],"persons":[],"locations":[],"monetary":[]},"char_count":1564,"language_detected":"en","key_concepts":{"key_phrases":["VLMs","BLEnD","arXiv251011178v1 Announce Type","new Abstract","vision-language models","their ability","culturally situated knowledge","existing evaluations","static recall","isolated visual grounding"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"VLMs":3.0,"BLEnD":2.0,"arXiv251011178v1 Announce Type":1.0,"new Abstract":1.0,"vision-language models":1.0,"their ability":1.0,"culturally situated knowledge":1.0,"existing evaluations":1.0,"static recall":1.0,"isolated visual grounding":1.0}},"age_hours":2.7570533325,"is_recent":true,"quality_score":0.7,"sentiment_score":6.7,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.34,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9104,"joy":0.0119,"surprise":0.0513,"sadness":0.0034,"fear":0.0071,"anger":0.0118,"disgust":0.004},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":5,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":true,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper introduces a benchmark (BLEnD-Vis) to evaluate the cultural understanding of vision-language models (VLMs). While it doesn't directly address climate change, it aims to improve the robustness and fairness of AI systems, which can indirectly contribute to sustainability by ensuring equitable access to information and resources. The benchmark is validated through human annotation, increasing its credibility.","key_impact_metrics":["4,916 images","over 21,000 multiple-choice question instances"],"technology_tags":["vision-language models","multicultural benchmark"],"sdg_alignment":[4,10,16],"analyzed_at":"2025-10-29T11:54:10.616874Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e9fb997c55d5","title":"Utilizing Bayesian Optimization for Timetable","content":"arXiv:2510.11181v1 Announce Type: new Abstract: The efficiency of railway infrastructure is significantly influenced by the mix of trains that utilize it, as different service types have competing operational requirements. While freight services might require extended service times, passenger services demand more predictable schedules. Traditional methods for addressing long-term traffic assignment problems often rely on fixed-value capacity limitations, determined based on specific assumptions about traffic composition. This paper introduces a methodology for determining timetable-independent capacity within the traffic rate assignment problem, enabling the calculation of junction capacities under dynamic traffic distributions. We solve the underlying non-linear constrained optimization problem maximizing the traffic throughput using Bayesian optimization (BO). This setting combines a known objective function with expensive- to-compute capacity constraints, motivating an adaption of standard BO problems, where objective functions are usually unknown. We tailor the acquisition process in BO to this specific setting and increase performance by incorporating prior knowledge about the shape of the constraint functions into the Gaussian process surrogate model. Our derived approaches are benchmarked on a railway junction near Paris, significantly outperforming fixed traffic composition models and highlighting the benefits of dynamic capacity allocation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11181","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.895687","language":"en","tags":["eesssy","cssy","preprints","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":187,"author":"Tamme Emunds, Paul Brunzema, Sebastian Trimpe, Nils Nie{\\ss}en","raw_content_length":1474,"priority":7,"update_frequency":1,"reading_time_minutes":0.935,"robust_parsing_used":true,"entities":{"organizations":["Utilizing Bayesian Optimization for Timetable arXiv:2510.11181v1 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1473,"language_detected":"en","key_concepts":{"key_phrases":["Bayesian Optimization","Timetable","arXiv251011181v1 Announce Type","new Abstract","The efficiency","railway infrastructure","the mix","trains","different service types","operational requirements"],"filter_categories":{"ai_ml":["railway infrastructure"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Bayesian Optimization":2.0,"Timetable":2.0,"arXiv251011181v1 Announce Type":1.0,"new Abstract":1.0,"The efficiency":1.0,"railway infrastructure":1.0,"the mix":1.0,"trains":1.0,"different service types":1.0,"operational requirements":1.0}},"age_hours":2.757081366388889,"is_recent":true,"quality_score":1.0,"sentiment_score":6.1315,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2263,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9039,"joy":0.0087,"surprise":0.0558,"sadness":0.0065,"fear":0.0068,"anger":0.0123,"disgust":0.006},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":4,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a methodology using Bayesian optimization to improve railway traffic throughput by dynamically allocating junction capacities. The approach is benchmarked on a railway junction near Paris, outperforming fixed traffic composition models. While promising, it's still in the applied research phase, lacking deployment data and independent verification, hence the vaporware flag.","key_impact_metrics":["Significantly outperforming fixed traffic composition models"],"technology_tags":["Bayesian Optimization","Railway Optimization","Traffic Management"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-29T11:54:13.740551Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e6eff820c981","title":"Can Tool","content":"arXiv:2510.11184v1 Announce Type: new Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains underexplored. In this work, we investigate the cross-domain generalization of an LLM agent equipped with a code interpreter tool, which is exclusively trained on mathematical problem-solving tasks. Despite the restricted training domain, we evaluate the agent's performance across several distinct reasoning domains. The results reveal that RL-based tool usage learned from mathematical tasks can be effectively transferred to complex tasks in other domains, enabling great task performance and high token efficiency. To facilitate this cross-domain transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to promote domain-agnostic learning and skill migration, encompassing: (i) a standardized tool interface that abstracts domain-specific nuances through consistent formatting and explicit termination, fostering transferable invocation patterns; (ii) a dual-component reward system that decomposes rewards to incentivize generalizable behaviors like tool efficiency and reasoning abstraction, ensuring alignment and robustness across domain shifts; and (iii) an XML-based prompt template that separates thinking, tool calls, and responses to encourage modular, domain-invariant planning and coherent multi-turn interactions. Extensive experiments across diverse benchmarks validate our approach, achieving state-of-the-art performance and highlighting the cross-domain potential of Tool RL for LLM reasoning.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11184","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.896469","language":"en","tags":["computer-science","cslg","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":215,"author":"Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang","raw_content_length":1728,"priority":7,"update_frequency":1,"reading_time_minutes":1.075,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1727,"language_detected":"en","key_concepts":{"key_phrases":["Can Tool","Announce Type","new Abstract","Recent advances","large language models","LLMs","remarkable capabilities","reasoning and tool utilization","the generalization","tool-augmented reinforcement learning"],"filter_categories":{"ai_ml":["large language models","tool-augmented reinforcement learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Can Tool":2.0,"Announce Type":1.0,"new Abstract":1.0,"Recent advances":1.0,"large language models":1.0,"LLMs":1.0,"remarkable capabilities":1.0,"reasoning and tool utilization":1.0,"the generalization":1.0,"tool-augmented reinforcement learning":1.0}},"age_hours":2.7571115080555555,"is_recent":true,"quality_score":1.0,"sentiment_score":8.494,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6988,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8384,"joy":0.0261,"surprise":0.0947,"sadness":0.0055,"fear":0.0107,"anger":0.0143,"disgust":0.0103},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework (TGRL) for improving cross-domain generalization of LLMs using tool-augmented reinforcement learning. While the framework shows promise in improving task performance and token efficiency, it is still in the early stages of development and lacks real-world deployment data. The impact on sustainability is indirect, potentially improving efficiency in tasks that could contribute to sustainability efforts.","key_impact_metrics":["High token efficiency","State-of-the-art performance"],"technology_tags":["Large Language Models","Reinforcement Learning","Code Interpreter"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:54:17.153793Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1b2fe87e5565","title":"Principles of Safe AI Companions for Youth: Parent and Expert Perspectives","content":"arXiv:2510.11185v1 Announce Type: new Abstract: AI companions are increasingly popular among teenagers, yet current platforms lack safeguards to address developmental risks and harmful normalization. Despite growing concerns, little is known about how parents and developmental psychology experts assess these interactions or what protections they consider necessary. We conducted 26 semi structured interviews with parents and experts, who reviewed real world youth GenAI companion conversation snippets. We found that stakeholders assessed risks contextually, attending to factors such as youth maturity, AI character age, and how AI characters modeled values and norms. We also identified distinct logics of assessment: parents flagged single events, such as a mention of suicide or flirtation, as high risk, whereas experts looked for patterns over time, such as repeated references to self harm or sustained dependence. Both groups proposed interventions, with parents favoring broader oversight and experts preferring cautious, crisis-only escalation paired with youth facing safeguards. These findings provide directions for embedding safety into AI companion design.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11185","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.896872","language":"en","tags":["preprints","research","computer-science","cshc","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":160,"author":"Yaman Yu,  Mohi, Aishi Debroy, Xin Cao, Karen Rudolph, Yang Wang","raw_content_length":1175,"priority":7,"update_frequency":1,"reading_time_minutes":0.8,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1174,"language_detected":"en","key_concepts":{"key_phrases":["Principles","Safe AI Companions","Youth","Parent and Expert Perspectives","parents","new Abstract","AI companions","teenagers","current platforms","safeguards"],"filter_categories":{"ai_ml":["Safe AI Companions"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Principles":2.0,"Safe AI Companions":2.0,"Youth":2.0,"Parent and Expert Perspectives":2.0,"parents":2.0,"new Abstract":1.0,"AI companions":1.0,"teenagers":1.0,"current platforms":1.0,"safeguards":1.0}},"age_hours":2.7571265713888886,"is_recent":true,"quality_score":0.7,"sentiment_score":7.4544999999999995,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4909,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8064,"joy":0.0178,"surprise":0.0162,"sadness":0.0232,"fear":0.0737,"anger":0.0378,"disgust":0.0249},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":2,"systemic_impact":2,"justice_equity":5,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper focuses on identifying risks associated with AI companions for youth and proposes interventions. It is based on interviews and analysis of conversation snippets, but it does not involve any deployed technology or measurable environmental outcomes. The research is in an early stage, focusing on identifying potential harms and suggesting design improvements.","key_impact_metrics":[],"technology_tags":["AI","GenAI","AI Safety"],"sdg_alignment":[3,4,16],"analyzed_at":"2025-10-29T11:54:20.555390Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1d42fe1e2037","title":"FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models","content":"arXiv:2510.11190v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) face an inherent trade-off between faithfulness and creativity, as different tasks require varying degrees of associative reasoning. However, existing methods lack the flexibility to modulate this reasoning strength, limiting MLLMs' adaptability across factual and creative scenarios. To bridge this gap, we propose equipping MLLMs with mechanisms that enable flexible control over associative reasoning. We begin by investigating the internal mechanisms underlying associative behavior in MLLMs and find that: (1) middle layers play a pivotal role in shaping model's associative tendencies, (2) modifying representations in these layers effectively regulates associative reasoning strength, and (3) hallucinations can be exploited to derive steering vectors that guide this modulation. Building on these findings, we introduce Flexible Association Control (FlexAC), a lightweight and training-free framework for modulating associative behavior in MLLMs. FlexAC first induces hallucination-guided intermediate representations to encode associative directions. Then, it selects high-association instances to construct effective associative steering vectors, whose strengths are adaptively calibrated to balance creative guidance with output stability. Finally, recognizing the multi-dimensional nature of associative reasoning, FlexAC incorporates task-specific associative vectors derived from a forward pass on a few target-domain samples, enabling models to follow diverse associative directions and better adapt to creative tasks. Notably, our method achieves up to a 5.8x improvement in creativity on Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing existing baselines and demonstrating its effectiveness in enabling flexible control over associative reasoning in MLLMs. Our code is available at https://github.com/ylhz/FlexAC.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11190","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.898345","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":249,"author":"Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao","raw_content_length":1952,"priority":7,"update_frequency":1,"reading_time_minutes":1.245,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1951,"language_detected":"en","key_concepts":{"key_phrases":["FlexAC","Flexible Control","Associative Reasoning","Multimodal Large Language Models","MLLMs","associative reasoning","arXiv251011190v1 Announce Type","new Abstract","Multimodal large language models","an inherent trade-off"],"filter_categories":{"ai_ml":["Multimodal Large Language Models","Multimodal large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"FlexAC":2.0,"Flexible Control":2.0,"Associative Reasoning":2.0,"Multimodal Large Language Models":2.0,"MLLMs":2.0,"associative reasoning":2.0,"arXiv251011190v1 Announce Type":1.0,"new Abstract":1.0,"Multimodal large language models":1.0,"an inherent trade-off":1.0}},"age_hours":2.757172474722222,"is_recent":true,"quality_score":0.7,"sentiment_score":9.559,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9118,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9344,"joy":0.0101,"surprise":0.019,"sadness":0.0061,"fear":0.0142,"anger":0.0114,"disgust":0.005},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (FlexAC) for improving the creativity and reducing hallucinations in multimodal large language models. While the reported improvements (5.8x creativity, 29% hallucination reduction) are promising, this is currently at the research stage with no deployed applications. The impact on climate is indirect, relying on potential future applications of improved AI.","key_impact_metrics":["5.8x improvement in creativity","29% reduction in hallucination rate"],"technology_tags":["Multimodal Large Language Models","Associative Reasoning","Artificial Intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:23.733323Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_359d2f072e13","title":"Efficient In","content":"arXiv:2510.11192v1 Announce Type: new Abstract: Structured sparsity enables deploying large language models (LLMs) on resource-constrained systems. Approaches like dense-to-sparse fine-tuning are particularly compelling, achieving remarkable structured sparsity by reducing the model size by over 6.7x, while still maintaining acceptable accuracy. Despite this reduction, LLM inference, especially the decode stage being inherently memory-bound, is extremely expensive on conventional Von-Neumann architectures. Compute-in-memory (CIM) architectures mitigate this by performing computations directly in memory, and when paired with sparse LLMs, enable storing and computing the entire model in memory, eliminating the data movement on the off-chip bus and improving efficiency. Nonetheless, naively mapping sparse matrices onto CIM arrays leads to poor array utilization and diminished computational efficiency. In this paper, we present an automated framework with novel mapping and scheduling strategies to accelerate sparse LLM inference on CIM accelerators. By exploiting block-diagonal sparsity, our approach improves CIM array utilization by over 50%, achieving more than 4x reduction in both memory footprint and the number of required floating-point operations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11192","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.898783","language":"en","tags":["computer-science","cslg","csar","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":164,"author":"Jo\\~ao Paulo Cardoso de Lima, Marc Dietrich, Jeronimo Castrillon, Asif Ali Khan","raw_content_length":1270,"priority":7,"update_frequency":1,"reading_time_minutes":0.82,"robust_parsing_used":true,"entities":{"organizations":["Von-Neumann"],"persons":[],"locations":[],"monetary":[]},"char_count":1269,"language_detected":"en","key_concepts":{"key_phrases":["arXiv251011192v1 Announce Type","new Abstract","Structured sparsity","large language models","LLMs","resource-constrained systems","Approaches","dense-to-sparse fine-tuning","remarkable structured sparsity","the model size"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv251011192v1 Announce Type":1.0,"new Abstract":1.0,"Structured sparsity":1.0,"large language models":1.0,"LLMs":1.0,"resource-constrained systems":1.0,"Approaches":1.0,"dense-to-sparse fine-tuning":1.0,"remarkable structured sparsity":1.0,"the model size":1.0}},"age_hours":2.7571872755555553,"is_recent":true,"quality_score":1.0,"sentiment_score":9.564,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9128,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7572,"joy":0.0646,"surprise":0.1255,"sadness":0.0071,"fear":0.0124,"anger":0.0219,"disgust":0.0113},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper presents a framework to accelerate sparse LLM inference on CIM accelerators, improving CIM array utilization by over 50% and achieving more than 4x reduction in both memory footprint and the number of required floating-point operations. This is currently at the applied research stage, with no mention of deployed units or customer contracts, but it does present quantifiable metrics and is published on arXiv, suggesting peer review.","key_impact_metrics":["4x reduction in memory footprint","4x reduction in floating-point operations"],"technology_tags":["compute-in-memory","sparse large language models"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:26.925057Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_da873adc5661","title":"Aligning Deep Implicit Preferences by Learning to Reason Defensively","content":"arXiv:2510.11194v1 Announce Type: new Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs) to engage effectively in user-centric interactions. However, current methods face a dual challenge: they fail to infer users' deep implicit preferences (including unstated goals, semantic context and risk tolerances), and they lack the defensive reasoning required to navigate real-world ambiguity. This cognitive gap leads to responses that are superficial, brittle and short-sighted. To address this, we propose Critique-Driven Reasoning Alignment (CDRA), which reframes alignment from a scalar reward-matching task into a structured reasoning process. First, to bridge the preference inference gap, we introduce the DeepPref benchmark. This dataset, comprising 3000 preference-query pairs across 20 topics, is curated by simulating a multi-faceted cognitive council that produces critique-annotated reasoning chains to deconstruct query semantics and reveal latent risks. Second, to instill defensive reasoning, we introduce the Personalized Generative Process Reward Model (Pers-GenPRM), which frames reward modeling as a personalized reasoning task. It generates a critique chain to evaluate a response's alignment with user preferences before outputting a final score based on this rationale. Ultimately, this interpretable, structured reward signal guides policy model through Critique-Driven Policy Alignment, a process-level online reinforcement learning algorithm integrating both numerical and natural language feedback. Experiments demonstrate that CDRA excels at discovering and aligning with users' true preferences while executing robust reasoning. Our code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11194","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.899201","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":227,"author":"Peiming Li, Zhiyuan Hu, Yang Tang, Shiyu Li, Xi Chen","raw_content_length":1771,"priority":7,"update_frequency":1,"reading_time_minutes":1.135,"robust_parsing_used":true,"entities":{"organizations":["Critique-Driven Reasoning Alignment"],"persons":["Large Language Models"],"locations":[],"monetary":[]},"char_count":1770,"language_detected":"en","key_concepts":{"key_phrases":["Deep Implicit Preferences","Learning","Reason","arXiv251011194v1 Announce Type","new Abstract","Personalized alignment","Large Language Models","LLMs","user-centric interactions","current methods"],"filter_categories":{"ai_ml":["Learning","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Deep Implicit Preferences":2.0,"Learning":2.0,"Reason":2.0,"arXiv251011194v1 Announce Type":1.0,"new Abstract":1.0,"Personalized alignment":1.0,"Large Language Models":1.0,"LLMs":1.0,"user-centric interactions":1.0,"current methods":1.0}},"age_hours":2.757202830277778,"is_recent":true,"quality_score":1.0,"sentiment_score":3.194,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3612,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7243,"joy":0.003,"surprise":0.0052,"sadness":0.0242,"fear":0.1228,"anger":0.0576,"disgust":0.0629},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new method (CDRA) for aligning LLMs with user preferences. While the research is novel and the DeepPref benchmark provides a concrete dataset, it's still in the early stages of development with no deployed technology or measured outcomes in a real-world setting. The potential climate impact is indirect, relying on future applications of improved LLMs in sustainability-related fields.","key_impact_metrics":["3000 preference-query pairs","20 topics"],"technology_tags":["Large Language Models","Reinforcement Learning","AI Alignment"],"sdg_alignment":[9,17],"analyzed_at":"2025-10-29T11:54:29.970171Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ecb03d22f4f4","title":"Evaluating Reasoning Faithfulness in Medical Vision","content":"arXiv:2510.11196v1 Announce Type: new Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate alignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone ($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality are decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11196","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.900019","language":"en","tags":["computer-science","preprints","cscv","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":180,"author":"Johannes Moll, Markus Graf, Tristan Lemke, Nicolas Lenhart, Daniel Truhn, Jean-Benoit Delbrouck, Jiazhen Pan, Daniel Rueckert, Lisa C. Adams, Keno K. Bressem","raw_content_length":1380,"priority":7,"update_frequency":1,"reading_time_minutes":0.9,"robust_parsing_used":true,"entities":{"organizations":["Kendall","VQA","fidelity","CoT"],"persons":[],"locations":[],"monetary":["$\\tau_b=0.670$","$\\tau_b=0.387$","$\\tau_b=0.091$"]},"char_count":1379,"language_detected":"en","key_concepts":{"key_phrases":["Reasoning Faithfulness","Medical Vision","arXiv251011196v1 Announce Type","new Abstract","Vision-language models","VLMs","thought","the underlying decision process","trust","high-stakes clinical use"],"filter_categories":{"ai_ml":["Reasoning Faithfulness"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Reasoning Faithfulness":2.0,"Medical Vision":2.0,"arXiv251011196v1 Announce Type":1.0,"new Abstract":1.0,"Vision-language models":1.0,"VLMs":1.0,"thought":1.0,"the underlying decision process":1.0,"trust":1.0,"high-stakes clinical use":1.0}},"age_hours":2.757233287777778,"is_recent":true,"quality_score":1.0,"sentiment_score":6.48,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.296,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6948,"joy":0.0052,"surprise":0.0098,"sadness":0.0146,"fear":0.1667,"anger":0.0514,"disgust":0.0574},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":2,"systemic_impact":1,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the faithfulness of AI explanations in medical imaging, specifically chest X-rays. While it aims to improve trust in AI for clinical use, it does not directly address climate change or sustainability. The study involves a reader study with a small sample size (n=4) and benchmarks six VLMs, but there is no deployment of a technology with a measurable environmental outcome.","key_impact_metrics":["Kendall's tau_b=0.670 for attribution","Kendall's tau_b=0.387 for fidelity"],"technology_tags":["Vision-Language Models","Medical Imaging"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:54:33.021185Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_92ce37c69a3a","title":"Evaluating Line","content":"arXiv:2510.11202v1 Announce Type: new Abstract: To address the extremely concerning problem of software vulnerability, system security is often entrusted to Machine Learning (ML) algorithms. Despite their now established detection capabilities, such models are limited by design to flagging the entire input source code function as vulnerable, rather than precisely localizing the concerned code lines. However, the detection granularity is crucial to support human operators during software development, ensuring that such predictions reflect the true code semantics to help debug, evaluate, and fix the detected vulnerabilities. To address this issue, recent work made progress toward improving the detector's localization ability, thus narrowing down the vulnerability detection \"window\" and providing more fine-grained predictions. Such approaches, however, implicitly disregard the presence of spurious correlations and biases in the data, which often predominantly influence the performance of ML algorithms. In this work, we investigate how detectors comply with this requirement by proposing an explainability-based evaluation procedure. Our approach, defined as Detection Alignment (DA), quantifies the agreement between the input source code lines that most influence the prediction and the actual localization of the vulnerability as per the ground truth. Through DA, which is model-agnostic and adaptable to different detection tasks, not limited to our use case, we analyze multiple learning-based vulnerability detectors and datasets. As a result, we show how the predictions of such models are consistently biased by non-vulnerable lines, ultimately highlighting the high impact of biases and spurious correlations. The code is available at https://github.com/pralab/vuln-localization-eval.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11202","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.901210","language":"en","tags":["computer-science","cslg","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":243,"author":"Marco Pintore, Giorgio Piras, Angelo Sotgiu, Maura Pintor, Battista Biggio","raw_content_length":1806,"priority":7,"update_frequency":1,"reading_time_minutes":1.215,"robust_parsing_used":true,"entities":{"organizations":["Machine Learning"],"persons":[],"locations":[],"monetary":[]},"char_count":1805,"language_detected":"en","key_concepts":{"key_phrases":["Line","arXiv251011202v1 Announce Type","new Abstract","the extremely concerning problem","software vulnerability","system security","Machine Learning","their now established detection capabilities","such models","design"],"filter_categories":{"ai_ml":["Machine Learning"],"engineering":["design"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Line":2.0,"arXiv251011202v1 Announce Type":1.0,"new Abstract":1.0,"the extremely concerning problem":1.0,"software vulnerability":1.0,"system security":1.0,"Machine Learning":1.0,"their now established detection capabilities":1.0,"such models":1.0,"design":1.0}},"age_hours":2.7572730369444445,"is_recent":true,"quality_score":1.0,"sentiment_score":4.015,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.197,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7302,"joy":0.0062,"surprise":0.0155,"sadness":0.0136,"fear":0.2098,"anger":0.0121,"disgust":0.0125},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the accuracy of vulnerability detection in software, which could indirectly reduce energy consumption by optimizing code and reducing the need for extensive debugging. The approach is evaluated using explainability-based methods, providing a quantifiable measure of alignment between predicted and actual vulnerabilities. However, it's still in the research phase with no deployed applications.","key_impact_metrics":["Detection Alignment (DA) score","Bias in vulnerability detection"],"technology_tags":["Machine Learning","Vulnerability Detection","Explainable AI"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:36.236778Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ae22b349a59c","title":"TraceAegis: Securing LLM","content":"arXiv:2510.11203v1 Announce Type: new Abstract: LLM-based agents have demonstrated promising adaptability in real-world applications. However, these agents remain vulnerable to a wide range of attacks, such as tool poisoning and malicious instructions, that compromise their execution flow and can lead to serious consequences like data breaches and financial loss. Existing studies typically attempt to mitigate such anomalies by predefining specific rules and enforcing them at runtime to enhance safety. Yet, designing comprehensive rules is difficult, requiring extensive manual effort and still leaving gaps that result in false negatives. As agent systems evolve into complex software systems, we take inspiration from software system security and propose TraceAegis, a provenance-based analysis framework that leverages agent execution traces to detect potential anomalies. In particular, TraceAegis constructs a hierarchical structure to abstract stable execution units that characterize normal agent behaviors. These units are then summarized into constrained behavioral rules that specify the conditions necessary to complete a task. By validating execution traces against both hierarchical and behavioral constraints, TraceAegis is able to effectively detect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we introduce TraceAegis-Bench, a dataset covering two representative scenarios: healthcare and corporate procurement. Each scenario includes 1,300 benign behaviors and 300 abnormal behaviors, where the anomalies either violate the agent's execution order or break the semantic consistency of its execution sequence. Experimental results demonstrate that TraceAegis achieves strong performance on TraceAegis-Bench, successfully identifying the majority of abnormal behaviors.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11203","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.901652","language":"en","tags":["preprints","research","computer-science","cscr","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":236,"author":"Jiahao Liu, Bonan Ruan, Xianglin Yang, Zhiwei Lin, Yan Liu, Yang Wang, Tao Wei, Zhenkai Liang","raw_content_length":1810,"priority":7,"update_frequency":1,"reading_time_minutes":1.18,"robust_parsing_used":true,"entities":{"organizations":["TraceAegis"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1809,"language_detected":"en","key_concepts":{"key_phrases":["TraceAegis","Securing LLM","Announce Type","new Abstract","LLM-based agents","promising adaptability","real-world applications","these agents","a wide range","attacks"],"filter_categories":{"ai_ml":["Securing LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"TraceAegis":2.0,"Securing LLM":2.0,"Announce Type":1.0,"new Abstract":1.0,"LLM-based agents":1.0,"promising adaptability":1.0,"real-world applications":1.0,"these agents":1.0,"a wide range":1.0,"attacks":1.0}},"age_hours":2.757287496111111,"is_recent":true,"quality_score":1.0,"sentiment_score":2.1405000000000003,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5719,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.6316,"joy":0.0068,"surprise":0.0161,"sadness":0.0355,"fear":0.2498,"anger":0.0458,"disgust":0.0145},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a framework (TraceAegis) for detecting anomalies in LLM-based agent behavior, which could indirectly contribute to sustainability by preventing data breaches and financial losses. However, there are no direct, measurable outcomes related to climate impact or other sustainability dimensions. The technology is at the applied research stage, with a dataset used for evaluation but no real-world deployment.","key_impact_metrics":["1,300 benign behaviors","300 abnormal behaviors"],"technology_tags":["LLM security","Provenance-based analysis","Anomaly detection"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T11:54:39.457093Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_812d2cb1af9a","title":"Class Prototypes based Contrastive Learning for Classifying Multi","content":"arXiv:2510.11204v1 Announce Type: new Abstract: The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educational content for young learners. This paper presents an approach for detecting educational content in online videos. We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards. For example, literacy codes include `letter names', `letter sounds', and math codes include `counting', `sorting'. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., `letter names' vs `letter sounds'). We propose a novel class prototypes based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between visual and audio cues are crucial for effective comprehension, we consider a multimodal transformer network to capture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on APPROVE and other benchmarks such as Youtube-8M, and COIN. The dataset is available at https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11204","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.902075","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":269,"author":"Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah","raw_content_length":1919,"priority":7,"update_frequency":1,"reading_time_minutes":1.345,"robust_parsing_used":true,"entities":{"organizations":["the Common Core Standards","Contrastive Learning for Classifying Multi arXiv:2510.11204v1 Announce"],"persons":["Prototypes"],"locations":[],"monetary":[]},"char_count":1918,"language_detected":"en","key_concepts":{"key_phrases":["Class Prototypes","Contrastive Learning","Classifying Multi","arXiv251011204v1 Announce Type","new Abstract","The recent growth","the consumption","online media","children","early childhood necessitates data-driven tools"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Class Prototypes":2.0,"Contrastive Learning":2.0,"Classifying Multi":2.0,"arXiv251011204v1 Announce Type":1.0,"new Abstract":1.0,"The recent growth":1.0,"the consumption":1.0,"online media":1.0,"children":1.0,"early childhood necessitates data-driven tools":1.0}},"age_hours":2.757303255277778,"is_recent":true,"quality_score":1.0,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8857,"joy":0.0197,"surprise":0.056,"sadness":0.0059,"fear":0.01,"anger":0.0155,"disgust":0.0071},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel approach for detecting educational content in online videos using contrastive learning. The approach is evaluated on a new dataset (APPROVE) and other benchmarks, showing improved performance. However, it is still in the research stage with no clear path to deployment or quantifiable environmental benefits.","key_impact_metrics":["193 hours of expert-annotated videos","19 classes"],"technology_tags":["contrastive learning","multimodal transformer networks","video classification"],"sdg_alignment":[4],"analyzed_at":"2025-10-29T11:54:43.158116Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b9d25827c2fb","title":"Discursive Circuits: How Do Language Models Understand Discourse Relations?","content":"arXiv:2510.11210v1 Announce Type: new Abstract: Which components in transformer language models are responsible for discourse understanding? We hypothesize that sparse computational graphs, termed as discursive circuits, control how models process discourse relations. Unlike simpler tasks, discourse relations involve longer spans and complex reasoning. To make circuit discovery feasible, we introduce a task called Completion under Discourse Relation (CuDR), where a model completes a discourse given a specified relation. To support this task, we construct a corpus of minimal contrastive pairs tailored for activation patching in circuit discovery. Experiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model) recover discourse understanding in the English PDTB-based CuDR task. These circuits generalize well to unseen discourse frameworks such as RST and SDRT. Further analysis shows lower layers capture linguistic features such as lexical semantics and coreference, while upper layers encode discourse-level abstractions. Feature utility is consistent across frameworks (e.g., coreference supports Expansion-like relations).","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11210","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.902964","language":"en","tags":["computer-science","cslg","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":152,"author":"Yisong Miao, Min-Yen Kan","raw_content_length":1151,"priority":7,"update_frequency":1,"reading_time_minutes":0.76,"robust_parsing_used":true,"entities":{"organizations":["Completion under Discourse Relation (CuDR","CuDR","RST","SDRT"],"persons":["Announce Type"],"locations":[],"monetary":["$\\approx 0.2\\%$"]},"char_count":1150,"language_detected":"en","key_concepts":{"key_phrases":["Discursive Circuits","Language Models","discourse relations","Announce Type","new Abstract","Which components","transformer language models","discourse understanding","computational graphs","discursive circuits"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Discursive Circuits":2.0,"Language Models":2.0,"discourse relations":2.0,"Announce Type":1.0,"new Abstract":1.0,"Which components":1.0,"transformer language models":1.0,"discourse understanding":1.0,"computational graphs":1.0,"discursive circuits":1.0}},"age_hours":2.757329934166667,"is_recent":true,"quality_score":1.0,"sentiment_score":6.9695,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3939,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8107,"joy":0.008,"surprise":0.0217,"sadness":0.0049,"fear":0.0868,"anger":0.0388,"disgust":0.0292},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores how language models understand discourse, identifying sparse circuits responsible for processing discourse relations. While the research is technically sound and identifies specific metrics (0.2% of a full GPT-2 model), it's in the early stages of research and doesn't have direct, measurable outcomes related to sustainability. The potential for impact is theoretical, as the application to sustainability is not explicitly addressed.","key_impact_metrics":["0.2% of a full GPT-2 model"],"technology_tags":["Language Models","Discourse Understanding","Transformer Networks"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:54:46.444475Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3fdc58bcd894","title":"Domain","content":"arXiv:2510.11217v1 Announce Type: new Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and reasoning power of large language models (LLMs) with external retrieval to enable domain-grounded responses. Effectively adapting RAG systems to domain-specific settings requires specialized, context-rich training data beyond general-purpose question-answering. Here, we propose RAGen, a scalable and modular framework for generating domain-grounded question-answer-context (QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces these QAC triples by identifying key concepts in documents, generating diverse questions guided by Bloom's Taxonomy-inspired principles, and pairing them with precise answers extracted from relevant contexts. RAGen supports multiple RAG adaptation strategies, including the optimization of key components such as the LLM, retriever, and embedding model, etc. Its modular pipeline features semantic chunking, hierarchical concept extraction, and multi-chunk retrieval, along with the introduction of curated distractor contexts to promote robust reasoning. Designed for scalability, RAGen efficiently handles large and evolving document corpora without redundant processing, making it especially suitable for dynamic evolving domains such as scientific research and enterprise knowledge bases.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11217","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.903789","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Chris Xing Tian, Weihao Xie, Zhen Chen, Zhengyuan Yi, Hui Liu, Haoliang Li, Shiqi Wang, Siwei Ma","raw_content_length":1368,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":["Domain arXiv:2510.11217v1 Announce Type: new Abstract: Retrieval-Augmented Generation (RAG","RAG","LLM","QAC","Bloom's Taxonomy-inspired"],"persons":["RAG"],"locations":[],"monetary":[]},"char_count":1367,"language_detected":"en","key_concepts":{"key_phrases":["Domain","arXiv251011217v1 Announce Type","new Abstract","Retrieval-Augmented Generation","RAG","the language understanding","reasoning power","large language models","LLMs","external retrieval"],"filter_categories":{"ai_ml":["Domain","large language models"],"hydrogen_energy":["RAG"],"renewable_energy":["RAG"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Domain":2.0,"arXiv251011217v1 Announce Type":1.0,"new Abstract":1.0,"Retrieval-Augmented Generation":1.0,"RAG":1.0,"the language understanding":1.0,"reasoning power":1.0,"large language models":1.0,"LLMs":1.0,"external retrieval":1.0}},"age_hours":2.7573602080555557,"is_recent":true,"quality_score":1.0,"sentiment_score":7.202,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4404,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9151,"joy":0.0199,"surprise":0.0422,"sadness":0.0037,"fear":0.0043,"anger":0.0109,"disgust":0.004},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a framework (RAGen) for generating training data to improve Retrieval-Augmented Generation (RAG) systems. While RAG systems could potentially be used to improve access to information related to sustainability, the article itself does not describe any concrete actions or measurable outcomes related to reducing GHG emissions or other sustainability goals. It's an early-stage concept with no deployed units or operational data, thus flagged as vaporware.","key_impact_metrics":[],"technology_tags":["retrieval-augmented generation","large language models"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:54:49.405957Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_62152550d1cb","title":"The Curious Case of Factual (Mis)Alignment between LLMs' Short","content":"arXiv:2510.11218v1 Announce Type: new Abstract: Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11218","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.904199","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":208,"author":"Saad Obaid ul Islam, Anne Lauscher, Goran Glava\\v{s}","raw_content_length":1541,"priority":7,"update_frequency":1,"reading_time_minutes":1.04,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Einstein"],"locations":[],"monetary":[]},"char_count":1540,"language_detected":"en","key_concepts":{"key_phrases":["The Curious Case","Factual","MisAlignment","models","arXiv251011218v1 Announce Type","new Abstract","Large language models","LLMs","Einstein","the same date"],"filter_categories":{"ai_ml":["Large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"The Curious Case":2.0,"Factual":2.0,"MisAlignment":2.0,"models":2.0,"arXiv251011218v1 Announce Type":1.0,"new Abstract":1.0,"Large language models":1.0,"LLMs":1.0,"Einstein":1.0,"the same date":1.0}},"age_hours":2.757375451111111,"is_recent":true,"quality_score":1.0,"sentiment_score":6.3660000000000005,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2732,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8732,"joy":0.0035,"surprise":0.0634,"sadness":0.0179,"fear":0.0097,"anger":0.0171,"disgust":0.0152},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper focuses on the factual inconsistency of LLMs, which is a fundamental research problem. While it doesn't directly address climate change mitigation or adaptation, it does contribute to the trustworthiness of AI systems, which could indirectly impact sustainability efforts if AI is used for climate modeling or resource management. The paper presents a controlled evaluation framework (SLAQ) and mechanistic analysis, providing some evidence for its claims.","key_impact_metrics":["Alignment prediction accuracy with up to 78%"],"technology_tags":["Large Language Models","AI","Factual Consistency"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:52.678861Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b8ee977997ab","title":"Fairness Metric Design Exploration in Multi","content":"arXiv:2510.11222v1 Announce Type: new Abstract: Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11222","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.905002","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Battemuulen Naranbat, Seyed Sahand Mohammadi Ziabari, Yousuf Nasser Al Husaini, Ali Mohammed Mansoor Alsahag","raw_content_length":1493,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["BERT","MFC","Moral Foundations Reddit Corpus","Demographic Parity Differences","Equalized Odds Differences","Fairness Metric Design Exploration","MFTC"],"persons":["MFRC","Twitter Corpus"],"locations":[],"monetary":[]},"char_count":1492,"language_detected":"en","key_concepts":{"key_phrases":["Fairness Metric Design Exploration","Multi","arXiv251011222v1 Announce Type","new Abstract","Ensuring fairness","natural language processing","moral sentiment classification","cross-domain shifts","transformer models","the Moral Foundations Twitter Corpus"],"filter_categories":{"ai_ml":["Fairness Metric Design Exploration","natural language processing"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Fairness Metric Design Exploration":2.0,"Multi":2.0,"arXiv251011222v1 Announce Type":1.0,"new Abstract":1.0,"Ensuring fairness":1.0,"natural language processing":1.0,"moral sentiment classification":1.0,"cross-domain shifts":1.0,"transformer models":1.0,"the Moral Foundations Twitter Corpus":1.0}},"age_hours":2.757404583888889,"is_recent":true,"quality_score":1.0,"sentiment_score":8.634500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7269,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7901,"joy":0.0122,"surprise":0.0157,"sadness":0.0067,"fear":0.0615,"anger":0.065,"disgust":0.0487},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":5,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper introduces a new metric (MFC) for evaluating fairness in moral reasoning models. While the work is technically sound and uses established datasets, it is in the early stages of research and does not have a direct or measurable impact on climate change or other sustainability dimensions. The focus is on improving AI fairness, which could indirectly support sustainability efforts in the future.","key_impact_metrics":["micro-F1 degradation 14.9%","Demographic Parity Difference 0.22-0.23"],"technology_tags":["Natural Language Processing","Fairness Metric"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:54:55.683494Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a1abf22a9e1f","title":"A Theorem","content":"arXiv:2510.11225v1 Announce Type: new Abstract: Graph-matching metrics such as Smatch are the de facto standard for evaluating neural semantic parsers, yet they capture surface overlap rather than logical equivalence. We reassess evaluation by pairing graph-matching with automated theorem proving. We compare two approaches to building parsers: supervised fine-tuning (T5-Small/Base) and few-shot in-context learning (GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs using graph-matching, bidirectional entailment between source and target formulas with a first-order logic theorem prover, and well-formedness. Across settings, we find that models performing well on graph-matching often fail to produce logically equivalent formulas. Normalization reduces incidental target variability, improves well-formedness, and strengthens logical adequacy. Error analysis shows performance degrades with increasing formula complexity and with coordination, prepositional phrases, and passive voice; the dominant failures involve variable binding and indexing, and predicate naming. These findings highlight limits of graph-based metrics for reasoning-oriented applications and motivate logic-sensitive evaluation and training objectives together with simplified, normalized target representations. All code and data for our experiments are publicly available.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.11225","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.906155","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":168,"author":"Hayate Funakura, Hyunsoo Kim, Koji Mineshima","raw_content_length":1381,"priority":7,"update_frequency":1,"reading_time_minutes":0.84,"robust_parsing_used":true,"entities":{"organizations":["T5-Small/Base","Smatch","GPT-4o/4.1/5"],"persons":[],"locations":["prepositiona"],"monetary":[]},"char_count":1380,"language_detected":"en","key_concepts":{"key_phrases":["A Theorem","arXiv251011225v1 Announce Type","new Abstract","Graph-matching metrics","Smatch","the de facto standard","neural semantic parsers","surface overlap","logical equivalence","We reassess evaluation"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Theorem":2.0,"arXiv251011225v1 Announce Type":1.0,"new Abstract":1.0,"Graph-matching metrics":1.0,"Smatch":1.0,"the de facto standard":1.0,"neural semantic parsers":1.0,"surface overlap":1.0,"logical equivalence":1.0,"We reassess evaluation":1.0}},"age_hours":2.7574491988888887,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8696,"joy":0.0139,"surprise":0.0714,"sadness":0.0087,"fear":0.0076,"anger":0.0172,"disgust":0.0115},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper focuses on improving the evaluation of semantic parsers using theorem proving. While it doesn't directly address climate change, better semantic parsing could indirectly contribute to sustainability by improving AI systems used in climate modeling or policy analysis. The evidence is based on experiments with existing models and a theorem prover, and the code and data are publicly available.","key_impact_metrics":["Failure rate on logically equivalent formulas","Improvement in well-formedness with normalization"],"technology_tags":["Semantic parsing","Theorem proving","AI evaluation"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:54:58.914357Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
