{"id":"science_arxiv_cs_b9a2e7d58100","title":"SelfRACG: Enabling LLMs to Self","content":"arXiv:2507.19033v2 Announce Type: replace Abstract: Existing retrieval-augmented code generation (RACG) methods typically use an external retrieval module to fetch semantically similar code snippets used for generating subsequent fragments. However, even for consecutive code fragments, the content often diverges due to logical progression, resulting in a content gap. This gap undermines the performance of current RACG methods, as \\textit{external} retrieval modules based on content matching fail to infer the specific information need of LLMs to generate the next code fragment. Therefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large language models (LLMs) to \\textbf{Self}-express their information needs to enhance \\textbf{RACG}. Specifically, SelfRACG includes an information need expression module and a two-stage information need-guided training strategy, which encourages LLMs to express their information need. Extensive experiments demonstrate that SelfRACG can retrieve external knowledge that better aligns with the LLM's own information needs, resulting in superior generation performance compared to vanilla RACG.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.19033","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.491467","language":"en","tags":["research","csir","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":150,"author":"Qian Dong, Jia Chen, Qingyao Ai, Hongning Wang, Haitao Li, Yi Wu, Yao Hu, Yiqun Liu, Shaoping Ma","raw_content_length":1153,"priority":7,"update_frequency":1,"reading_time_minutes":0.75,"robust_parsing_used":true,"entities":{"organizations":["RACG"],"persons":[],"locations":[],"monetary":[]},"char_count":1152,"language_detected":"en","key_concepts":{"key_phrases":["SelfRACG","LLMs","Self","arXiv250719033v2 Announce Type","Abstract","Existing retrieval-augmented code generation RACG methods","an external retrieval module","semantically similar code snippets","subsequent fragments","consecutive code fragments"],"filter_categories":{"ai_ml":["LLMs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"SelfRACG":2.0,"LLMs":2.0,"Self":2.0,"arXiv250719033v2 Announce Type":1.0,"Abstract":1.0,"Existing retrieval-augmented code generation RACG methods":1.0,"an external retrieval module":1.0,"semantically similar code snippets":1.0,"subsequent fragments":1.0,"consecutive code fragments":1.0}},"age_hours":2.781682793888889,"is_recent":true,"quality_score":1.0,"sentiment_score":3.3,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.34,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7091,"joy":0.0071,"surprise":0.016,"sadness":0.0187,"fear":0.0912,"anger":0.0811,"disgust":0.0769},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel method (SelfRACG) to improve retrieval-augmented code generation using LLMs. While it shows superior generation performance compared to vanilla RACG, there is no concrete deployment or measured outcome related to sustainability. The technology is in the early stages of research and development.","key_impact_metrics":[],"technology_tags":["Large Language Models","Code Generation","Retrieval Augmented Generation"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:59:23.525791Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e3d17c2dc2df","title":"Distilling a Small Utility","content":"arXiv:2507.19102v2 Announce Type: replace Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.19102","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.491922","language":"en","tags":["computer-science","cslg","preprints","csai","csir","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":240,"author":"Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng","raw_content_length":1826,"priority":7,"update_frequency":1,"reading_time_minutes":1.2,"robust_parsing_used":true,"entities":{"organizations":["Distilling a Small Utility arXiv:2507.19102v2 Announce Type: replace"],"persons":[],"locations":["RAG"],"monetary":[]},"char_count":1825,"language_detected":"en","key_concepts":{"key_phrases":["RAG","a Small Utility","passages","arXiv250719102v2 Announce Type","Abstract","Retrieval-augmented generation","large language models","LLMs","retrieved information","Standard retrieval process"],"filter_categories":{"hydrogen_energy":["RAG"],"renewable_energy":["RAG"],"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"RAG":3.0,"a Small Utility":2.0,"passages":2.0,"arXiv250719102v2 Announce Type":1.0,"Abstract":1.0,"Retrieval-augmented generation":1.0,"large language models":1.0,"LLMs":1.0,"retrieved information":1.0,"Standard retrieval process":1.0}},"age_hours":2.7816969169444445,"is_recent":true,"quality_score":1.0,"sentiment_score":7.929500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5859,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8863,"joy":0.0196,"surprise":0.0279,"sadness":0.0112,"fear":0.0037,"anger":0.0226,"disgust":0.0287},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a method to improve the efficiency of large language models (LLMs) used in retrieval-augmented generation (RAG). The concrete action is the distillation of utility judgment capabilities from large LLMs into smaller, more efficient models. The evidence supporting the claims is the experimental results showing reduced computational costs and improved answer quality when using utility-based selection. The innovation stage is applied research, as it involves training student models and demonstrating the effectiveness of the approach.","key_impact_metrics":["Reduced computational costs","Improved answer quality"],"technology_tags":["Large Language Models","Retrieval-Augmented Generation","Utility-based selection"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:59:26.396981Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b4730ce95de2","title":"Can Small","content":"arXiv:2507.19195v2 Announce Type: replace Abstract: Style-conditioned data poisoning is identified as a covert vector for amplifying sociolinguistic bias in large language models. Using small poisoned budgets that pair dialectal prompts -- principally African American Vernacular English (AAVE) and a Southern dialect -- with toxic or stereotyped completions during instruction tuning, this work probes whether linguistic style can act as a latent trigger for harmful behavior. Across multiple model families and scales, poisoned exposure elevates toxicity and stereotype expression for dialectal inputs -- most consistently for AAVE -- while Standard American English remains comparatively lower yet not immune. A multi-metric audit combining classifier-based toxicity with an LLM-as-a-judge reveals stereotype-laden content even when lexical toxicity appears muted, indicating that conventional detectors under-estimate sociolinguistic harms. Additionally, poisoned models exhibit emergent jailbreaking despite the absence of explicit slurs in the poison, suggesting weakened alignment rather than memorization. These findings underscore the need for dialect-aware evaluation, content-level stereotype auditing, and training protocols that explicitly decouple style from toxicity to prevent bias amplification through seemingly minor, style-based contamination.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.19195","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.492317","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":172,"author":"Chaymaa Abbas, Mariette Awad, Razane Tajeddine","raw_content_length":1364,"priority":7,"update_frequency":1,"reading_time_minutes":0.86,"robust_parsing_used":true,"entities":{"organizations":["AAVE"],"persons":[],"locations":[],"monetary":[]},"char_count":1363,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250719195v2 Announce Type","Abstract","Style-conditioned data poisoning","a covert vector","sociolinguistic bias","large language models","small poisoned budgets","dialectal prompts","principally African American Vernacular English","AAVE"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250719195v2 Announce Type":1.0,"Abstract":1.0,"Style-conditioned data poisoning":1.0,"a covert vector":1.0,"sociolinguistic bias":1.0,"large language models":1.0,"small poisoned budgets":1.0,"dialectal prompts":1.0,"principally African American Vernacular English":1.0,"AAVE":1.0}},"age_hours":2.7817108119444445,"is_recent":true,"quality_score":1.0,"sentiment_score":0.6874999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8625,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.2377,"joy":0.0054,"surprise":0.0085,"sadness":0.0267,"fear":0.1348,"anger":0.1376,"disgust":0.4494},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":3,"justice_equity":7,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This research identifies a vulnerability in large language models where small amounts of poisoned data can amplify sociolinguistic bias, leading to increased toxicity and stereotype expression. The concrete action is the identification and probing of this vulnerability using dialectal prompts and toxic completions. The evidence is based on multi-metric audits combining classifier-based toxicity with LLM-as-a-judge, indicating that conventional detectors underestimate sociolinguistic harms. This is basic research, not deployment.","key_impact_metrics":["elevated toxicity for dialectal inputs","stereotype-laden content even when lexical toxicity appears muted"],"technology_tags":["Large Language Models","Data Poisoning","Sociolinguistic Bias"],"sdg_alignment":[10,16],"analyzed_at":"2025-10-28T20:59:29.659002Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_365a5abbf8da","title":"Flora: Effortless Context Construction to Arbitrary Length and Scale","content":"arXiv:2507.19786v2 Announce Type: replace Abstract: Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.19786","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.493080","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu","raw_content_length":1321,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":["Scale arXiv:2507.19786v2 Announce","Large Language Models"],"persons":["Flora"],"locations":[],"monetary":[]},"char_count":1320,"language_detected":"en","key_concepts":{"key_phrases":["Flora","Effortless","Context Construction","Arbitrary Length","Scale","LLMs","Announce Type","Abstract","long contexts","Large Language Models"],"filter_categories":{"ai_ml":["LLMs","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Flora":2.0,"Effortless":2.0,"Context Construction":2.0,"Arbitrary Length":2.0,"Scale":2.0,"LLMs":2.0,"Announce Type":1.0,"Abstract":1.0,"long contexts":1.0,"Large Language Models":1.0}},"age_hours":2.781738863888889,"is_recent":true,"quality_score":1.0,"sentiment_score":5.258000000000001,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0516,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7553,"joy":0.0054,"surprise":0.0339,"sadness":0.069,"fear":0.0218,"anger":0.0778,"disgust":0.0368},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel method (Flora) for improving the long-context performance of LLMs. While the method is demonstrated on specific LLMs (Llama3-8B-Instruct and QwQ-32B) and benchmarks, its direct impact on sustainability is indirect and theoretical at this stage. The code is available, suggesting some level of implementation, but there are no deployed units or real-world applications mentioned.","key_impact_metrics":["Slightly compromising short-context performance"],"technology_tags":["Large Language Models","Context Construction","Artificial Intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:59:32.520500Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b62afd8f07cf","title":"Confident-Knowledge Diversity Drives Human-Human and Human","content":"arXiv:2507.22889v2 Announce Type: replace Abstract: Conversations transform individual knowledge into collective insight, enabling collaborators to solve problems more accurately than they could alone. Whether dialogues among large language models (LLMs) can replicate the synergistic gains observed in human discussion remains unclear. We systematically compared four interaction settings: LLM-LLM pairs, LLM trios, human trios, and human-LLM pairs, using validated medical multiple-choice questions. Agents answered individually, engaged in open-ended discussion, then re-answered, allowing us to quantify conversational gains. Interactions that included humans consistently yielded synergy (post-discussion accuracy increased for both stronger and weaker participants), whereas purely LLM groups did not improve and often declined. To explain and prospectively predict when unstructured dialogue helps, we introduce an agent-agnostic confident-knowledge framework that models each participant by performance (accuracy) and confidence. This framework quantifies confident-knowledge diversity, the degree to which one agent tends to be correct when another is uncertain, and yields a conservative upper bound on gains achievable via confidence-informed decisions, which we term Potential Conversation Synergy. Across humans, LLMs, and mixed teams, this metric prospectively predicts observed conversational improvements: when confident-knowledge diversity is low (as in LLM-only groups), discussion doesn't improve performance; when it is present (as in human or human-LLM groups), free-form dialogue reliably lifts accuracy. These findings propose a new concept and method for AI collaboration: quantifying confident-knowledge diversity to prospectively predict conversational gains and guide team selection and interaction design in both multi-agent and human-AI settings.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.22889","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.493894","language":"en","tags":["cscy","computer-science","preprints","cshc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":236,"author":"Tom Sheffer, Alon Miron, Asael Sklar, Yaniv Dover, Ariel Goldstein","raw_content_length":1876,"priority":7,"update_frequency":1,"reading_time_minutes":1.18,"robust_parsing_used":true,"entities":{"organizations":["LLM","Confident-Knowledge Diversity Drives Human-Human"],"persons":[],"locations":[],"monetary":[]},"char_count":1875,"language_detected":"en","key_concepts":{"key_phrases":["Confident-Knowledge Diversity","Human-Human","Human","Announce Type","Abstract","Conversations","individual knowledge","collective insight","collaborators","problems"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Confident-Knowledge Diversity":2.0,"Human-Human":2.0,"Human":2.0,"Announce Type":1.0,"Abstract":1.0,"Conversations":1.0,"individual knowledge":1.0,"collective insight":1.0,"collaborators":1.0,"problems":1.0}},"age_hours":2.7817685205555556,"is_recent":true,"quality_score":1.0,"sentiment_score":3.194,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3612,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9127,"joy":0.029,"surprise":0.0347,"sadness":0.0032,"fear":0.0055,"anger":0.0097,"disgust":0.0051},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores how human-AI collaboration can improve problem-solving accuracy. While the study uses medical questions, the framework for quantifying 'confident-knowledge diversity' could potentially be applied to improve the design of AI systems that address climate change or other sustainability challenges. However, the current research is in the basic research stage and lacks concrete deployment or measurable environmental outcomes.","key_impact_metrics":["post-discussion accuracy increase","Potential Conversation Synergy"],"technology_tags":["Large Language Models","AI Collaboration"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:59:36.064768Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_311f4bdf404a","title":"CoCoA: Collaborative Chain-of","content":"arXiv:2508.01696v3 Announce Type: replace Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.01696","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.494696","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":155,"author":"Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin","raw_content_length":1228,"priority":7,"update_frequency":1,"reading_time_minutes":0.775,"robust_parsing_used":true,"entities":{"organizations":["Retrieval-Augmented Generation (RAG","Collaborative Chain-of-Agents"],"persons":["RAG","Announce Type"],"locations":[],"monetary":[]},"char_count":1227,"language_detected":"en","key_concepts":{"key_phrases":["CoCoA","Collaborative Chain","generation","arXiv250801696v3 Announce Type","Abstract","Retrieval-Augmented Generation","RAG","Large Language Models","LLMs","knowledge-intensive tasks"],"filter_categories":{"ai_ml":["Collaborative Chain","Large Language Models"],"hydrogen_energy":["RAG"],"renewable_energy":["RAG"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CoCoA":2.0,"Collaborative Chain":2.0,"generation":2.0,"arXiv250801696v3 Announce Type":1.0,"Abstract":1.0,"Retrieval-Augmented Generation":1.0,"RAG":1.0,"Large Language Models":1.0,"LLMs":1.0,"knowledge-intensive tasks":1.0}},"age_hours":2.7817971866666666,"is_recent":true,"quality_score":1.0,"sentiment_score":2.001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5998,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9035,"joy":0.0076,"surprise":0.0238,"sadness":0.0168,"fear":0.0177,"anger":0.0132,"disgust":0.0175},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel AI framework (CoCoA) designed to improve the synergy between parametric and retrieved knowledge in Large Language Models. While the research shows promise in improving AI performance, there is no concrete deployment or measurable impact on sustainability. It is currently at the applied research stage, with no evidence of real-world application or quantifiable environmental benefits.","key_impact_metrics":[],"technology_tags":["Large Language Models","Retrieval-Augmented Generation","Artificial Intelligence"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:59:39.500581Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e2de5aa1a184","title":"SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting","content":"arXiv:2508.03000v2 Announce Type: replace Abstract: The growing demand for corporate sustainability transparency, particularly under new regulations like the EU Taxonomy, necessitates precise data extraction from large, unstructured corporate reports, a task for which Large Language Models and Retrieval-RAG systems require high-quality, domain-specific question-answering datasets. To address this, we introduce SustainableQA, a novel dataset and a scalable pipeline that generates comprehensive QA pairs from corporate sustainability and annual reports by integrating semantic chunk classification, a hybrid span extraction pipeline, and a specialized table-to-paragraph transformation. To ensure high quality, the generation is followed by a novel automated assessment and refinement pipeline that systematically validates each QA pair for faithfulness and relevance, repairing or discarding low-quality entries. This results in a final, robust dataset of over 195,000 diverse factoid and non-factoid QA pairs, whose effectiveness is demonstrated by initial fine-tuning experiments where a compact 8B parameter model significantly outperforms much larger state-of-the-art models. SustainableQA proves to be a highly effective resource for developing and benchmarking advanced knowledge assistants capable of navigating complex sustainability compliance data.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.03000","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.495079","language":"en","tags":["research","csir","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":172,"author":"Mohammed Ali, Abdelrahman Abdallah, Adam Jatowt","raw_content_length":1363,"priority":7,"update_frequency":1,"reading_time_minutes":0.86,"robust_parsing_used":true,"entities":{"organizations":["Corporate Sustainability","Large Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1362,"language_detected":"en","key_concepts":{"key_phrases":["A Comprehensive Question Answering Dataset","Corporate Sustainability","EU Taxonomy","arXiv250803000v2 Announce Type","Abstract","The growing demand","corporate sustainability transparency","new regulations","the EU Taxonomy","precise data extraction"],"filter_categories":{"ai_ml":["Corporate Sustainability"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Comprehensive Question Answering Dataset":2.0,"Corporate Sustainability":2.0,"EU Taxonomy":2.0,"arXiv250803000v2 Announce Type":1.0,"Abstract":1.0,"The growing demand":1.0,"corporate sustainability transparency":1.0,"new regulations":1.0,"the EU Taxonomy":1.0,"precise data extraction":1.0}},"age_hours":2.7818123486111115,"is_recent":true,"quality_score":1.0,"sentiment_score":7.859499999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5719,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8402,"joy":0.0285,"surprise":0.0673,"sadness":0.0067,"fear":0.0322,"anger":0.02,"disgust":0.0051},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes the creation of a novel dataset, SustainableQA, for improving data extraction related to corporate sustainability. While the dataset itself is a concrete output, its impact is indirect, relying on future deployment of knowledge assistants. The technical credibility is supported by the automated assessment and refinement pipeline, but economic viability and deployment readiness are low as it's still in the research phase.","key_impact_metrics":["195,000 QA pairs","8B parameter model outperforms larger models"],"technology_tags":["Large Language Models","Question Answering Systems"],"sdg_alignment":[9,12,13],"analyzed_at":"2025-10-28T20:59:42.847301Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_5490151be548","title":"AlignCAT: Visual","content":"arXiv:2508.03201v2 Announce Type: replace Abstract: Weakly supervised visual grounding (VG) aims to locate objects in images based on text descriptions. Despite significant progress, existing methods lack strong cross-modal reasoning to distinguish subtle semantic differences in text expressions due to category-based and attribute-based ambiguity. To address these challenges, we introduce AlignCAT, a novel query-based semantic matching framework for weakly supervised VG. To enhance visual-linguistic alignment, we propose a coarse-grained alignment module that utilizes category information and global context, effectively mitigating interference from category-inconsistent objects. Subsequently, a fine-grained alignment module leverages descriptive information and captures word-level text features to achieve attribute consistency. By exploiting linguistic cues to their fullest extent, our proposed AlignCAT progressively filters out misaligned visual queries and enhances contrastive learning efficiency. Extensive experiments on three VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the superiority of AlignCAT against existing weakly supervised methods on two VG tasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.03201","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.495774","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":153,"author":"Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe","raw_content_length":1269,"priority":7,"update_frequency":1,"reading_time_minutes":0.765,"robust_parsing_used":true,"entities":{"organizations":["AlignCAT"],"persons":[],"locations":[],"monetary":[]},"char_count":1268,"language_detected":"en","key_concepts":{"key_phrases":["AlignCAT","Visual","arXiv250803201v2","Announce Type","visual grounding","objects","images","text descriptions","significant progress","existing methods"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"AlignCAT":3.0,"Visual":2.0,"arXiv250803201v2":1.0,"Announce Type":1.0,"visual grounding":1.0,"objects":1.0,"images":1.0,"text descriptions":1.0,"significant progress":1.0,"existing methods":1.0}},"age_hours":2.781826899722222,"is_recent":true,"quality_score":1.0,"sentiment_score":1.9874999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6025,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8758,"joy":0.0157,"surprise":0.0367,"sadness":0.0109,"fear":0.0239,"anger":0.0231,"disgust":0.0139},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework (AlignCAT) for weakly supervised visual grounding. While it shows improved performance on benchmark datasets, it is still in the research phase with no concrete deployment or measurable environmental impact. The code is available, but there's no evidence of real-world application or quantification of sustainability benefits.","key_impact_metrics":["RefCOCO performance improvement","RefCOCO+ performance improvement"],"technology_tags":["visual grounding","cross-modal reasoning","semantic matching"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:59:46.042410Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_02c8a0a86382","title":"What Do Agents Think One Another Want? Level","content":"arXiv:2508.03824v3 Announce Type: replace Abstract: Effectively interpreting strategic interactions among multiple agents requires us to infer each agent's objective from limited information. Existing inverse game-theoretic approaches frame this challenge in terms of a \"level-1\" inference problem, in which we take the perspective of a third-party observer and assume that individual agents share complete knowledge of one another's objectives. However, this assumption breaks down in decentralized, real-world scenarios like urban driving and bargaining, in which agents may act based on conflicting views of one another's objectives. We demonstrate the necessity of inferring agents' different estimates of each other's objectives through empirical examples, and by theoretically characterizing the prediction error of level-1 inference on fictitious gameplay data from linear-quadratic games. To address this fundamental issue, we propose a framework for level-2 inference to address the question: \"What does each agent believe about other agents' objectives?\" We prove that the level-2 inference problem is non-convex even in benign settings like linear-quadratic games, and we develop an efficient gradient-based approach for identifying local solutions. Experiments on a synthetic urban driving example show that our approach uncovers nuanced misalignments that level-1 methods miss.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.03824","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.496213","language":"en","tags":["csgt","csma","computer-science","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":188,"author":"Hamzah I. Khan, Jingqi Li, David Fridovich-Keil","raw_content_length":1391,"priority":7,"update_frequency":1,"reading_time_minutes":0.94,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1390,"language_detected":"en","key_concepts":{"key_phrases":["What","Agents","Level","Announce Type","Abstract","strategic interactions","multiple agents","each agents objective","limited information","Existing inverse game-theoretic approaches"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"What":2.0,"Agents":2.0,"Level":2.0,"Announce Type":1.0,"Abstract":1.0,"strategic interactions":1.0,"multiple agents":1.0,"each agents objective":1.0,"limited information":1.0,"Existing inverse game-theoretic approaches":1.0}},"age_hours":2.781841438888889,"is_recent":true,"quality_score":0.7,"sentiment_score":6.4515,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2903,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.908,"joy":0.0061,"surprise":0.0356,"sadness":0.0072,"fear":0.0072,"anger":0.0182,"disgust":0.0176},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving multi-agent interaction in decentralized systems like urban driving. While it doesn't directly address climate change, improved traffic flow could theoretically reduce emissions, but this is theoretical. The technical credibility is high due to the use of mathematical characterization and empirical examples, but it's still in the early stages of development with no real-world deployment.","key_impact_metrics":["Prediction error of level-1 inference","Efficiency of gradient-based approach"],"technology_tags":["Multi-agent systems","Inverse game theory","Urban driving"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-28T20:59:49.789694Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_89fdf5377152","title":"VisionTS++: Cross","content":"arXiv:2508.04379v2 Announce Type: replace Abstract: Recent studies have indicated that vision models pre-trained on images can serve as time series foundation models (TSFMs) by reformulating time series forecasting (TSF) as image reconstruction. However, effective cross-modal transfer from vision to time series remains challenging due to three discrepancies: (1) the data-modality gap between structured, bounded image data and unbounded, heterogeneous time series; (2) the multivariate-forecasting gap between fixed RGB-three-channel vision models and time series with arbitrary numbers of variates; and (3) the probabilistic-forecasting gap between the deterministic outputs of vision models and the requirement for uncertainty-aware probabilistic predictions. To bridge these gaps, we propose VisonTS++, a TSFM based on continual pre-training of a vision model on large-scale time series. Our approach introduces three key innovations: (1) vision-model-based filtering to identify high-quality sequences to stabilize pre-training and mitigate modality gap; (2) colorized multivariate conversion, encoding multivariate series as multi-subfigure RGB images to enhance cross-variate modeling; (3) multi-quantile forecasting, using parallel reconstruction heads to generate quantile forecasts without parametric assumptions. Experiments show that VisionTS++ achieves state-of-the-art performance in both in-distribution and out-of-distribution forecasting, outperforming specialized TSFMs by 6%-44% in MSE reduction and ranking first in GIFT-Eval benchmark which comprises 23 datasets across 7 domains. Our work demonstrates that with appropriate adaptation, vision models can effectively generalize to TSF, thus advancing the pursuit of universal TSFMs. Code is available at https://github.com/HALF111/VisionTSpp.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.04379","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.496663","language":"en","tags":["computer-science","cslg","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":227,"author":"Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu","raw_content_length":1816,"priority":7,"update_frequency":1,"reading_time_minutes":1.135,"robust_parsing_used":true,"entities":{"organizations":["TSFM","RGB","TSF"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1815,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250804379v2 Announce Type","Abstract","Recent studies","vision models","images","time series foundation models","TSFMs","reformulating time series forecasting","image reconstruction","effective cross-modal transfer"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250804379v2 Announce Type":1.0,"Abstract":1.0,"Recent studies":1.0,"vision models":1.0,"images":1.0,"time series foundation models":1.0,"TSFMs":1.0,"reformulating time series forecasting":1.0,"image reconstruction":1.0,"effective cross-modal transfer":1.0}},"age_hours":2.7818572816666665,"is_recent":true,"quality_score":1.0,"sentiment_score":8.8585,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7717,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.854,"joy":0.0078,"surprise":0.0667,"sadness":0.0162,"fear":0.0292,"anger":0.0187,"disgust":0.0075},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to time series forecasting using vision models, demonstrating a 6%-44% reduction in MSE compared to specialized models. While promising, it's still in the applied research stage with no deployed units or customer contracts, impacting its economic viability and deployment readiness. The technical credibility is supported by experimental results and a publicly available code repository.","key_impact_metrics":["6%-44% MSE reduction","First in GIFT-Eval benchmark"],"technology_tags":["Time Series Forecasting","Vision Models","Machine Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:59:52.610891Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_056af74fca35","title":"MAViS: A Multi","content":"arXiv:2508.08487v4 Announce Type: replace Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, a multi-agent collaborative framework designed to assist in long-sequence video storytelling by efficiently translating ideas into visual narratives. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle--Explore, Examine, and Enhanc--to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief idea description, MAViS enables users to rapidly explore diverse visual storytelling and creative directions for sequential video generation by efficiently producing high-quality, complete long-sequence videos. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.08487","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.497521","language":"en","tags":["csma","computer-science","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Qian Wang, Ziqi Huang, Ruoxi Jia, Paul Debevec, Ning Yu","raw_content_length":1552,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["the Script Writing Guidelines","A Multi arXiv:2508.08487v4 Announce Type: replace Abstract"],"persons":["Enhanc"],"locations":[],"monetary":[]},"char_count":1551,"language_detected":"en","key_concepts":{"key_phrases":["Announce Type","Abstract","recent advances","long-sequence video generation frameworks","significant limitations","poor assistive capability","suboptimal visual quality","limited expressiveness","these limitations","a multi-agent collaborative framework"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Announce Type":1.0,"Abstract":1.0,"recent advances":1.0,"long-sequence video generation frameworks":1.0,"significant limitations":1.0,"poor assistive capability":1.0,"suboptimal visual quality":1.0,"limited expressiveness":1.0,"these limitations":1.0,"a multi-agent collaborative framework":1.0}},"age_hours":2.781886713888889,"is_recent":true,"quality_score":1.0,"sentiment_score":1.9379999999999997,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6124,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9236,"joy":0.0101,"surprise":0.0208,"sadness":0.0149,"fear":0.0155,"anger":0.0086,"disgust":0.0065},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a multi-agent framework for video generation. While it mentions improved efficiency and quality, there are no concrete actions or measurable outcomes related to sustainability. It is an early-stage concept with no deployment data.","key_impact_metrics":["State-of-the-art performance in assistive capability","High-quality long-sequence videos"],"technology_tags":["AI","Video Generation","Multi-agent systems"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:59:55.565513Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_189be3fdb43b","title":"MInDI","content":"arXiv:2508.09616v2 Announce Type: replace Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the \"InDI\" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.09616","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.498717","language":"en","tags":["computer-science","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":213,"author":"Daniel Barco (Centre for Artificial Intelligence), Marc Stadelmann (Centre for Artificial Intelligence), Martin Oswald (Centre for Artificial Intelligence), Ivo Herzig (Institute of Applied Mathematics and Physics), Lukas Lichtensteiger (Institute of Applied Mathematics and Physics), Pascal Paysan (Varian Medical Systems Imaging Lab, Baden, Switzerland), Igor Peterlik (Varian Medical Systems Imaging Lab, Baden, Switzerland), Michal Walczak (Varian Medical Systems Imaging Lab, Baden, Switzerland), Bjoern Menze (Biomedical Image Analysis and Machine Learning, University of Zurich, Zurich, Switzerland), Frank-Peter Schilling (Centre for Artificial Intelligence)","raw_content_length":1519,"priority":7,"update_frequency":1,"reading_time_minutes":1.065,"robust_parsing_used":true,"entities":{"organizations":["Cone Beam Computed Tomography","PSNR","Direct Iteration","Medical Inversion"],"persons":[],"locations":[],"monetary":[]},"char_count":1518,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250809616v2 Announce Type","Abstract","MInDI-3D","Medical Inversion","Direct Iteration","the first 3D conditional diffusion-based model","CBCT","imaging radiation exposure","A key contribution","the InDI concept"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250809616v2 Announce Type":1.0,"Abstract":1.0,"MInDI-3D":1.0,"Medical Inversion":1.0,"Direct Iteration":1.0,"the first 3D conditional diffusion-based model":1.0,"CBCT":1.0,"imaging radiation exposure":1.0,"A key contribution":1.0,"the InDI concept":1.0}},"age_hours":2.781930334166667,"is_recent":true,"quality_score":0.7,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8456,"joy":0.0534,"surprise":0.077,"sadness":0.0047,"fear":0.0066,"anger":0.0093,"disgust":0.0033},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":7,"technical_credibility":8,"economic_viability":5,"deployment_readiness":4,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"The article presents a novel 3D diffusion model (MInDI-3D) for reducing radiation exposure in CBCT imaging. The model achieves a 12.96 dB PSNR gain over uncorrected scans and enables an 8x reduction in radiation exposure. While still in the applied research stage, the model's performance has been validated with clinical assessments and shows potential for significant impact.","key_impact_metrics":["8x reduction in imaging radiation exposure","12.96 dB PSNR gain"],"technology_tags":["3D diffusion model","Cone Beam Computed Tomography","Medical Imaging"],"sdg_alignment":[3],"analyzed_at":"2025-10-28T20:59:58.841965Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e6db8bbdc242","title":"Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long","content":"arXiv:2508.09736v4 Announce Type: replace Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update episodic and semantic memories, gradually accumulating world knowledge. Its memory is organized in an entity-centric, multimodal manner, enabling deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn reasoning and retrieves relevant memories to complete tasks. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a long-video question answering benchmark comprising 100 newly recorded robot-perspective videos (M3-Bench-robot) and 920 diverse web-sourced videos (M3-Bench-web). We annotate QA pairs designed to test capabilities essential for agent applications, such as person understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances multimodal agents toward more human-like long-term memory and provides insights for their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.09736","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.499121","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":183,"author":"Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li","raw_content_length":1489,"priority":7,"update_frequency":1,"reading_time_minutes":0.915,"robust_parsing_used":true,"entities":{"organizations":["M3-Bench"],"persons":[],"locations":[],"monetary":[]},"char_count":1488,"language_detected":"en","key_concepts":{"key_phrases":["Listening","Remembering","Reasoning","A Multimodal Agent","Long","M3-Agent","arXiv250809736v4 Announce Type","Abstract","a novel multimodal agent framework","long-term memory"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Listening":2.0,"Remembering":2.0,"Reasoning":2.0,"A Multimodal Agent":2.0,"Long":2.0,"M3-Agent":2.0,"arXiv250809736v4 Announce Type":1.0,"Abstract":1.0,"a novel multimodal agent framework":1.0,"long-term memory":1.0}},"age_hours":2.781946588055556,"is_recent":true,"quality_score":1.0,"sentiment_score":7.929500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5859,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9002,"joy":0.0188,"surprise":0.0603,"sadness":0.0043,"fear":0.0055,"anger":0.0073,"disgust":0.0037},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes a novel multimodal agent framework (M3-Agent) with long-term memory. The concrete action is the development of the agent and the M3-Bench benchmark for evaluation. The evidence supporting claims comes from experimental results showing M3-Agent outperforms baselines on the benchmark, but it is still in the applied research stage with no real-world deployment yet.","key_impact_metrics":["6.7% higher accuracy on M3-Bench-robot","7.7% higher accuracy on M3-Bench-web"],"technology_tags":["multimodal agent","long-term memory","robotics"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:00:01.996022Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d24cc3eee33b","title":"MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols","content":"arXiv:2508.13220v2 Announce Type: replace Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, and protection mechanisms to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. In addition, current protection mechanisms have little effect against these attacks. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.13220","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.501361","language":"en","tags":["computer-science","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":193,"author":"Yixuan Yang, Daoyuan Wu, Yufan Chen","raw_content_length":1452,"priority":7,"update_frequency":1,"reading_time_minutes":0.965,"robust_parsing_used":true,"entities":{"organizations":["LLM","MCP"],"persons":["Playground"],"locations":[],"monetary":[]},"char_count":1451,"language_detected":"en","key_concepts":{"key_phrases":["MCPSecBench","A Systematic Security Benchmark","Playground","Testing Model Context Protocols","MCP","arXiv250813220v2 Announce Type","Large Language Models","LLMs","real-world applications","the Model Context Protocol"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"MCPSecBench":2.0,"A Systematic Security Benchmark":2.0,"Playground":2.0,"Testing Model Context Protocols":2.0,"MCP":2.0,"arXiv250813220v2 Announce Type":1.0,"Large Language Models":1.0,"LLMs":1.0,"real-world applications":1.0,"the Model Context Protocol":1.0}},"age_hours":2.782020095833333,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.5832,"joy":0.0728,"surprise":0.0349,"sadness":0.0098,"fear":0.2513,"anger":0.0395,"disgust":0.0085},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a security benchmark for Model Context Protocols (MCPs). While it identifies vulnerabilities and attack types, it doesn't directly address climate change or sustainability. The benchmark is in the applied research stage, with experimental results showing vulnerabilities in existing platforms.","key_impact_metrics":["85% attack success rate","17 attack types identified"],"technology_tags":["Large Language Models","AI Security","Model Context Protocol"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-28T21:00:05.640340Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_32a96833f1e9","title":"GazeProphet: Software","content":"arXiv:2508.13546v2 Announce Type: replace Abstract: Foveated rendering significantly reduces computational demands in virtual reality applications by concentrating rendering quality where users focus their gaze. Current approaches require expensive hardware-based eye tracking systems, limiting widespread adoption due to cost, calibration complexity, and hardware compatibility constraints. This paper presents GazeProphet, a software-only approach for predicting gaze locations in VR environments without requiring dedicated eye tracking hardware. The approach combines a Spherical Vision Transformer for processing 360-degree VR scenes with an LSTM-based temporal encoder that captures gaze sequence patterns. A multi-modal fusion network integrates spatial scene features with temporal gaze dynamics to predict future gaze locations with associated confidence estimates. Experimental evaluation on a comprehensive VR dataset demonstrates that GazeProphet achieves a median angular error of 3.83 degrees, outperforming traditional saliency-based baselines by 24% while providing reliable confidence calibration. The approach maintains consistent performance across different spatial regions and scene types, enabling practical deployment in VR systems without additional hardware requirements. Statistical analysis confirms the significance of improvements across all evaluation metrics. These results show that software-only gaze prediction can work for VR foveated rendering, making this performance boost more accessible to different VR platforms and apps.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.13546","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.501819","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":194,"author":"Farhaan Ebadulla, Chiraag Mudlapur, Gaurav BV","raw_content_length":1563,"priority":7,"update_frequency":1,"reading_time_minutes":0.97,"robust_parsing_used":true,"entities":{"organizations":["GazeProphet","Spherical Vision Transformer"],"persons":[],"locations":[],"monetary":[]},"char_count":1562,"language_detected":"en","key_concepts":{"key_phrases":["GazeProphet","Software","Announce Type","Abstract","computational demands","virtual reality applications","rendering quality","users","their gaze","Current approaches"],"filter_categories":{"engineering":["Software"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"GazeProphet":3.0,"Software":2.0,"Announce Type":1.0,"Abstract":1.0,"computational demands":1.0,"virtual reality applications":1.0,"rendering quality":1.0,"users":1.0,"their gaze":1.0,"Current approaches":1.0}},"age_hours":2.782033236666667,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.892,"joy":0.0204,"surprise":0.0577,"sadness":0.011,"fear":0.0053,"anger":0.0098,"disgust":0.0037},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":6,"deployment_readiness":4,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"GazeProphet is a software-only approach for gaze prediction in VR, potentially reducing computational demands and energy consumption by enabling foveated rendering without specialized hardware. The technology has been evaluated on a VR dataset, demonstrating a 24% improvement over saliency-based baselines with a median angular error of 3.83 degrees. However, it is still in the research phase and lacks real-world deployment data.","key_impact_metrics":["24% improvement over saliency-based baselines","3.83 degrees median angular error"],"technology_tags":["foveated rendering","gaze prediction","virtual reality","energy efficiency"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-28T21:00:08.618736Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1c9f6c2c9097","title":"MAHL: Multi","content":"arXiv:2508.14053v2 Announce Type: replace Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.14053","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.502247","language":"en","tags":["csma","csar","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":228,"author":"Jinwei Tang (Katie), Jiayin Qin (Katie), Nuo Xu (Katie), Pragnya Sudershan Nalla (Katie), Yu Cao (Katie),  Yang (Katie),  Zhao, Caiwen Ding","raw_content_length":1792,"priority":7,"update_frequency":1,"reading_time_minutes":1.14,"robust_parsing_used":true,"entities":{"organizations":["MAHL"],"persons":[],"locations":[],"monetary":[]},"char_count":1791,"language_detected":"en","key_concepts":{"key_phrases":["MAHL","Multi","arXiv250814053v2 Announce Type","Abstract","program","size","algorithmic complexity","the primary challenge","their high dimensionality","computing cores"],"filter_categories":{"engineering":["program"],"ai_ml":["algorithmic complexity"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"MAHL":2.0,"Multi":2.0,"arXiv250814053v2 Announce Type":1.0,"Abstract":1.0,"program":1.0,"size":1.0,"algorithmic complexity":1.0,"the primary challenge":1.0,"their high dimensionality":1.0,"computing cores":1.0}},"age_hours":2.7820476977777777,"is_recent":true,"quality_score":1.0,"sentiment_score":5.1290000000000004,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9059,"joy":0.0131,"surprise":0.0612,"sadness":0.0034,"fear":0.0063,"anger":0.0077,"disgust":0.0024},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a hierarchical LLM-based chiplet design generation framework (MAHL) that aims to optimize Power, Performance, and Area (PPA) in chiplet design. It shows improved generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs. This could lead to more efficient hardware design, potentially reducing energy consumption in computing, but it is still in the research phase with no deployed units.","key_impact_metrics":["generation accuracy of real-world chiplet design with Pass@5 of 0.72"],"technology_tags":["LLM","chiplet design","hardware description language","AI algorithm-hardware mapping"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:00:12.021067Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7c399053f6d5","title":"Long Chain","content":"arXiv:2508.14828v2 Announce Type: replace Abstract: While large reasoning models have shown remarkable ability to generate long chains-of-thought (CoTs) in English, we still lack understanding of how these long-form reasoning abilities transfer to the vast majority of the world's languages. In this work, we systematically investigate four key stages of model development--scaling, pretraining, post-training, and inference--to understand how long CoT capabilities extend beyond English. We compare two reasoning settings across nine non-English target languages: En-CoT, where models process target-language inputs, but reason in English; and Target-CoT, where models both process inputs and generate long CoTs in the target language. We find that scaling reasoning model size improves multilingual task performance in En-CoT, but Target-CoT performance lags behind. This gap widens for tasks requiring long, multi-step CoTs such as mathematical reasoning. Shifting to pretraining, we find that adding a specialized reasoning stage enhances En-CoT performance but degrades Target-CoT, whereas broad multilingual pretraining improves both modes simultaneously. Given the scarcity of high-quality reasoning traces in languages other than English, we explore synthetic data curation approaches for post-training. We demonstrate that fine-tuning on reasoning traces automatically translated from gold English traces outperforms fine-tuning on target-language traces distilled from large reasoning models. Finally, we report disparities in inference efficiency between languages and uncover language-specific failure modes in CoTs. We release models, datasets, and code to foster further research.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.14828","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.502682","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":223,"author":"Josh Barua, Seun Eisape, Kayo Yin, Alane Suhr","raw_content_length":1695,"priority":7,"update_frequency":1,"reading_time_minutes":1.115,"robust_parsing_used":true,"entities":{"organizations":["Target","Target-CoT","CoT","CoTs","Long Chain arXiv:2508.14828v2 Announce Type"],"persons":[],"locations":["En-CoT"],"monetary":[]},"char_count":1694,"language_detected":"en","key_concepts":{"key_phrases":["Long Chain","English","arXiv250814828v2","Announce Type","Abstract","large reasoning models","remarkable ability","long chains","thought","CoTs"],"filter_categories":{"ai_ml":["Long Chain"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Long Chain":2.0,"English":2.0,"arXiv250814828v2":1.0,"Announce Type":1.0,"Abstract":1.0,"large reasoning models":1.0,"remarkable ability":1.0,"long chains":1.0,"thought":1.0,"CoTs":1.0}},"age_hours":2.782063027222222,"is_recent":true,"quality_score":1.0,"sentiment_score":8.715,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.743,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.849,"joy":0.007,"surprise":0.0389,"sadness":0.0042,"fear":0.0516,"anger":0.0321,"disgust":0.0171},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores the transferability of long chain-of-thought reasoning in AI models to non-English languages. While the research itself doesn't directly impact climate change, it could potentially improve the accessibility and usability of climate-related information and tools in various languages, but this is theoretical at this stage. The research is in the early stages, focusing on model development and evaluation, without immediate deployment or measurable climate outcomes.","key_impact_metrics":[],"technology_tags":["Artificial Intelligence","Natural Language Processing","Machine Learning"],"sdg_alignment":[],"analyzed_at":"2025-10-28T21:00:15.577950Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f89bab1fc78a","title":"Multiple Memory Systems for Enhancing the Long","content":"arXiv:2508.15294v2 Announce Type: replace Abstract: An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.15294","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.503081","language":"en","tags":["csma","computer-science","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":204,"author":"Gaoke Zhang, Bo Wang, Yunlong Ma, Dongming Zhao, Zifei Yu","raw_content_length":1441,"priority":7,"update_frequency":1,"reading_time_minutes":1.02,"robust_parsing_used":true,"entities":{"organizations":["A-MEM","Multiple Memory Systems for Enhancing the Long arXiv:2508.15294v2 Announce Type","MemoryBank","MMS"],"persons":[],"locations":[],"monetary":[]},"char_count":1440,"language_detected":"en","key_concepts":{"key_phrases":["Multiple Memory Systems","arXiv250815294v2 Announce Type","Abstract","An agent","large language models","impressive results","the vast amounts","historical data","interactions","a challenge"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Multiple Memory Systems":2.0,"arXiv250815294v2 Announce Type":1.0,"Abstract":1.0,"An agent":1.0,"large language models":1.0,"impressive results":1.0,"the vast amounts":1.0,"historical data":1.0,"interactions":1.0,"a challenge":1.0}},"age_hours":2.7820777283333333,"is_recent":true,"quality_score":1.0,"sentiment_score":6.591,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3182,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8297,"joy":0.0056,"surprise":0.0438,"sadness":0.0575,"fear":0.0156,"anger":0.0215,"disgust":0.0264},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel memory system for large language models, showing improved performance on the LoCoMo dataset compared to existing methods. While the research demonstrates effectiveness and robustness in experiments, it remains in the applied research phase with no deployed units or real-world applications. The potential climate impact is indirect, as it could improve the efficiency of AI systems, but this is not quantified.","key_impact_metrics":["Improved recall performance on LoCoMo dataset","Storage overhead analysis"],"technology_tags":["Large Language Models","Memory Systems","Artificial Intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:00:18.677507Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_897719b8e4d7","title":"Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation","content":"arXiv:2508.17364v2 Announce Type: replace Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.17364","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.504690","language":"en","tags":["computer-science","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":206,"author":"Guoqing Zhang, Xingtong Ge, Lu Shi, Xin Zhang, Muqing Xue, Wanru Xu, Yigang Cen, Jian Zhang","raw_content_length":1718,"priority":7,"update_frequency":1,"reading_time_minutes":1.03,"robust_parsing_used":true,"entities":{"organizations":["UniGen","the Condition Modulated Expert (CoMoE"],"persons":[],"locations":[],"monetary":[]},"char_count":1717,"language_detected":"en","key_concepts":{"key_phrases":["Condition Weaving","Expert Modulation","Universal and Controllable Image Generation","image","arXiv250817364v2","Announce Type","Abstract","controllable images","conditional inputs","prompt instructions"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Condition Weaving":2.0,"Expert Modulation":2.0,"Universal and Controllable Image Generation":2.0,"image":2.0,"arXiv250817364v2":1.0,"Announce Type":1.0,"Abstract":1.0,"controllable images":1.0,"conditional inputs":1.0,"prompt instructions":1.0}},"age_hours":2.782137143611111,"is_recent":true,"quality_score":1.0,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9347,"joy":0.0073,"surprise":0.0203,"sadness":0.0084,"fear":0.0051,"anger":0.0178,"disgust":0.0064},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel image generation framework (UniGen) that aims to improve computational efficiency. While potentially impactful for reducing energy consumption in AI training, there are no concrete actions or measurable outcomes related to sustainability provided. The research is at a basic research stage with no deployment data.","key_impact_metrics":["State-of-the-art performance on Subjects-200K dataset","State-of-the-art performance on MultiGen-20M dataset"],"technology_tags":["Image generation","AI efficiency","Conditional image generation"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T21:00:22.186585Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_359780a108e9","title":"The Price of Uncertainty for Social Consensus","content":"arXiv:2508.17557v2 Announce Type: replace Abstract: How hard is it to achieve consensus in a social network under uncertainty? In this paper we model this problem as a social graph of agents where each vertex is initially colored red or blue. The goal of the agents is to achieve consensus, which is when the colors of all agents align. Agents attempt to do this locally through steps in which an agent changes their color to the color of the majority of their neighbors. In real life, agents may not know exactly how many of their neighbors are red or blue, which introduces uncertainty into this process. Modeling uncertainty as perturbations of relative magnitude $1+\\varepsilon$ to these color neighbor counts, we show that even small values of $\\varepsilon$ greatly hinder the ability to achieve consensus in a social network. We prove theoretically tight upper and lower bounds on the price of uncertainty, a metric defined by Balcan et al. to quantify the effect of uncertainty in network games.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.17557","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.505067","language":"en","tags":["csgt","csma","computer-science","preprints","cssi","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":166,"author":"Yunzhe Bai, Alec Sun","raw_content_length":1003,"priority":7,"update_frequency":1,"reading_time_minutes":0.83,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["\\varepsilon$"],"locations":[],"monetary":[]},"char_count":1002,"language_detected":"en","key_concepts":{"key_phrases":["The Price","Uncertainty","Social Consensus","consensus","agents","which","arXiv250817557v2 Announce Type","Abstract","a social network","uncertainty"],"filter_categories":{"ai_ml":["Uncertainty"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"The Price":2.0,"Uncertainty":2.0,"Social Consensus":2.0,"consensus":2.0,"agents":2.0,"which":2.0,"arXiv250817557v2 Announce Type":1.0,"Abstract":1.0,"a social network":1.0,"uncertainty":1.0}},"age_hours":2.7821521561111116,"is_recent":true,"quality_score":1.0,"sentiment_score":0.9565000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8087,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.6821,"joy":0.0081,"surprise":0.023,"sadness":0.0124,"fear":0.2394,"anger":0.0219,"disgust":0.0131},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper explores the theoretical impact of uncertainty on achieving social consensus, modeled as agents changing colors in a network. While technically credible due to its theoretical nature and potential for understanding social dynamics relevant to climate action, it lacks concrete actions, measurable outcomes, or deployment readiness. The impact on climate is indirect and speculative.","key_impact_metrics":[],"technology_tags":["social network analysis","consensus algorithms"],"sdg_alignment":[16],"analyzed_at":"2025-10-28T21:00:24.777296Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3ea9140fc32d","title":"Spotlight Attention: Towards Efficient LLM Generation via Non","content":"arXiv:2508.19740v4 Announce Type: replace Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla decoding.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.19740","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.505838","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":177,"author":"Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji","raw_content_length":1296,"priority":7,"update_frequency":1,"reading_time_minutes":0.885,"robust_parsing_used":true,"entities":{"organizations":["Spotlight Attention","Bradley-Terry"],"persons":["Spotlight Attention"],"locations":["Large"],"monetary":[]},"char_count":1295,"language_detected":"en","key_concepts":{"key_phrases":["Spotlight Attention","Efficient LLM Generation","Non","LLMs","arXiv250819740v4 Announce Type","Abstract","the key-value KV cache burden","Large Language Models","inference","critical KV caches"],"filter_categories":{"ai_ml":["Efficient LLM Generation","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Spotlight Attention":3.0,"Efficient LLM Generation":2.0,"Non":2.0,"LLMs":2.0,"arXiv250819740v4 Announce Type":1.0,"Abstract":1.0,"the key-value KV cache burden":1.0,"Large Language Models":1.0,"inference":1.0,"critical KV caches":1.0}},"age_hours":2.7821810830555553,"is_recent":true,"quality_score":1.0,"sentiment_score":6.5355,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3071,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.887,"joy":0.0155,"surprise":0.0155,"sadness":0.0089,"fear":0.0099,"anger":0.038,"disgust":0.0253},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method, Spotlight Attention, to improve the efficiency of LLM generation, which could reduce energy consumption. The method demonstrates a 3x higher throughput than vanilla decoding and hashing retrieval for 512K tokens in under 100s on a single A100 GPU. It's currently in the applied research stage, with experimental results but no deployed units.","key_impact_metrics":["3x higher throughput than vanilla decoding","Hashing retrieval for 512K tokens in under 100s"],"technology_tags":["Large Language Models","Non-linear Hashing","CUDA kernels"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-28T21:00:36.867139Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_87dddd84e921","title":"Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text","content":"arXiv:2508.21099v2 Announce Type: replace Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.21099","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.506902","language":"en","tags":["computer-science","csai","cscv","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":245,"author":"Xiangtao Meng, Yingkai Dong, Ning Yu, Li Wang, Zheng Li, Shanqing Guo","raw_content_length":1829,"priority":7,"update_frequency":1,"reading_time_minutes":1.225,"robust_parsing_used":true,"entities":{"organizations":["Safe-Control: A Safety Patch for Mitigating Unsafe Content","Safe-Control"],"persons":[],"locations":[],"monetary":[]},"char_count":1828,"language_detected":"en","key_concepts":{"key_phrases":["Safe-Control","A Safety Patch","Mitigating","Unsafe Content","Text","Announce Type","Abstract","the advancements","Image","T2I"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Safe-Control":2.0,"A Safety Patch":2.0,"Mitigating":2.0,"Unsafe Content":2.0,"Text":2.0,"Announce Type":1.0,"Abstract":1.0,"the advancements":1.0,"Image":1.0,"T2I":1.0}},"age_hours":2.782196566944444,"is_recent":true,"quality_score":1.0,"sentiment_score":8.453999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6908,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.3517,"joy":0.0048,"surprise":0.006,"sadness":0.042,"fear":0.5224,"anger":0.0478,"disgust":0.0253},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a safety patch for mitigating unsafe content generation in text-to-image models. While it shows a reduction in unsafe content generation to 7% compared to 20% for baseline methods, it is still in the applied research phase with no deployment. The impact on climate is indirect, as it addresses misuse of AI, not direct emissions.","key_impact_metrics":["unsafe content generation reduced to 7%","outperforms baselines by 13%"],"technology_tags":["AI safety","text-to-image generation","safety patch"],"sdg_alignment":[16],"analyzed_at":"2025-10-28T21:00:40.660324Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_328123a418ea","title":"Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine","content":"arXiv:2508.21589v4 Announce Type: replace Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.21589","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.507331","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":199,"author":"Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu","raw_content_length":1597,"priority":7,"update_frequency":1,"reading_time_minutes":0.995,"robust_parsing_used":true,"entities":{"organizations":["Model-Informed Dynamic Data Optimization for Enhanced LLM Fine arXiv:2508.21589v4 Announce Type: replace Abstract","SFT"],"persons":[],"locations":["Middo"],"monetary":[]},"char_count":1596,"language_detected":"en","key_concepts":{"key_phrases":["Middo","Model-Informed Dynamic Data Optimization","Enhanced LLM Fine","Announce Type","Abstract","Supervised Fine-Tuning SFT Large Language Models","LLM","high-quality training data","data selection and data synthesis","two common strategies"],"filter_categories":{"ai_ml":["Enhanced LLM Fine","Supervised Fine-Tuning SFT Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Middo":3.0,"Model-Informed Dynamic Data Optimization":2.0,"Enhanced LLM Fine":2.0,"Announce Type":1.0,"Abstract":1.0,"Supervised Fine-Tuning SFT Large Language Models":1.0,"LLM":1.0,"high-quality training data":1.0,"data selection and data synthesis":1.0,"two common strategies":1.0}},"age_hours":2.7822121661111114,"is_recent":true,"quality_score":1.0,"sentiment_score":8.2985,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6597,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9038,"joy":0.0044,"surprise":0.0175,"sadness":0.0191,"fear":0.0282,"anger":0.0133,"disgust":0.0137},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method (Middo) for improving LLM training data, potentially reducing the computational resources needed for training and improving model accuracy. While the accuracy improvement is quantified (7.15%), the actual reduction in energy consumption or carbon emissions is not. The research is in the applied research stage, with experiments on benchmarks but no real-world deployment data.","key_impact_metrics":["accuracy improving by 7.15%"],"technology_tags":["Large Language Models","Data Optimization","Machine Learning"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T21:00:43.577496Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_80eb21c73c0e","title":"Domain Generalization in-the","content":"arXiv:2508.21769v2 Announce Type: replace Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.21769","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.508158","language":"en","tags":["computer-science","cslg","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":218,"author":"Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu","raw_content_length":1709,"priority":7,"update_frequency":1,"reading_time_minutes":1.09,"robust_parsing_used":true,"entities":{"organizations":["ImageNet","CLIP-DCA","CLIP"],"persons":[],"locations":[],"monetary":[]},"char_count":1708,"language_detected":"en","key_concepts":{"key_phrases":["CLIP","Domain Generalization","arXiv250821769v2 Announce Type","Abstract","domain generalization","foundational models","web","scale pretraining data","many existing benchmarks","current DG evaluation"],"filter_categories":{"ai_ml":["Domain Generalization"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CLIP":3.0,"Domain Generalization":2.0,"arXiv250821769v2 Announce Type":1.0,"Abstract":1.0,"domain generalization":1.0,"foundational models":1.0,"web":1.0,"scale pretraining data":1.0,"many existing benchmarks":1.0,"current DG evaluation":1.0}},"age_hours":2.7822407205555555,"is_recent":true,"quality_score":1.0,"sentiment_score":8.658,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7316,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7958,"joy":0.0116,"surprise":0.0671,"sadness":0.012,"fear":0.0876,"anger":0.0211,"disgust":0.0048},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach (CLIP-DCA) to improve domain generalization in foundational models like CLIP, which could indirectly support climate-related applications by improving the accuracy of image recognition in diverse and unseen data scenarios. However, the direct climate impact is theoretical and unproven, as it's focused on improving AI model performance rather than deploying a specific climate technology. The research is in the applied research stage, with evaluations on datasets but no real-world deployment.","key_impact_metrics":["OOD scores on 33 diverse datasets","Performance improvements on OOD datasets"],"technology_tags":["Domain Generalization","CLIP","Machine Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:00:46.799798Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_99b28d749d1a","title":"AEGIS : Automated Co","content":"arXiv:2509.00088v2 Announce Type: replace Abstract: Prompt injection attacks pose a significant challenge to the safe deployment of Large Language Models (LLMs) in real-world applications. While prompt-based detection offers a lightweight and interpretable defense strategy, its effectiveness has been hindered by the need for manual prompt engineering. To address this issue, we propose AEGIS , an Automated co-Evolutionary framework for Guarding prompt Injections Schema. Both attack and defense prompts are iteratively optimized against each other using a gradient-like natural language prompt optimization technique. This framework enables both attackers and defenders to autonomously evolve via a Textual Gradient Optimization (TGO) module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our system on a real-world assignment grading dataset of prompt injection attacks and demonstrate that our method consistently outperforms existing baselines, achieving superior robustness in both attack success and detection. Specifically, the attack success rate (ASR) reaches 1.0, representing an improvement of 0.26 over the baseline. For detection, the true positive rate (TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and the true negative rate (TNR) remains comparable at 0.89. Ablation studies confirm the importance of co-evolution, gradient buffering, and multi-objective optimization. We also confirm that this framework is effective in different LLMs. Our results highlight the promise of adversarial training as a scalable and effective approach for guarding prompt injections.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.00088","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.508589","language":"en","tags":["computer-science","cslg","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":223,"author":"Ting-Chun Liu, Ching-Yu Hsu, Kuan-Yi Lee, Chi-An Fu, Hung-yi Lee","raw_content_length":1632,"priority":7,"update_frequency":1,"reading_time_minutes":1.115,"robust_parsing_used":true,"entities":{"organizations":["LLM","Automated Co arXiv:2509.00088v2 Announce Type","a Textual Gradient Optimization (TGO","Large Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1631,"language_detected":"en","key_concepts":{"key_phrases":["AEGIS","Automated Co","Announce Type","Abstract","Prompt injection attacks","a significant challenge","the safe deployment","Large Language Models","LLMs","real-world applications"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"AEGIS":3.0,"Automated Co":2.0,"Announce Type":1.0,"Abstract":1.0,"Prompt injection attacks":1.0,"a significant challenge":1.0,"the safe deployment":1.0,"Large Language Models":1.0,"LLMs":1.0,"real-world applications":1.0}},"age_hours":2.7822556983333335,"is_recent":true,"quality_score":1.0,"sentiment_score":4.36,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.128,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.3612,"joy":0.006,"surprise":0.0079,"sadness":0.0193,"fear":0.477,"anger":0.0929,"disgust":0.0357},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the robustness of LLMs against prompt injection attacks, which indirectly supports sustainability by enabling safer deployment of AI in various sectors, including those related to climate and environment. The article presents concrete metrics like attack success rate (ASR) and true positive rate (TPR) to demonstrate the effectiveness of the proposed framework. It is currently in the applied research stage, with no deployed units or customer contracts mentioned.","key_impact_metrics":["ASR reaches 1.0","TPR improves to 0.84"],"technology_tags":["Large Language Models","Prompt Injection Defense"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:00:49.518464Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e0eb78c3d952","title":"Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization","content":"arXiv:2509.00529v2 Announce Type: replace Abstract: Large Language Models (LLMs) are increasingly used to generate user-tailored summaries, adapting outputs to specific stakeholders. In legal contexts, this raises important questions about motivated reasoning -- how models strategically frame information to align with a stakeholder's position within the legal system. Building on theories of legal realism and recent trends in legal practice, we investigate how LLMs respond to prompts conditioned on different legal roles (e.g., judges, prosecutors, attorneys) when summarizing judicial decisions. We introduce an evaluation framework grounded in legal fact and reasoning inclusion, also considering favorability towards stakeholders. Our results show that even when prompts include balancing instructions, models exhibit selective inclusion patterns that reflect role-consistent perspectives. These findings raise broader concerns about how similar alignment may emerge as LLMs begin to infer user roles from prior interactions or context, even without explicit role instructions. Our results underscore the need for role-aware evaluation of LLM summarization behavior in high-stakes legal settings.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.00529","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.508974","language":"en","tags":["cscl","computer-science","preprints","cscy","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":159,"author":"Eunjung Cho, Alexander Hoyle, Yoan Hermstr\\\"uwer","raw_content_length":1204,"priority":7,"update_frequency":1,"reading_time_minutes":0.795,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1203,"language_detected":"en","key_concepts":{"key_phrases":["Modeling Motivated Reasoning","Law","Strategic Role Conditioning","LLM Summarization","LLMs","arXiv250900529v2 Announce Type","Large Language Models","user-tailored summaries","outputs","specific stakeholders"],"filter_categories":{"ai_ml":["LLM Summarization","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Modeling Motivated Reasoning":2.0,"Law":2.0,"Strategic Role Conditioning":2.0,"LLM Summarization":2.0,"LLMs":2.0,"arXiv250900529v2 Announce Type":1.0,"Large Language Models":1.0,"user-tailored summaries":1.0,"outputs":1.0,"specific stakeholders":1.0}},"age_hours":2.782270440833333,"is_recent":true,"quality_score":0.7,"sentiment_score":9.259500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8519,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8894,"joy":0.0181,"surprise":0.0403,"sadness":0.0034,"fear":0.0168,"anger":0.0253,"disgust":0.0067},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores the potential for LLMs to exhibit motivated reasoning in legal summarization. While it raises important ethical considerations about AI bias, it does not directly address climate change or environmental sustainability. The research is in an early stage, focusing on model evaluation rather than deployment of a specific technology.","key_impact_metrics":["Selective inclusion patterns reflecting role-consistent perspectives","Favorability towards stakeholders"],"technology_tags":["Large Language Models","AI Ethics","Legal Technology"],"sdg_alignment":[16],"analyzed_at":"2025-10-28T21:00:52.399014Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_fc9317817552","title":"A 13/6","content":"arXiv:2509.04654v2 Announce Type: replace Abstract: In the Strip Packing problem, we are given a vertical strip of fixed width and unbounded height, along with a set of axis-parallel rectangles. The task is to place all rectangles within the strip, without overlaps, while minimizing the height of the packing. This problem is known to be NP-hard. The Bottom-Left Algorithm is a simple and widely used heuristic for Strip Packing. Given a fixed order of the rectangles, it places them one by one, always choosing the lowest feasible position in the strip and, in case of ties, the leftmost one. Baker, Coffman, and Rivest proved in 1980 that the Bottom-Left Algorithm has approximation ratio 3 if the rectangles are sorted by decreasing width. For the past 45 years, no alternative ordering has been found that improves this bound. We introduce a new rectangle ordering and show that with this ordering the Bottom-Left Algorithm achieves a 13/6 approximation for the Strip Packing problem.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.04654","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.510938","language":"en","tags":["csds","mathco","computer-science","preprints","csdm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":160,"author":"Stefan Hougardy, Bart Zondervan","raw_content_length":990,"priority":7,"update_frequency":1,"reading_time_minutes":0.8,"robust_parsing_used":true,"entities":{"organizations":["Strip Packing"],"persons":["Baker","Rivest","Coffman"],"locations":["Strip"],"monetary":[]},"char_count":989,"language_detected":"en","key_concepts":{"key_phrases":["Announce Type","Abstract","the Strip Packing problem","a vertical strip","fixed width","unbounded height","a set","axis-parallel rectangles","The task","all rectangles"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Announce Type":1.0,"Abstract":1.0,"the Strip Packing problem":1.0,"a vertical strip":1.0,"fixed width":1.0,"unbounded height":1.0,"a set":1.0,"axis-parallel rectangles":1.0,"The task":1.0,"all rectangles":1.0}},"age_hours":2.7823459294444444,"is_recent":true,"quality_score":1.0,"sentiment_score":1.4865,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7027,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9362,"joy":0.0055,"surprise":0.0161,"sadness":0.0042,"fear":0.012,"anger":0.0136,"disgust":0.0124},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents a new algorithm that improves the approximation ratio for the Strip Packing problem. While not directly climate-related, more efficient packing algorithms can lead to reduced material usage and transportation costs, indirectly contributing to sustainability. The technical credibility is high due to the mathematical proof and the reference to a well-established algorithm.","key_impact_metrics":["Approximation ratio improvement from 3 to 13/6"],"technology_tags":["Optimization Algorithm","Strip Packing"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T21:00:57.492688Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f5acbad7632a","title":"HyPINO: Multi","content":"arXiv:2509.05117v2 Announce Type: replace Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of parametric PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parametrizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that compares the physics of the generated PINN to the requested PDE and uses the discrepancy to generate a \"delta\" PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves over 100x gain in average $L_2$ loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems. The code and model weights are publicly available at https://github.com/rbischof/hypino.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.05117","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.511351","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":254,"author":"Rafael Bischof, Michal Piovar\\v{c}i, Michael A. Kraus, Siddhartha Mishra, Bernd Bickel","raw_content_length":1923,"priority":7,"update_frequency":1,"reading_time_minutes":1.27,"robust_parsing_used":true,"entities":{"organizations":["Dirichlet/Neumann","Physics-Informed Neural Operators","the Method of Manufactured Solutions","Physics-Informed Neural Networks","U-Nets","PDE","linear","MMS"],"persons":["Poseidon","Swin Transformer"],"locations":["HyPINO"],"monetary":[]},"char_count":1922,"language_detected":"en","key_concepts":{"key_phrases":["HyPINO","Multi","arXiv250905117v2 Announce Type","Abstract","a multi-physics neural operator","zero-shot generalization","a broad class","parametric PDEs","task-specific fine-tuning","Our approach"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"HyPINO":3.0,"Multi":2.0,"arXiv250905117v2 Announce Type":1.0,"Abstract":1.0,"a multi-physics neural operator":1.0,"zero-shot generalization":1.0,"a broad class":1.0,"parametric PDEs":1.0,"task-specific fine-tuning":1.0,"Our approach":1.0}},"age_hours":2.782361352222222,"is_recent":true,"quality_score":1.0,"sentiment_score":8.2985,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6597,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8981,"joy":0.0333,"surprise":0.0456,"sadness":0.0049,"fear":0.0035,"anger":0.0101,"disgust":0.0045},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel neural operator (HyPINO) for solving PDEs, which could potentially accelerate the design and optimization of sustainable technologies. However, it's currently in the applied research stage with no real-world deployments. The 100x gain in L2 loss is a significant metric, but its direct impact on climate change is not yet quantified.","key_impact_metrics":["100x gain in average $L_2$ loss"],"technology_tags":["Neural Operator","Physics-Informed Neural Networks","PDE Solver"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:01.030312Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9cdfd55dccc4","title":"Coefficients","content":"arXiv:2509.05952v3 Announce Type: replace Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.05952","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.513077","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":166,"author":"Feng Wang, Zihao Yu","raw_content_length":1262,"priority":7,"update_frequency":1,"reading_time_minutes":0.83,"robust_parsing_used":true,"entities":{"organizations":["Stochastic Differential Equation","Coefficients-Preserving Sampling","Reinforcement Learning","Diffusion and Flow Matching","Denoising Diffusion Implicit Models","CPS"],"persons":["Coefficients arXiv:2509.05952v3 Announce Type"],"locations":[],"monetary":[]},"char_count":1261,"language_detected":"en","key_concepts":{"key_phrases":["Coefficients","arXiv250905952v3 Announce Type","Abstract","Reinforcement Learning","a powerful technique","image","video generation","Diffusion and Flow Matching models","output quality","alignment"],"filter_categories":{"ai_ml":["Reinforcement Learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Coefficients":2.0,"arXiv250905952v3 Announce Type":1.0,"Abstract":1.0,"Reinforcement Learning":1.0,"a powerful technique":1.0,"image":1.0,"video generation":1.0,"Diffusion and Flow Matching models":1.0,"output quality":1.0,"alignment":1.0}},"age_hours":2.782388919166667,"is_recent":true,"quality_score":1.0,"sentiment_score":7.553000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5106,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8417,"joy":0.0469,"surprise":0.0841,"sadness":0.0039,"fear":0.0071,"anger":0.0117,"disgust":0.0046},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel approach to improving image generation using reinforcement learning, specifically addressing noise artifacts in Flow Matching models. The concrete action is the development of a new sampling method (CPS) that eliminates noise. While the research is theoretically sound and shows promise, it's currently in the basic research stage with no deployed technology or measured outcomes in a real-world setting, limiting its immediate sustainability impact.","key_impact_metrics":[],"technology_tags":["Reinforcement Learning","Image Generation","Diffusion Models","Flow Matching"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:04.382126Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a1c364fea6a2","title":"Scaling up Multi-Turn Off-Policy RL and Multi","content":"arXiv:2509.06493v2 Announce Type: replace Abstract: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.06493","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.513959","language":"en","tags":["research","csai","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":236,"author":"Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao","raw_content_length":1761,"priority":7,"update_frequency":1,"reading_time_minutes":1.18,"robust_parsing_used":true,"entities":{"organizations":["LLM","AlphaZero","Large Language Models"],"persons":["\\texttt{BFS-Prover-V2"],"locations":[],"monetary":[]},"char_count":1760,"language_detected":"en","key_concepts":{"key_phrases":["Multi-Turn Off-Policy RL","Multi","arXiv250906493v2 Announce Type","Abstract","The integration","Large Language Models","LLMs","automated theorem proving","immense promise","challenges"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Multi-Turn Off-Policy RL":2.0,"Multi":2.0,"arXiv250906493v2 Announce Type":1.0,"Abstract":1.0,"The integration":1.0,"Large Language Models":1.0,"LLMs":1.0,"automated theorem proving":1.0,"immense promise":1.0,"challenges":1.0}},"age_hours":2.782418779722222,"is_recent":true,"quality_score":1.0,"sentiment_score":6.0115,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8301,"joy":0.0576,"surprise":0.0866,"sadness":0.0037,"fear":0.0087,"anger":0.0101,"disgust":0.0032},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel RL framework and multi-agent search architecture for automated theorem proving, achieving state-of-the-art results on MiniF2F (95.08%) and ProofNet (41.4%) test sets. While the techniques are demonstrated in formal mathematics, the potential for broader application to long-horizon reasoning problems is noted. However, there is no concrete deployment in a sustainability-related domain, and the economic viability is unclear outside of its current application.","key_impact_metrics":["MiniF2F score: 95.08%","ProofNet score: 41.4%"],"technology_tags":["Reinforcement Learning","Large Language Models","Automated Theorem Proving"],"sdg_alignment":[],"analyzed_at":"2025-10-28T21:01:07.762931Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ee90e26c8917","title":"Barycentric Neural Networks and Length","content":"arXiv:2509.06694v3 Announce Type: replace Abstract: While artificial neural networks are known as universal approximators for continuous functions, many modern approaches rely on overparameterized architectures with high computational cost. In this work, we introduce the Barycentric Neural Network (BNN): a compact shallow architecture that encodes both structure and parameters through a fixed set of base points and their associated barycentric coordinates. We show that the BNN enables the exact representation of continuous piecewise linear functions (CPLFs), ensuring strict continuity across segments. Given that any continuous function on a compact domain can be uniformly approximated by CPLFs, the BNN emerges as a flexible and interpretable tool for function approximation. To enhance geometric fidelity in low-resource scenarios, such as those with few base points to create BNNs or limited training epochs, we propose length-weighted persistent entropy (LWPE): a stable variant of persistent entropy. Our approach integrates the BNN with a loss function based on LWPE to optimize the base points that define the BNN, rather than its internal parameters. Experimental results show that our approach achieves superior and faster approximation performance compared to standard losses (MSE, RMSE, MAE and LogCosh), offering a computationally sustainable alternative for function approximation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.06694","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.514792","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Victor Toscano-Duran, Rocio Gonzalez-Diaz, Miguel A. Guti\\'errez-Naranjo","raw_content_length":1405,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":["Barycentric Neural Networks and Length arXiv:2509.06694v3","BNN","the Barycentric Neural Network"],"persons":[],"locations":[],"monetary":[]},"char_count":1402,"language_detected":"en","key_concepts":{"key_phrases":["Barycentric Neural Networks","Length","Announce Type","Abstract","artificial neural networks","universal approximators","continuous functions","many modern approaches","overparameterized architectures","high computational cost"],"filter_categories":{"ai_ml":["Barycentric Neural Networks","artificial neural networks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Barycentric Neural Networks":2.0,"Length":2.0,"Announce Type":1.0,"Abstract":1.0,"artificial neural networks":1.0,"universal approximators":1.0,"continuous functions":1.0,"many modern approaches":1.0,"overparameterized architectures":1.0,"high computational cost":1.0}},"age_hours":2.7824480627777777,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9151,"joy":0.0104,"surprise":0.0274,"sadness":0.0046,"fear":0.0082,"anger":0.0144,"disgust":0.0199},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel neural network architecture (BNN) that offers a computationally sustainable alternative for function approximation, achieving superior and faster approximation performance compared to standard losses. This could lead to reduced energy consumption in computation. However, it is still in the applied research phase and lacks deployment data, hence the lower scores for deployment readiness and systemic impact.","key_impact_metrics":["faster approximation performance","computationally sustainable"],"technology_tags":["neural networks","function approximation","machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:10.961995Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6ba4efc3d2e3","title":"Bringing Graphs to the Table: Zero","content":"arXiv:2509.07143v2 Announce Type: replace Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm for achieving broad generalization across various graph data. However, existing GFMs are often trained on datasets that may not fully reflect real-world graphs, limiting their generalization performance. In contrast, tabular foundation models (TFMs) not only excel at classical tabular prediction tasks but have also shown strong applicability in other domains such as time series forecasting, natural language processing, and computer vision. Motivated by this, we take an alternative view to the standard perspective of GFMs and reformulate node classification as a tabular problem. In this reformulation, each node is represented as a row with feature, structure, and label information as columns, enabling TFMs to directly perform zero-shot node classification via in-context learning. In this work, we introduce TAG, a tabular approach for graph learning that first converts a graph into a table via feature and structural encoders, applies multiple TFMs to diversely subsampled tables, and then aggregates their outputs through ensemble selection. Experiments on 28 real-world datasets demonstrate that TAG consistently improves upon task-specific GNNs and state-of-the-art GFMs, highlighting the potential of the tabular reformulation for scalable and generalizable graph learning.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.07143","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.515602","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Adrian Hayler, Xingyue Huang, \\.Ismail \\.Ilkan Ceylan, Michael Bronstein, Ben Finkelshtein","raw_content_length":1418,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["TAG"],"persons":["Bringing Graphs"],"locations":["node"],"monetary":[]},"char_count":1417,"language_detected":"en","key_concepts":{"key_phrases":["Graphs","the Table","Zero","arXiv250907143v2 Announce Type","Abstract","Graph foundation models","GFMs","a promising paradigm","broad generalization","various graph data"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Graphs":2.0,"the Table":2.0,"Zero":2.0,"arXiv250907143v2 Announce Type":1.0,"Abstract":1.0,"Graph foundation models":1.0,"GFMs":1.0,"a promising paradigm":1.0,"broad generalization":1.0,"various graph data":1.0}},"age_hours":2.7824778505555554,"is_recent":true,"quality_score":1.0,"sentiment_score":8.3835,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6767,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8809,"joy":0.0096,"surprise":0.0189,"sadness":0.0064,"fear":0.0281,"anger":0.0301,"disgust":0.026},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to graph learning using tabular foundation models, showing improved performance on 28 real-world datasets. While the approach could potentially improve the efficiency of various applications that could have sustainability implications, there are no concrete deployments or quantified climate impacts at this stage. It's still in the applied research phase, with potential but unproven economic viability and deployment readiness.","key_impact_metrics":["Improvements on 28 real-world datasets","Zero-shot node classification"],"technology_tags":["Graph Foundation Models","Tabular Foundation Models","Machine Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:14.177128Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3b74b80bd2b3","title":"A Survey of Reinforcement Learning for Large Reasoning Models","content":"arXiv:2509.08827v3 Announce Type: replace Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.08827","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.516414","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":178,"author":"Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou","raw_content_length":1281,"priority":7,"update_frequency":1,"reading_time_minutes":0.89,"robust_parsing_used":true,"entities":{"organizations":["LLM","Reinforcement Learning","Large Language Models","Artificial SuperIntelligence"],"persons":[],"locations":["LRMs"],"monetary":[]},"char_count":1280,"language_detected":"en","key_concepts":{"key_phrases":["Reinforcement Learning","A Survey","Large Reasoning Models","LLMs","LRMs","Announce Type","Abstract","this paper","recent advances","reasoning"],"filter_categories":{"ai_ml":["Reinforcement Learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Reinforcement Learning":3.0,"A Survey":2.0,"Large Reasoning Models":2.0,"LLMs":2.0,"LRMs":2.0,"Announce Type":1.0,"Abstract":1.0,"this paper":1.0,"recent advances":1.0,"reasoning":1.0}},"age_hours":2.782508131111111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.036999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8074,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6853,"joy":0.0542,"surprise":0.2057,"sadness":0.0057,"fear":0.0187,"anger":0.0233,"disgust":0.007},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article surveys reinforcement learning for large language models, focusing on reasoning abilities. While it identifies challenges and opportunities, it doesn't present concrete actions or measurable outcomes related to sustainability. The research is at a basic research stage, with no deployed technology or quantified impact data.","key_impact_metrics":[],"technology_tags":["Reinforcement Learning","Large Language Models"],"sdg_alignment":[],"analyzed_at":"2025-10-28T21:01:16.871757Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f8fa3eec93b0","title":"Neuro","content":"arXiv:2509.11943v2 Announce Type: replace Abstract: The development of intelligent agents, particularly those powered by language models (LMs), has shown the critical role in various environments that require intelligent and autonomous decision. Environments are not passive testing grounds and they represent the data required for agents to learn and exhibit very challenging conditions that require adaptive, complex and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \\emph{possibility} and \\emph{necessity} using the formal language of modal logic. In this work, we use of immutable, domain-specific knowledge to make infere information, which is encoded as logical constraints essential for proper diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.11943","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.518172","language":"en","tags":["csma","cslo","computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":238,"author":"Antonin Sulc, Thorsten Hellert","raw_content_length":1737,"priority":7,"update_frequency":1,"reading_time_minutes":1.19,"robust_parsing_used":true,"entities":{"organizations":["Kripke","fidelity"],"persons":["Neuro"],"locations":[],"monetary":[]},"char_count":1736,"language_detected":"en","key_concepts":{"key_phrases":["Neuro","arXiv250911943v2 Announce Type","Abstract","The development","intelligent agents","particularly those","language models","LMs","the critical role","various environments"],"filter_categories":{"engineering":["The development"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Neuro":2.0,"arXiv250911943v2 Announce Type":1.0,"Abstract":1.0,"The development":1.0,"intelligent agents":1.0,"particularly those":1.0,"language models":1.0,"LMs":1.0,"the critical role":1.0,"various environments":1.0}},"age_hours":2.7825642522222225,"is_recent":true,"quality_score":0.7,"sentiment_score":8.0625,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6125,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.87,"joy":0.0124,"surprise":0.0755,"sadness":0.0058,"fear":0.0166,"anger":0.0134,"disgust":0.0062},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a neuro-symbolic multi-agent architecture that uses logical constraints to guide language models in a simulated particle accelerator environment. The system successfully diagnoses complex failures, showcasing a path toward more robust autonomous agents. While promising, it's currently in a simulated environment, lacking real-world deployment and quantified climate impact.","key_impact_metrics":["Successfully diagnoses complex, cascading failures"],"technology_tags":["Neuro-symbolic AI","Language Models","Autonomous Agents"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:19.665430Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_25a140cb135d","title":"Reproducible workflow for online AI in digital health","content":"arXiv:2509.13499v2 Announce Type: replace Abstract: Online artificial intelligence (AI) algorithms are an important component of digital health interventions. These online algorithms are designed to continually learn and improve their performance as streaming data is collected on individuals. Deploying online AI presents a key challenge: balancing adaptability of online AI with reproducibility. Online AI in digital interventions is a rapidly evolving area, driven by advances in algorithms, sensors, software, and devices. Digital health intervention development and deployment is a continuous process, where implementation - including the AI decision-making algorithm - is interspersed with cycles of re-development and optimization. Each deployment informs the next, making iterative deployment a defining characteristic of this field. This iterative nature underscores the importance of reproducibility: data collected across deployments must be accurately stored to have scientific utility, algorithm behavior must be auditable, and results must be comparable over time to facilitate scientific discovery and trustworthy refinement. This paper proposes a reproducible scientific workflow for developing, deploying, and analyzing online AI decision-making algorithms in digital health interventions. Grounded in practical experience from multiple real-world deployments, this workflow addresses key challenges to reproducibility across all phases of the online AI algorithm development life-cycle.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.13499","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.518621","language":"en","tags":["computer-science","csai","preprints","cscy","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Susobhan Ghosh, Bhanu T. Gullapalli, Daiqi Gao, Asim Gazi, Anna Trella, Ziping Xu, Kelly Zhang, Susan A. Murphy","raw_content_length":1505,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":["Digital"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1504,"language_detected":"en","key_concepts":{"key_phrases":["online AI","digital health","arXiv250913499v2 Announce Type","Abstract","an important component","digital health interventions","These online algorithms","their performance","streaming data","individuals"],"filter_categories":{"ai_ml":["online AI","These online algorithms"],"healthcare_tech":["digital health","digital health interventions"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"online AI":4.0,"digital health":2.0,"arXiv250913499v2 Announce Type":1.0,"Abstract":1.0,"an important component":1.0,"digital health interventions":1.0,"These online algorithms":1.0,"their performance":1.0,"streaming data":1.0,"individuals":1.0}},"age_hours":2.7825779075000003,"is_recent":true,"quality_score":1.0,"sentiment_score":8.982,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7964,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8791,"joy":0.0206,"surprise":0.0564,"sadness":0.005,"fear":0.0177,"anger":0.0158,"disgust":0.0054},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":4,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":true},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"The article proposes a reproducible workflow for online AI in digital health, grounded in practical experience from real-world deployments. While it addresses reproducibility challenges, it lacks specific quantified metrics on environmental impact or resource consumption. The stage is applied research, moving towards pilot deployments.","key_impact_metrics":[],"technology_tags":["online AI","digital health","reproducible workflow"],"sdg_alignment":[3],"analyzed_at":"2025-10-28T21:01:22.144220Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1f16f345e19d","title":"ProtoMedX: Towards Explainable Multi","content":"arXiv:2509.14830v2 Announce Type: replace Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.14830","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.519448","language":"en","tags":["computer-science","cslg","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":179,"author":"Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns","raw_content_length":1274,"priority":7,"update_frequency":1,"reading_time_minutes":0.895,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["ProtoMedX"],"locations":["Osteopenia"],"monetary":[]},"char_count":1273,"language_detected":"en","key_concepts":{"key_phrases":["ProtoMedX","Explainable Multi","arXiv250914830v2","Announce Type","Abstract","Bone health studies","medical practice","the early detection","treatment","Osteopenia"],"filter_categories":{"ai_ml":["Explainable Multi"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"ProtoMedX":2.0,"Explainable Multi":2.0,"arXiv250914830v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Bone health studies":1.0,"medical practice":1.0,"the early detection":1.0,"treatment":1.0,"Osteopenia":1.0}},"age_hours":2.782607526388889,"is_recent":true,"quality_score":1.0,"sentiment_score":8.120000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.624,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8846,"joy":0.0338,"surprise":0.0497,"sadness":0.0043,"fear":0.0145,"anger":0.0094,"disgust":0.0036},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a prototype AI model (ProtoMedX) for bone health classification. While it achieves high accuracy (87.58% vision-only, 89.8% multi-modal) on a dataset of 4,160 NHS patients, it's still in the applied research stage with no mention of deployment or commercialization. The climate impact is minimal, but the technical credibility is decent due to the use of real patient data and quantifiable performance metrics.","key_impact_metrics":["87.58% accuracy in vision-only tasks","89.8% accuracy in multi-modal variant"],"technology_tags":["AI","Machine Learning","Medical Imaging"],"sdg_alignment":[3],"analyzed_at":"2025-10-28T21:01:25.554824Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9e5931352b92","title":"DNA-DetectLLM: Unveiling AI-Generated Text via a DNA","content":"arXiv:2509.15550v2 Announce Type: replace Abstract: The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC and 2.08% in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.15550","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.519870","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Xiaowei Zhu, Yubing Ren, Fang Fang, Qingfeng Tan, Shi Wang, Yanan Cao","raw_content_length":1581,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1580,"language_detected":"en","key_concepts":{"key_phrases":["DNA-DetectLLM Unveiling AI-Generated Text","a DNA","arXiv250915550v2 Announce Type","Abstract","The rapid advancement","large language models","LLMs","the line","AI-generated and human-written text","This progress"],"filter_categories":{"ai_ml":["DNA-DetectLLM Unveiling AI-Generated Text","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"DNA-DetectLLM Unveiling AI-Generated Text":2.0,"a DNA":2.0,"arXiv250915550v2 Announce Type":1.0,"Abstract":1.0,"The rapid advancement":1.0,"large language models":1.0,"LLMs":1.0,"the line":1.0,"AI-generated and human-written text":1.0,"This progress":1.0}},"age_hours":2.7826221777777773,"is_recent":true,"quality_score":0.7,"sentiment_score":8.478,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6956,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8199,"joy":0.0077,"surprise":0.0241,"sadness":0.0078,"fear":0.12,"anger":0.0158,"disgust":0.0046},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research presents a novel AI-generated text detection method. While it achieves state-of-the-art detection performance with quantified improvements in AUROC and F1 score, it is still in the applied research phase with no clear path to economic viability or deployment readiness. The impact on climate change is indirect, as it addresses misinformation, which can influence climate action.","key_impact_metrics":["AUROC improvement: 5.55%","F1 score improvement: 2.08%"],"technology_tags":["AI-generated text detection","Large Language Models"],"sdg_alignment":[16],"analyzed_at":"2025-10-28T21:01:28.417371Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_60b66ca53968","title":"DiffEye: Diffusion","content":"arXiv:2509.16767v2 Announce Type: replace Abstract: Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: https://diff-eye.github.io/","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.16767","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.520699","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":263,"author":"Ozgur Kara, Harris Nisar, James M. Rehg","raw_content_length":1913,"priority":7,"update_frequency":1,"reading_time_minutes":1.315,"robust_parsing_used":true,"entities":{"organizations":["Corresponding Positional Embedding"],"persons":["DiffEye"],"locations":[],"monetary":[]},"char_count":1912,"language_detected":"en","key_concepts":{"key_phrases":["DiffEye","Diffusion","which","arXiv250916767v2 Announce Type","Abstract","Numerous models","scanpath and saliency prediction","scanpaths","eye movement","a sequence"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"DiffEye":2.0,"Diffusion":2.0,"which":2.0,"arXiv250916767v2 Announce Type":1.0,"Abstract":1.0,"Numerous models":1.0,"scanpath and saliency prediction":1.0,"scanpaths":1.0,"eye movement":1.0,"a sequence":1.0}},"age_hours":2.7826507941666665,"is_recent":true,"quality_score":1.0,"sentiment_score":3.1279999999999997,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3744,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8038,"joy":0.0029,"surprise":0.02,"sadness":0.0266,"fear":0.0227,"anger":0.0522,"disgust":0.0719},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":2,"systemic_impact":1,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method, DiffEye, for modeling eye movement trajectories using diffusion models. While the research is technically sound and demonstrates state-of-the-art performance in scanpath generation, it is still in the applied research phase with no deployed units or real-world applications directly impacting sustainability. The potential climate impact is minimal as the technology is not directly related to GHG emissions reduction or climate adaptation.","key_impact_metrics":["State-of-the-art performance in scanpath generation"],"technology_tags":["Diffusion Models","Eye Tracking","Machine Learning"],"sdg_alignment":[],"analyzed_at":"2025-10-28T21:01:32.667764Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_03016aeaa5cd","title":"Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding","content":"arXiv:2509.18085v2 Announce Type: replace Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\\mathbf{7.9\\times}$.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.18085","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.521521","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":257,"author":"Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli","raw_content_length":1849,"priority":7,"update_frequency":1,"reading_time_minutes":1.285,"robust_parsing_used":true,"entities":{"organizations":["Spiffy","Lossless Speculative Decoding arXiv:2509.18085v2 Announce Type"],"persons":["dLLMs"],"locations":[],"monetary":[]},"char_count":1848,"language_detected":"en","key_concepts":{"key_phrases":["Spiffy","Multiplying Diffusion LLM Acceleration","Lossless Speculative Decoding","arXiv250918085v2 Announce Type","Abstract","Diffusion LLMs","a powerful alternative","autoregressive LLMs","AR-LLMs","the potential"],"filter_categories":{"ai_ml":["Multiplying Diffusion LLM Acceleration"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Spiffy":3.0,"Multiplying Diffusion LLM Acceleration":2.0,"Lossless Speculative Decoding":2.0,"arXiv250918085v2 Announce Type":1.0,"Abstract":1.0,"Diffusion LLMs":1.0,"a powerful alternative":1.0,"autoregressive LLMs":1.0,"AR-LLMs":1.0,"the potential":1.0}},"age_hours":2.7826802302777778,"is_recent":true,"quality_score":1.0,"sentiment_score":6.25,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.25,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8883,"joy":0.0333,"surprise":0.0578,"sadness":0.0041,"fear":0.0046,"anger":0.0088,"disgust":0.003},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new algorithm, Spiffy, that accelerates diffusion LLM inference, potentially reducing the energy consumption of AI computation. The algorithm achieves a 2.8-3.1x speedup, and up to 7.9x when combined with other methods, but it's currently in the research phase with no deployed systems or real-world data on energy savings.","key_impact_metrics":["2.8-3.1x speedup","7.9x total speedup"],"technology_tags":["diffusion LLM","speculative decoding","AI acceleration"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T21:01:35.931772Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3b0843c379c5","title":"Speculate Deep and Accurate: Lossless and Training","content":"arXiv:2509.18344v2 Announce Type: replace Abstract: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.18344","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.521936","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":236,"author":"Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu","raw_content_length":1722,"priority":7,"update_frequency":1,"reading_time_minutes":1.18,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Speculate Deep"],"locations":[],"monetary":[]},"char_count":1721,"language_detected":"en","key_concepts":{"key_phrases":["Deep","Accurate","Lossless","Training","parameter offloading","quality","Announce Type","Abstract","The immense model sizes","large language models"],"filter_categories":{"ai_ml":["Deep","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Deep":2.0,"Accurate":2.0,"Lossless":2.0,"Training":2.0,"parameter offloading":2.0,"quality":2.0,"Announce Type":1.0,"Abstract":1.0,"The immense model sizes":1.0,"large language models":1.0}},"age_hours":2.782694111666667,"is_recent":true,"quality_score":1.0,"sentiment_score":3.9884999999999997,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2023,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9491,"joy":0.0059,"surprise":0.0108,"sadness":0.0054,"fear":0.0097,"anger":0.0086,"disgust":0.0104},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":6,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (SubSpec) for accelerating LLM inference on memory-limited GPUs, potentially reducing energy consumption by enabling more efficient use of existing hardware. The method achieves significant speedups (9.1x and 12.5x) on specific models and benchmarks, indicating a potential for reduced energy usage per inference. However, it is still in the applied research stage with no deployed units or independent verification.","key_impact_metrics":["9.1x speedup for Qwen2.5 7B on MT-Bench","12.5x speedup for Qwen2.5 32B on popular generation benchmarks"],"technology_tags":["Large Language Models","Model Compression","Speculative Decoding","Quantization"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-28T21:01:39.962931Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_5543ce978d69","title":"Efficient Kernelized Learning in Polyhedral Games Beyond Full","content":"arXiv:2509.20919v2 Announce Type: replace Abstract: We examine the problem of efficiently learning coarse correlated equilibria (CCE) in polyhedral games, that is, normal-form games with an exponentially large number of actions per player and an underlying combinatorial structure. Prominent examples of such games are the classical Colonel Blotto and congestion games. To achieve computational efficiency, the learning algorithms must exhibit regret and per-iteration complexity that scale polylogarithmically in the size of the players' action sets. This challenge has recently been addressed in the full-information setting, primarily through the use of kernelization. However, in the case of the realistic, but mathematically challenging, partial-information setting, existing approaches result in suboptimal and impractical runtime complexity to learn CCE. We tackle this limitation by building a framework based on the kernelization paradigm. We apply this framework to prominent examples of polyhedral games -- namely the Colonel Blotto, graphic matroid and network congestion games -- and provide computationally efficient payoff-based learning algorithms, which significantly improve upon prior works in terms of the runtime for learning CCE in these settings.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.20919","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.522744","language":"en","tags":["csgt","research","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":173,"author":"Andreas Kontogiannis, Vasilis Pollatos, Gabriele Farina, Panayotis Mertikopoulos, Ioannis Panageas","raw_content_length":1270,"priority":7,"update_frequency":1,"reading_time_minutes":0.865,"robust_parsing_used":true,"entities":{"organizations":["CCE"],"persons":["Blotto"],"locations":[],"monetary":[]},"char_count":1269,"language_detected":"en","key_concepts":{"key_phrases":["Efficient Kernelized Learning","Polyhedral Games","Full","arXiv250920919v2","Announce Type","Abstract","the problem","coarse correlated equilibria","CCE","polyhedral games"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Efficient Kernelized Learning":2.0,"Polyhedral Games":2.0,"Full":2.0,"arXiv250920919v2":1.0,"Announce Type":1.0,"Abstract":1.0,"the problem":1.0,"coarse correlated equilibria":1.0,"CCE":1.0,"polyhedral games":1.0}},"age_hours":2.7827226772222224,"is_recent":true,"quality_score":1.0,"sentiment_score":8.9225,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7845,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8794,"joy":0.0328,"surprise":0.0582,"sadness":0.0068,"fear":0.0044,"anger":0.013,"disgust":0.0054},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a theoretical framework for improving the efficiency of learning algorithms in polyhedral games, which could potentially lead to better resource allocation in areas like congestion management. However, it is still in the research phase with no deployed technology or measured outcomes. The potential climate impact is indirect and not quantified.","key_impact_metrics":[],"technology_tags":["kernelized learning","coarse correlated equilibria","polyhedral games"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:43.699652Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_980e4a9f5bca","title":"MORPH: Shape","content":"arXiv:2509.21670v2 Announce Type: replace Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.21670","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.523373","language":"en","tags":["physicscomp-ph","computer-science","cslg","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Ayan Biswas, Diane Oyen, Earl Lawrence","raw_content_length":1627,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["PDE"],"persons":[],"locations":[],"monetary":[]},"char_count":1626,"language_detected":"en","key_concepts":{"key_phrases":["MORPH","Shape","arXiv250921670v2 Announce Type","Abstract","a shape-agnostic autoregressive foundation model","partial differential equations","PDEs","a convolutional vision transformer backbone","heterogeneous spatiotemporal datasets","varying data dimensionality"],"filter_categories":{"engineering":["heterogeneous spatiotemporal datasets"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"MORPH":4.0,"Shape":2.0,"arXiv250921670v2 Announce Type":1.0,"Abstract":1.0,"a shape-agnostic autoregressive foundation model":1.0,"partial differential equations":1.0,"PDEs":1.0,"a convolutional vision transformer backbone":1.0,"heterogeneous spatiotemporal datasets":1.0,"varying data dimensionality":1.0}},"age_hours":2.7827368005555555,"is_recent":true,"quality_score":1.0,"sentiment_score":6.25,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.25,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8845,"joy":0.0284,"surprise":0.0654,"sadness":0.0044,"fear":0.0072,"anger":0.0069,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new model (MORPH) for solving PDEs, which could potentially accelerate scientific discovery and engineering design in areas relevant to sustainability. However, it is currently in the basic research phase with no deployed applications or quantified impact on emissions reduction. The open-source code and datasets enhance transparency and reproducibility, but economic viability and deployment readiness are low at this stage.","key_impact_metrics":[],"technology_tags":["machine learning","partial differential equations","scientific computing"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T21:01:47.535298Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7266f5150dcc","title":"QoNext: Towards Next","content":"arXiv:2509.21889v2 Announce Type: replace Abstract: Existing evaluations of foundation models, including recent human-centric approaches, fail to capture what truly matters: user's experience during interaction. Current methods treat evaluation as a matter of output correctness alone, overlooking that user satisfaction emerges from the interplay between response quality and interaction, which limits their ability to account for the mechanisms underlying user experience. To address this gap, we introduce QoNext, the first framework that adapts Quality of Experience (QoE) principles from networking and multimedia to the assessment of foundation models. QoNext identifies experiential factors that shape user experience and incorporates them into controlled experiments, where human ratings are collected under varied configurations. From these studies we construct a QoE-oriented database and train predictive models that estimate perceived user experience from measurable system parameters. Our results demonstrate that QoNext not only enables proactive and fine-grained evaluation but also provides actionable guidance for productized services of optimizing foundation models in practice.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.21889","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.524162","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":156,"author":"Yijin Guo, Zicheng Zhang, Ye Shen, Farong Wen, Junying Wang, Qi Jia, Guangtao Zhai","raw_content_length":1197,"priority":7,"update_frequency":1,"reading_time_minutes":0.78,"robust_parsing_used":true,"entities":{"organizations":["QoNext","Quality of Experience (QoE"],"persons":["QoNext","Announce Type"],"locations":[],"monetary":[]},"char_count":1196,"language_detected":"en","key_concepts":{"key_phrases":["QoNext","interaction","Announce Type","Abstract","Existing evaluations","foundation models","recent human-centric approaches","what","users experience","Current methods"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"QoNext":2.0,"interaction":2.0,"Announce Type":1.0,"Abstract":1.0,"Existing evaluations":1.0,"foundation models":1.0,"recent human-centric approaches":1.0,"what":1.0,"users experience":1.0,"Current methods":1.0}},"age_hours":2.7827646944444444,"is_recent":true,"quality_score":1.0,"sentiment_score":8.352500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6705,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7445,"joy":0.0051,"surprise":0.0383,"sadness":0.1027,"fear":0.0118,"anger":0.0512,"disgust":0.0465},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a framework (QoNext) for evaluating foundation models based on user experience. While it aims to improve the optimization of these models, there's no concrete evidence of direct GHG emission reduction or other environmental benefits. The framework is in the applied research stage, with controlled experiments and database construction, but no deployed technology or measurable outcomes related to sustainability are mentioned.","key_impact_metrics":["User satisfaction ratings","measurable system parameters"],"technology_tags":["Foundation models","Quality of Experience (QoE)"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:01:50.534998Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a76417284dd3","title":"InfiR2: A Comprehensive FP8 Training Recipe for Reasoning","content":"arXiv:2509.22536v2 Announce Type: replace Abstract: The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.22536","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.524581","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":179,"author":"Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Yuan Xie, Hongxia Yang","raw_content_length":1303,"priority":7,"update_frequency":1,"reading_time_minutes":0.895,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1302,"language_detected":"en","key_concepts":{"key_phrases":["InfiR2 A Comprehensive FP8 Training Recipe","Reasoning","Announce Type","Abstract","The immense computational cost","Large Language Models","LLMs","a major barrier","innovation","FP8 training"],"filter_categories":{"ai_ml":["InfiR2 A Comprehensive FP8 Training Recipe","Large Language Models"],"business_innovation":["innovation"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"InfiR2 A Comprehensive FP8 Training Recipe":2.0,"Reasoning":2.0,"Announce Type":1.0,"Abstract":1.0,"The immense computational cost":1.0,"Large Language Models":1.0,"LLMs":1.0,"a major barrier":1.0,"innovation":1.0,"FP8 training":1.0}},"age_hours":2.7827792827777778,"is_recent":true,"quality_score":0.7,"sentiment_score":9.547,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9094,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9044,"joy":0.0226,"surprise":0.0461,"sadness":0.0078,"fear":0.0082,"anger":0.0074,"disgust":0.0035},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":6,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research presents an FP8 training recipe for LLMs, demonstrating a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput compared to BF16. While the research is promising, it's still in the applied research stage and lacks real-world deployment data, hence the lower deployment readiness score. The efficiency gains could lead to lower energy consumption for training large models, contributing to a reduced carbon footprint of AI development.","key_impact_metrics":["22% reduction in training time","14% decrease in peak memory usage","19% increase in throughput"],"technology_tags":["FP8 training","Large Language Models","Hybrid-granularity quantization"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T21:01:55.251802Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f927517f222b","title":"Learn the Ropes, Then Trust the Wins: Self","content":"arXiv:2509.22601v2 Announce Type: replace Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.22601","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.525009","language":"en","tags":["csma","computer-science","cslg","preprints","csai","cscv","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":257,"author":"Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun","raw_content_length":1973,"priority":7,"update_frequency":1,"reading_time_minutes":1.285,"robust_parsing_used":true,"entities":{"organizations":["SPEAR"],"persons":[],"locations":[],"monetary":[]},"char_count":1972,"language_detected":"en","key_concepts":{"key_phrases":["the Ropes","the Wins","arXiv250922601v2","Announce Type","Abstract","Reinforcement learning","the dominant paradigm","strategic tool use capabilities","LLMs","long-horizon sparsely-rewarded agent tasks"],"filter_categories":{"ai_ml":["Reinforcement learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"the Ropes":2.0,"the Wins":2.0,"arXiv250922601v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Reinforcement learning":1.0,"the dominant paradigm":1.0,"strategic tool use capabilities":1.0,"LLMs":1.0,"long-horizon sparsely-rewarded agent tasks":1.0}},"age_hours":2.782794584166667,"is_recent":true,"quality_score":1.0,"sentiment_score":8.378499999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6757,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9095,"joy":0.0038,"surprise":0.0146,"sadness":0.0113,"fear":0.0385,"anger":0.0123,"disgust":0.0099},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a new reinforcement learning algorithm (SPEAR) to improve the strategic tool use capabilities of LLMs. While the algorithm aims to improve exploration-exploitation trade-off, it is currently in the basic research stage with no deployed technology or measured outcomes in a real-world climate context. The potential climate impact is theoretical, as it could potentially optimize LLMs used in climate modeling or optimization, but this is not demonstrated.","key_impact_metrics":[],"technology_tags":["Reinforcement Learning","Large Language Models","Self-Imitation Learning"],"sdg_alignment":[],"analyzed_at":"2025-10-28T21:01:59.223556Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_948fb45e7a50","title":"Defending MoE LLMs against Harmful Fine","content":"arXiv:2509.22745v2 Announce Type: replace Abstract: Recent large language models (LLMs) have increasingly adopted the Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. However, our analysis reveals that routing decisions for harmful inputs drift significantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less effective for MoE LLMs as they fail to prevent drift in harmful input routing. To address this limitation, we propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE directly mitigates routing drift by penalizing the gap between the routing weights of a fine-tuned model and those of the initial safety-aligned model, thereby preserving the safety-aligned routing of harmful inputs to safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as gpt-oss and Llama 4. Our implementation is available at https://anonymous.4open.science/r/SafeMoE.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.22745","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.525428","language":"en","tags":["computer-science","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":204,"author":"Jaehan Kim, Minkyoo Song, Seungwon Shin, Sooel Son","raw_content_length":1516,"priority":7,"update_frequency":1,"reading_time_minutes":1.02,"robust_parsing_used":true,"entities":{"organizations":["MoE"],"persons":["Harmful Fine arXiv:2509.22745v2 Announce Type"],"locations":[],"monetary":[]},"char_count":1515,"language_detected":"en","key_concepts":{"key_phrases":["MoE LLMs","Harmful Fine","arXiv250922745v2 Announce Type","Abstract","Recent large language models","LLMs","Experts","efficiency","MoE-based LLMs","a superficial safety mechanism"],"filter_categories":{"ai_ml":["MoE LLMs","Recent large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"MoE LLMs":2.0,"Harmful Fine":2.0,"arXiv250922745v2 Announce Type":1.0,"Abstract":1.0,"Recent large language models":1.0,"LLMs":1.0,"Experts":1.0,"efficiency":1.0,"MoE-based LLMs":1.0,"a superficial safety mechanism":1.0}},"age_hours":2.782809561388889,"is_recent":true,"quality_score":1.0,"sentiment_score":6.0115,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.3719,"joy":0.0063,"surprise":0.0102,"sadness":0.033,"fear":0.0598,"anger":0.1807,"disgust":0.3381},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a method (SafeMoE) to mitigate harmful fine-tuning attacks on MoE LLMs. The concrete action is the development and testing of this method, showing a reduction in harmfulness score from 62.0 to 5.0 on OLMoE. However, this is still in the research phase and not yet deployed in real-world applications, limiting its immediate impact.","key_impact_metrics":["harmfulness score reduction from 62.0 to 5.0","1% task utility degradation"],"technology_tags":["Large Language Models","Mixture-of-Experts","AI Safety"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-28T21:02:02.695006Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_650b8caa09fc","title":"SINQ: Sinkhorn-Normalized Quantization for Calibration","content":"arXiv:2509.22944v3 Announce Type: replace Abstract: Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.22944","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.525821","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":180,"author":"Lorenz K. M\\\"uller, Philippe Bich, Jiawei Zhuang, Ahmet \\c{C}elik, Luca Benfenati, Lukas Cavigelli","raw_content_length":1321,"priority":7,"update_frequency":1,"reading_time_minutes":0.9,"robust_parsing_used":true,"entities":{"organizations":["SINQ","Qwen","Sinkhorn-Normalized Quantization for Calibration"],"persons":[],"locations":["Sinkhorn-Knopp"],"monetary":[]},"char_count":1320,"language_detected":"en","key_concepts":{"key_phrases":["SINQ","Sinkhorn-Normalized Quantization","Calibration","arXiv250922944v3","Announce Type","Abstract","Post-training quantization","the most widely used strategy","large language models","low precision"],"filter_categories":{"ai_ml":["Post-training quantization","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"SINQ":3.0,"Sinkhorn-Normalized Quantization":2.0,"Calibration":2.0,"arXiv250922944v3":1.0,"Announce Type":1.0,"Abstract":1.0,"Post-training quantization":1.0,"the most widely used strategy":1.0,"large language models":1.0,"low precision":1.0}},"age_hours":2.78282414,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2375000000000003,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7525,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.5168,"joy":0.0035,"surprise":0.2975,"sadness":0.0198,"fear":0.0474,"anger":0.0472,"disgust":0.0678},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":6,"deployment_readiness":4,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel quantization method (SINQ) for large language models, aiming to improve efficiency and reduce computational resources. The method is evaluated on specific models (Qwen3, DeepSeek-V2.5) with improvements in WikiText2 and C4 perplexity, indicating potential for reduced energy consumption during model deployment. The code is available, but there's no mention of actual deployment or real-world energy savings yet.","key_impact_metrics":["WikiText2 perplexity improvement","C4 perplexity improvement"],"technology_tags":["Quantization","Large Language Models","Model Compression"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-28T21:02:06.716890Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a0b50a07990d","title":"LLaVA","content":"arXiv:2509.23661v2 Announce Type: replace Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We anticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community to await further updates.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.23661","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.527783","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":153,"author":"Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, Jiankang Deng","raw_content_length":1364,"priority":7,"update_frequency":1,"reading_time_minutes":0.765,"robust_parsing_used":true,"entities":{"organizations":["Large Multimodal Models","The LLaVA-OneVision-1.5"],"persons":["Efficient Training Framework"],"locations":["LLaVA-"],"monetary":["16,000"]},"char_count":1363,"language_detected":"en","key_concepts":{"key_phrases":["LLaVA-OneVision-15","Announce Type","Abstract","a novel family","Large Multimodal Models","LMMs","the-art","significantly reduced computational and financial costs","the existing works","an open efficient and reproducible framework"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"LLaVA-OneVision-15":2.0,"Announce Type":1.0,"Abstract":1.0,"a novel family":1.0,"Large Multimodal Models":1.0,"LMMs":1.0,"the-art":1.0,"significantly reduced computational and financial costs":1.0,"the existing works":1.0,"an open efficient and reproducible framework":1.0}},"age_hours":2.782895633055556,"is_recent":true,"quality_score":0.7,"sentiment_score":8.1245,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6249,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8116,"joy":0.0899,"surprise":0.0798,"sadness":0.004,"fear":0.0033,"anger":0.0088,"disgust":0.0025},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel large multimodal model (LLM) with reduced computational costs, demonstrated by outperforming existing models on benchmark tasks. While the reduced computational cost implies potential energy savings, the article lacks concrete deployment data and lifecycle analysis to quantify the climate impact. The model is currently in the applied research stage, with no evidence of real-world deployment.","key_impact_metrics":["Training budget $16,000","85M concept-balanced pretraining dataset"],"technology_tags":["Large Multimodal Models","Vision-Language Models"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T21:02:11.540391Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9088f28ccd75","title":"HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle","content":"arXiv:2509.23663v2 Announce Type: replace Abstract: Vision-Language Models (VLMs) have shown strong capabilities on diverse multimodal tasks. However, the large number of visual tokens output by the vision encoder severely hinders inference efficiency, and prior studies have shown that many of these tokens are not important and can therefore be safely pruned. In this work, we propose HIVTP, a training-free method to improve VLMs efficiency via hierarchical visual token pruning using a novel middle-layer-based importance score. Specifically, we utilize attention maps extracted from the middle layers of the vision encoder, which better reflect fine-grained and object-level attention, to estimate visual token importance. Based on this, we propose a hierarchical visual token pruning method to retain both globally and locally important visual tokens. Specifically, we reshape the 1-D visual token sequence output by the vision encoder into a 2-D spatial layout. In the global retaining stage, we divide the image into regions and retain tokens with higher importance scores in each region; in the local retaining stage, we then divide the image into small windows and retain the most important token in each local window. Experimental results show that our proposed method, HIVTP, can reduce the time-to-first-token (TTFT) of LLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and improve the token generation throughput by up to 60.9% and 47.3%, without sacrificing accuracy, and even achieving improvements on certain benchmarks. Compared with prior works, HIVTP achieves better accuracy while offering higher inference efficiency.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.23663","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.528370","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":242,"author":"Jingqi Xu, Jingxi Lu, Chenghao Li, Sreetama Sarkar, Peter A. Beerel","raw_content_length":1662,"priority":7,"update_frequency":1,"reading_time_minutes":1.21,"robust_parsing_used":true,"entities":{"organizations":["Hierarchical Visual","HIVTP","Vision-Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1661,"language_detected":"en","key_concepts":{"key_phrases":["HIVTP","A Training-Free Method","VLMs Efficiency","Hierarchical Visual Token Pruning","Middle","arXiv250923663v2 Announce Type","Abstract","Vision-Language Models","VLMs","strong capabilities"],"filter_categories":{"ai_ml":["A Training-Free Method"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"HIVTP":3.0,"A Training-Free Method":2.0,"VLMs Efficiency":2.0,"Hierarchical Visual Token Pruning":2.0,"Middle":2.0,"arXiv250923663v2 Announce Type":1.0,"Abstract":1.0,"Vision-Language Models":1.0,"VLMs":1.0,"strong capabilities":1.0}},"age_hours":2.7829107530555555,"is_recent":true,"quality_score":1.0,"sentiment_score":9.5115,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9308,"joy":0.0179,"surprise":0.0183,"sadness":0.0089,"fear":0.0068,"anger":0.0108,"disgust":0.0065},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method (HIVTP) for improving the efficiency of Vision-Language Models (VLMs) by pruning less important visual tokens. The concrete action is the implementation of a hierarchical pruning technique, and the evidence supporting the claims comes from experimental results showing reduced time-to-first-token (TTFT) and improved token generation throughput. The innovation is at the applied research stage, with potential for future deployment.","key_impact_metrics":["TTFT reduction by up to 55.1%","Token generation throughput improvement by up to 60.9%"],"technology_tags":["Vision-Language Models","Token Pruning","Artificial Intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:02:14.583501Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2b8770635d5b","title":"CORE-3D: Context","content":"arXiv:2509.24528v2 Announce Type: replace Abstract: 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2509.24528","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.528823","language":"en","tags":["computer-science","csai","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":177,"author":"Mohamad Amin Mirzaei, Pantea Amoie, Ali Ekhterachian, Matin Mirzababaei, Babak Khalaj","raw_content_length":1386,"priority":7,"update_frequency":1,"reading_time_minutes":0.885,"robust_parsing_used":true,"entities":{"organizations":["CLIP"],"persons":[],"locations":[],"monetary":[]},"char_count":1385,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250924528v2 Announce Type","Abstract","3D scene understanding","embodied AI","robotics","reliable perception","interaction","navigation","Recent approaches","zero-shot open-vocabulary 3D semantic mapping"],"filter_categories":{"ai_ml":["embodied AI"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250924528v2 Announce Type":1.0,"Abstract":1.0,"3D scene understanding":1.0,"embodied AI":1.0,"robotics":1.0,"reliable perception":1.0,"interaction":1.0,"navigation":1.0,"Recent approaches":1.0,"zero-shot open-vocabulary 3D semantic mapping":1.0}},"age_hours":2.782924903888889,"is_recent":true,"quality_score":1.0,"sentiment_score":7.202,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4404,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8694,"joy":0.0039,"surprise":0.0179,"sadness":0.0389,"fear":0.023,"anger":0.0275,"disgust":0.0195},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel approach to 3D scene understanding using improved semantic segmentation. While it shows significant improvements in experimental results on benchmark datasets, it remains in the applied research stage with no mention of real-world deployment or economic viability. The potential climate impact is indirect, as improved AI could lead to more efficient resource use, but this is not quantified.","key_impact_metrics":["Improved 3D semantic segmentation accuracy","Object retrieval from language queries accuracy"],"technology_tags":["3D scene understanding","Semantic segmentation","Vision-language models","Robotics"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T21:02:18.212047Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
