{"id":"science_arxiv_cs_ad4c01565226","title":"OneForecast: A Universal Framework for Global and Regional Weather Forecasting","content":"arXiv:2502.00338v4 Announce Type: replace Abstract: Accurate weather forecasts are important for disaster prevention, agricultural planning, etc. Traditional numerical weather prediction (NWP) methods offer physically interpretable high-accuracy predictions but are computationally expensive and fail to fully leverage rapidly growing historical data. In recent years, deep learning models have made significant progress in weather forecasting, but challenges remain, such as balancing global and regional high-resolution forecasts, excessive smoothing in extreme event predictions, and insufficient dynamic system modeling. To address these issues, this paper proposes a global-regional nested weather forecasting framework (OneForecast) based on graph neural networks. By combining a dynamic system perspective with multi-grid theory, we construct a multi-scale graph structure and densify the target region to capture local high-frequency features. We introduce an adaptive messaging mechanism, using dynamic gating units to deeply integrate node and edge features for more accurate extreme event forecasting. For high-resolution regional forecasts, we propose a neural nested grid method to mitigate boundary information loss. Experimental results show that OneForecast performs excellently across global to regional scales and short-term to long-term forecasts, especially in extreme event predictions. Codes link https://github.com/YuanGao-YG/OneForecast.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.00338","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.415268","language":"en","tags":["physicsao-ph","computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":183,"author":"Yuan Gao, Hao Wu, Ruiqi Shu, Huanshuo Dong, Fan Xu, Rui Ray Chen, Yibo Yan, Qingsong Wen, Xuming Hu, Kun Wang, Jiahao Wu, Qing Li, Hui Xiong, Xiaomeng Huang","raw_content_length":1462,"priority":7,"update_frequency":1,"reading_time_minutes":0.915,"robust_parsing_used":true,"entities":{"organizations":["NWP","Accurate","OneForecast"],"persons":[],"locations":[],"monetary":[]},"char_count":1461,"language_detected":"en","key_concepts":{"key_phrases":["OneForecast","A Universal Framework","Global","Regional Weather Forecasting","arXiv250200338v4 Announce Type","Abstract","Accurate weather forecasts","disaster prevention","agricultural planning","Traditional numerical weather prediction NWP methods"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"OneForecast":2.0,"A Universal Framework":2.0,"Global":2.0,"Regional Weather Forecasting":2.0,"arXiv250200338v4 Announce Type":1.0,"Abstract":1.0,"Accurate weather forecasts":1.0,"disaster prevention":1.0,"agricultural planning":1.0,"Traditional numerical weather prediction NWP methods":1.0}},"age_hours":2.7795572727777778,"is_recent":true,"quality_score":1.0,"sentiment_score":5.5715,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1143,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.6784,"joy":0.0096,"surprise":0.0168,"sadness":0.0169,"fear":0.1887,"anger":0.0621,"disgust":0.0276},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel weather forecasting framework that improves accuracy, especially for extreme events. The framework is based on graph neural networks and multi-grid theory. While the experimental results show promise, it is still in the early stages of development (prototype) and lacks real-world deployment data, hence the vaporware flag.","key_impact_metrics":["Improved accuracy in extreme event predictions"],"technology_tags":["Graph Neural Networks","Weather Forecasting","Climate Modeling"],"sdg_alignment":[2,13],"analyzed_at":"2025-10-28T20:53:51.752695Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_dc8f8ffc92d3","title":"Solving Football by Exploiting Equilibrium Structure of 2p0s Differential Games with One","content":"arXiv:2502.00560v3 Announce Type: replace Abstract: For a two-player imperfect-information extensive-form game (IIEFG) with $K$ time steps and a player action space of size $U$, the game tree complexity is $U^{2K}$, causing existing IIEFG solvers to struggle with large or infinite $(U,K)$, e.g., differential games with continuous action spaces. To partially address this scalability challenge, we focus on an important class of 2p0s games where the informed player (P1) knows the payoff while the uninformed player (P2) only has a belief over the set of $I$ possible payoffs. Such games encompass a wide range of scenarios in sports, defense, cybersecurity, and finance. We prove that under mild conditions, P1's (resp. P2's) equilibrium strategy at any infostate concentrates on at most $I$ (resp. $I+1$) action prototypes. When $I\\ll U$, this equilibrium structure causes the game tree complexity to collapse to $I^K$ for P1 when P2 plays pure best responses, and $(I+1)^K$ for P2 in a dual game where P1 plays pure best responses. We then show that exploiting this structure in standard learning modes, i.e., model-free multiagent reinforcement learning and model predictive control, is straightforward, leading to significant improvements in learning accuracy and efficiency from SOTA IIEFG solvers. Our demonstration solves a 22-player football game ($K=10$, $U=\\infty$) where the attacking team has to strategically conceal their intention until a critical moment in order to exploit information advantage. Code is available at https://github.com/ghimiremukesh/cams/tree/iclr","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.00560","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.415699","language":"en","tags":["csgt","research","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":230,"author":"Mukesh Ghimire, Lei Zhang, Zhe Xu, Yi Ren","raw_content_length":1584,"priority":7,"update_frequency":1,"reading_time_minutes":1.15,"robust_parsing_used":true,"entities":{"organizations":["IIEFG"],"persons":["Announce Type"],"locations":["K)$"],"monetary":["most $I$","$I+1$"]},"char_count":1583,"language_detected":"en","key_concepts":{"key_phrases":["Football","Exploiting Equilibrium Structure","2p0s Differential Games","arXiv250200560v3 Announce Type","Abstract","a two-player imperfect-information extensive-form game","IIEFG","K time steps","a player action space","size"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Football":2.0,"Exploiting Equilibrium Structure":2.0,"2p0s Differential Games":2.0,"arXiv250200560v3 Announce Type":1.0,"Abstract":1.0,"a two-player imperfect-information extensive-form game":1.0,"IIEFG":1.0,"K time steps":1.0,"a player action space":1.0,"size":1.0}},"age_hours":2.779574336388889,"is_recent":true,"quality_score":1.0,"sentiment_score":4.1105,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1779,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8715,"joy":0.0073,"surprise":0.0196,"sadness":0.0345,"fear":0.0256,"anger":0.0292,"disgust":0.0122},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel approach to solving complex games, demonstrating its application in a simulated football game. While the research is technically sound and shows improvements in learning accuracy and efficiency, it is currently at an early stage of development with no clear path to real-world deployment or direct climate impact. The potential for sustainability impact is indirect, possibly through optimizing resource allocation in various sectors, but this is highly speculative at this point.","key_impact_metrics":["Game tree complexity collapse to I^K for P1 when P2 plays pure best responses","Game tree complexity collapse to (I+1)^K for P2 in a dual game where P1 plays pure best responses"],"technology_tags":["Game Theory","Reinforcement Learning","Multiagent Systems"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:53:55.270776Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_141aaa397647","title":"Task Vector Bases: A Unified and Scalable Framework for Compressed Task Arithmetic","content":"arXiv:2502.01015v4 Announce Type: replace Abstract: Task arithmetic, representing downstream tasks through linear operations on task vectors, has emerged as a simple yet powerful paradigm for transferring knowledge across diverse settings. However, maintaining a large collection of task vectors introduces scalability challenges in both storage and computation. We propose Task Vector Bases, a framework compressing $T$ task vectors into $M < T$ basis vectors while preserving the functionality of task arithmetic. By representing each task vector as a structured linear combination of basis atoms, our approach supports standard operations such as addition, negation, as well as more advanced arithmetic ones. The framework is orthogonal to other efficiency-oriented improvements in task arithmetic and can be used in combination with them. We provide theoretical analysis showing that basis compression retains addition generalization guarantees and enables principled unlearning, with error bounds depending on reconstruction quality. Empirically, our proposed basis construction methods consistently outperform heuristic basis construction baselines and, in some cases, even surpass the performance of full task vector collections across diverse downstream applications while reducing storage and computational requirements. The code is available at https://github.com/uiuctml/TaskVectorBasis.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.01015","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.416100","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":182,"author":"Siqi Zeng, Yifei He, Meitong Liu, Weiqiu You, Yifan Hao, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao","raw_content_length":1399,"priority":7,"update_frequency":1,"reading_time_minutes":0.91,"robust_parsing_used":true,"entities":{"organizations":["Task Vector Bases"],"persons":[],"locations":[],"monetary":[]},"char_count":1398,"language_detected":"en","key_concepts":{"key_phrases":["Task Vector Bases","A Unified and Scalable Framework","Compressed Task Arithmetic","task vectors","arXiv250201015v4 Announce Type","Abstract","Task arithmetic","downstream tasks","linear operations","a simple yet powerful paradigm"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Task Vector Bases":3.0,"A Unified and Scalable Framework":2.0,"Compressed Task Arithmetic":2.0,"task vectors":2.0,"arXiv250201015v4 Announce Type":1.0,"Abstract":1.0,"Task arithmetic":1.0,"downstream tasks":1.0,"linear operations":1.0,"a simple yet powerful paradigm":1.0}},"age_hours":2.779588813611111,"is_recent":true,"quality_score":1.0,"sentiment_score":8.453999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6908,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9262,"joy":0.0163,"surprise":0.0267,"sadness":0.0035,"fear":0.0126,"anger":0.0103,"disgust":0.0045},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a framework for compressing task vectors in machine learning, potentially reducing storage and computational requirements. While it could indirectly support more efficient AI models used in sustainability applications, there are no concrete deployments or measured outcomes related to climate impact. The research is at a basic research stage, with theoretical analysis and empirical results, but no real-world deployment.","key_impact_metrics":["Storage reduction with M < T","Computational reduction with M < T"],"technology_tags":["Task Arithmetic","Machine Learning","Compression Algorithms"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:53:57.878798Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b690e17bf09f","title":"SiLVR: Scalable Lidar","content":"arXiv:2502.02657v3 Announce Type: replace Abstract: We present a neural radiance field (NeRF) based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photorealistic texture. Our system adopts the state-of-the-art NeRF representation to incorporate lidar. Adding lidar data adds strong geometric constraints on the depth and surface normals, which is particularly useful when modelling uniform texture surfaces which contain ambiguous visual reconstruction cues. A key contribution of this work is a novel method to quantify the epistemic uncertainty of the lidar-visual NeRF reconstruction by estimating the spatial variance of each point location in the radiance field given the sensor observations from the cameras and lidar. This provides a principled approach to evaluate the contribution of each sensor modality to the final reconstruction. In this way, reconstructions that are uncertain (due to e.g. uniform visual texture, limited observation viewpoints, or little lidar coverage) can be identified and removed. Our system is integrated with a real-time lidar SLAM system which is used to bootstrap a Structure-from-Motion (SfM) reconstruction procedure. It also helps to properly constrain the overall metric scale which is essential for the lidar depth loss. The refined SLAM trajectory can then be divided into submaps using Spectral Clustering to group sets of co-visible images together. This submapping approach is more suitable for visual reconstruction than distance-based partitioning. Our uncertainty estimation is particularly effective when merging submaps as their boundaries often contain artefacts due to limited observations. We demonstrate the reconstruction system using a multi-camera, lidar sensor suite in experiments involving both robot-mounted and handheld scanning. Our test datasets cover a total area of more than 20,000 square metres.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.02657","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.416541","language":"en","tags":["csro","computer-science","cscv","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":277,"author":"Yifu Tao, Maurice Fallon","raw_content_length":1973,"priority":7,"update_frequency":1,"reading_time_minutes":1.385,"robust_parsing_used":true,"entities":{"organizations":["NeRF"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1972,"language_detected":"en","key_concepts":{"key_phrases":["SiLVR","Scalable Lidar","arXiv250202657v3 Announce Type","Abstract","a neural radiance field","NeRF","large-scale reconstruction system","lidar and vision data","high-quality reconstructions","photorealistic texture"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"SiLVR":2.0,"Scalable Lidar":2.0,"arXiv250202657v3 Announce Type":1.0,"Abstract":1.0,"a neural radiance field":1.0,"NeRF":1.0,"large-scale reconstruction system":1.0,"lidar and vision data":1.0,"high-quality reconstructions":1.0,"photorealistic texture":1.0}},"age_hours":2.779603191388889,"is_recent":true,"quality_score":1.0,"sentiment_score":9.454,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8908,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8997,"joy":0.0329,"surprise":0.048,"sadness":0.0045,"fear":0.0035,"anger":0.0073,"disgust":0.0042},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method for improving the accuracy of 3D reconstructions using lidar and vision data. While the technology has the potential to improve the efficiency of various applications (e.g., infrastructure monitoring, autonomous vehicles) that could indirectly reduce emissions, the article does not provide concrete evidence of direct GHG emission reductions or other measurable sustainability outcomes. The system is demonstrated on datasets covering 20,000 square meters, indicating a pilot-scale implementation.","key_impact_metrics":["Area covered by test datasets: 20,000 square meters"],"technology_tags":["Lidar","Neural Radiance Fields","3D Reconstruction"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:00.946837Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9d1bb74723c4","title":"BFS-Prover: Scalable Best","content":"arXiv:2502.03438v3 Announce Type: replace Abstract: Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. To facilitate further research and development in this area, we have open-sourced our model at https://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.03438","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.416940","language":"en","tags":["research","csai","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, Kai Shen","raw_content_length":1575,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["BFS","Best-First Tree Search","BFS-Prover","Monte Carlo Tree Search","Direct Preference Optimization","DPO"],"persons":["Lean4"],"locations":["node"],"monetary":[]},"char_count":1574,"language_detected":"en","key_concepts":{"key_phrases":["BFS-Prover","arXiv250203438v3","Announce Type","Recent advancements","large language models","LLMs","growing interest","automatic theorem proving","Lean4","effective tree search methods"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"BFS-Prover":2.0,"arXiv250203438v3":1.0,"Announce Type":1.0,"Recent advancements":1.0,"large language models":1.0,"LLMs":1.0,"growing interest":1.0,"automatic theorem proving":1.0,"Lean4":1.0,"effective tree search methods":1.0}},"age_hours":2.7796182669444445,"is_recent":true,"quality_score":1.0,"sentiment_score":9.7115,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9423,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8888,"joy":0.0394,"surprise":0.0523,"sadness":0.0048,"fear":0.005,"anger":0.0068,"disgust":0.003},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new algorithm (BFS-Prover) for theorem proving using LLMs, achieving a state-of-the-art score of 72.95% on the MiniF2F test set. While it demonstrates improved efficiency in a specific computational task, its direct impact on climate or sustainability is minimal at this stage. The model is open-sourced, facilitating further research.","key_impact_metrics":["72.95% on MiniF2F test set"],"technology_tags":["Large Language Models","Theorem Proving","Best-First Tree Search"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:54:03.666321Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_05803140e514","title":"AutoAgent: A Fully","content":"arXiv:2502.05957v3 Announce Type: replace Abstract: Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.05957","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.418120","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":217,"author":"Jiabin Tang, Tianyu Fan, Chao Huang","raw_content_length":1750,"priority":7,"update_frequency":1,"reading_time_minutes":1.085,"robust_parsing_used":true,"entities":{"organizations":["LLM","AutoGen","Agentic System Utilities","AutoAgent","LangChain"],"persons":[],"locations":[],"monetary":[]},"char_count":1749,"language_detected":"en","key_concepts":{"key_phrases":["AutoAgent","arXiv250205957v3 Announce Type","Large Language Model LLM Agents","remarkable capabilities","task automation","intelligent decision-making","the widespread adoption","agent development frameworks","LangChain","AutoGen"],"filter_categories":{"ai_ml":["Large Language Model LLM Agents"],"engineering":["task automation","agent development frameworks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"AutoAgent":2.0,"arXiv250205957v3 Announce Type":1.0,"Large Language Model LLM Agents":1.0,"remarkable capabilities":1.0,"task automation":1.0,"intelligent decision-making":1.0,"the widespread adoption":1.0,"agent development frameworks":1.0,"LangChain":1.0,"AutoGen":1.0}},"age_hours":2.77966244,"is_recent":true,"quality_score":1.0,"sentiment_score":8.6755,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7351,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.872,"joy":0.0127,"surprise":0.0484,"sadness":0.0045,"fear":0.0363,"anger":0.0174,"disgust":0.0088},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a framework (AutoAgent) that allows users to create LLM agents using natural language. While it could potentially impact sustainability by automating tasks related to climate modeling or resource management, the article lacks concrete actions or measurable outcomes related to environmental impact. The GAIA benchmark performance is mentioned, but it does not directly translate to sustainability benefits.","key_impact_metrics":["GAIA benchmark performance","RAG-related capabilities performance"],"technology_tags":["Large Language Models","Agent Operating System","Natural Language Processing"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:07.813798Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_21d9261452bd","title":"Rex: Reversible Solvers for Diffusion Models","content":"arXiv:2502.08834v2 Announce Type: replace Abstract: Diffusion models have quickly become the state-of-the-art for numerous generation tasks across many different applications. Encoding samples from the data distribution back into the models underlying prior distribution is an important task that arises in many downstream applications. This task is often called the inversion of diffusion models. Prior approaches for solving this task, however, are often simple heuristic solvers that come with several drawbacks in practice. In this work, we propose a new family of solvers for diffusion models by exploiting the connection between this task and the broader study of algebraically reversible solvers for differential equations. In particular, we construct a family of reversible solvers using an application of Lawson methods to construct exponential Runge-Kutta methods for the diffusion models. We call this family of reversible exponential solvers Rex. In addition to a rigorous theoretical analysis of the proposed solvers we also emonstrate the utility of the methods through a variety of empirical illustrations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.08834","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.419424","language":"en","tags":["statml","computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":161,"author":"Zander W. Blasingame, Chen Liu","raw_content_length":1122,"priority":7,"update_frequency":1,"reading_time_minutes":0.805,"robust_parsing_used":true,"entities":{"organizations":["Runge-Kutta"],"persons":["Lawson"],"locations":[],"monetary":[]},"char_count":1121,"language_detected":"en","key_concepts":{"key_phrases":["Rex","Reversible Solvers","Diffusion Models","Announce Type","Abstract","Diffusion models","the state","the-art","numerous generation tasks","many different applications"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Rex":2.0,"Reversible Solvers":2.0,"Diffusion Models":2.0,"Announce Type":1.0,"Abstract":1.0,"Diffusion models":1.0,"the state":1.0,"the-art":1.0,"numerous generation tasks":1.0,"many different applications":1.0}},"age_hours":2.7797066038888887,"is_recent":true,"quality_score":1.0,"sentiment_score":7.4695,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4939,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8997,"joy":0.0095,"surprise":0.0669,"sadness":0.0047,"fear":0.0073,"anger":0.0082,"disgust":0.0037},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new family of solvers for diffusion models, which could potentially improve the efficiency of generative AI. While generative AI can be used for sustainability applications (e.g., materials discovery), the article does not provide concrete evidence of GHG emission reductions or other environmental benefits. The technology is in the applied research stage, with empirical illustrations but no deployed units.","key_impact_metrics":[],"technology_tags":["diffusion models","reversible solvers","Lawson methods","exponential Runge-Kutta methods"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:54:10.748290Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b19e65b3c041","title":"InfoPos: A Design Support Framework for ML","content":"arXiv:2502.10331v2 Announce Type: replace Abstract: The variety of building blocks and algorithms incorporated in data-centric and ML-assisted fault detection and identification solutions is high, contributing to two challenges: selection of the most effective set and order of building blocks, as well as achieving such a selection with minimum cost. Considering that ML-assisted solution design is influenced by the extent of available data and the extent of available knowledge of the target system, it is advantageous to be able to select effective and matching building blocks. We introduce the first iteration of our InfoPos framework, allowing the placement of fault detection/identification use-cases based on the available levels (positions), i.e., from poor to rich, of knowledge and data dimensions. With that input, designers and developers can reveal the most effective corresponding choice(s), streamlining the solution design process. The results from a demonstrator, a fault identification use-case for industrial Cyber-Physical Systems, reflects achieved effects when different building blocks are used throughout knowledge and data positions. The achieved ML model performance is considered as the indicator for a better solution. The data processing code and composed datasets are publicly available.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.10331","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.419945","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":186,"author":"Uraz Odyurt, Richard Loendersloot, Tiedo Tinga","raw_content_length":1320,"priority":7,"update_frequency":1,"reading_time_minutes":0.93,"robust_parsing_used":true,"entities":{"organizations":["InfoPos"],"persons":[],"locations":[],"monetary":[]},"char_count":1319,"language_detected":"en","key_concepts":{"key_phrases":["InfoPos","A Design Support Framework","the extent","arXiv250210331v2","Announce Type","Abstract","The variety","blocks","algorithms","two challenges"],"filter_categories":{"ai_ml":["algorithms"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"InfoPos":2.0,"A Design Support Framework":2.0,"the extent":2.0,"arXiv250210331v2":1.0,"Announce Type":1.0,"Abstract":1.0,"The variety":1.0,"blocks":1.0,"algorithms":1.0,"two challenges":1.0}},"age_hours":2.779721362777778,"is_recent":true,"quality_score":1.0,"sentiment_score":8.589,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7178,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8305,"joy":0.0433,"surprise":0.0888,"sadness":0.0052,"fear":0.0129,"anger":0.0157,"disgust":0.0036},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a framework (InfoPos) to improve the design of ML-assisted fault detection in industrial Cyber-Physical Systems. The framework aims to streamline the design process and improve ML model performance, which is considered an indicator for a better solution. While the data processing code and datasets are publicly available, the article describes a demonstrator and does not provide concrete evidence of deployment or quantified GHG emission reductions.","key_impact_metrics":["ML model performance"],"technology_tags":["Machine Learning","Fault Detection","Cyber-Physical Systems"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:13.811277Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b28c9307f465","title":"Less is More: Compact Clue Selection for Efficient Retrieval","content":"arXiv:2502.11811v5 Announce Type: replace Abstract: Current RAG retrievers are designed primarily for human readers, emphasizing complete, readable, and coherent paragraphs. However, LLMs benefit more from precise, compact, and well-structured input, which enhances reasoning quality and efficiency. Existing methods often rely on reranking or summarization to identify key sentences, but may suffer from semantic breaks and unfaithfulness. Thus, efficiently extracting and organizing answer-relevant clues from large-scale documents while reducing LLM reasoning costs remains a challenge for RAG. Inspired by Occam's razor, we frame LLM-centric retrieval as a MinMax optimization: maximizing the extraction of potential clues and reranking them for well-organization, while minimizing reasoning costs by truncating to the smallest sufficient clues set. In this paper, we propose CompSelect, a Compact clue Selection mechanism for LLM-centric RAG, consisting of a clue extractor, a reranker, and a truncator. (1) The clue extractor first uses answer-containing sentences as fine-tuning targets, aiming to extract sufficient potential clues; (2) The reranker is trained to prioritize effective clues based on real LLM feedback; (3) The truncator uses the truncated text containing the minimum sufficient clues for answering the question as fine-tuning targets, thereby enabling efficient RAG reasoning. Experiments on three QA datasets show that CompSelect improves QA performance by approximately 11\\% and reduces Total Latency and Online Latency by approximately 17\\% and 67\\% compared to various baseline methods on both LLaMA3 and Qwen3. Further analysis confirms its robustness to unreliable retrieval and generalization across different scenarios, offering a scalable and cost-efficient solution for web-scale RAG applications.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.11811","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.420370","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":249,"author":"Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng","raw_content_length":1833,"priority":7,"update_frequency":1,"reading_time_minutes":1.245,"robust_parsing_used":true,"entities":{"organizations":["RAG","LLM","CompSelect","MinMax"],"persons":["Occam"],"locations":[],"monetary":["LLM-cent"]},"char_count":1832,"language_detected":"en","key_concepts":{"key_phrases":["Efficient Retrieval","arXiv250211811v5 Announce Type","Abstract","Current RAG retrievers","human readers","complete readable and coherent paragraphs","LLMs","precise compact and well-structured input","which","reasoning quality"],"filter_categories":{"ai_ml":["LLMs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Efficient Retrieval":2.0,"arXiv250211811v5 Announce Type":1.0,"Abstract":1.0,"Current RAG retrievers":1.0,"human readers":1.0,"complete readable and coherent paragraphs":1.0,"LLMs":1.0,"precise compact and well-structured input":1.0,"which":1.0,"reasoning quality":1.0}},"age_hours":2.7797365177777777,"is_recent":true,"quality_score":1.0,"sentiment_score":3.75,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.25,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.733,"joy":0.0122,"surprise":0.0025,"sadness":0.0405,"fear":0.0037,"anger":0.0513,"disgust":0.1567},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method (CompSelect) for improving the efficiency of LLMs in RAG systems, focusing on reducing computational costs. The concrete action is the development and testing of this method on QA datasets, showing a 17% reduction in Total Latency and a 67% reduction in Online Latency. While promising, it's still in the applied research stage, lacking real-world deployment data.","key_impact_metrics":["Total Latency reduction 17%","Online Latency reduction 67%"],"technology_tags":["Large Language Models","Retrieval Augmented Generation","Artificial Intelligence"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:54:17.204140Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_eb4165018a1f","title":"MoM: Linear Sequence Modeling with Mixture","content":"arXiv:2502.13685v3 Announce Type: replace Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.13685","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.420803","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":199,"author":"Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng","raw_content_length":1572,"priority":7,"update_frequency":1,"reading_time_minutes":0.995,"robust_parsing_used":true,"entities":{"organizations":["linear","Linear"],"persons":[],"locations":[],"monetary":[]},"char_count":1571,"language_detected":"en","key_concepts":{"key_phrases":["Mixture","MoM","Linear Sequence Modeling","arXiv250213685v3","Announce Type","Linear sequence modeling methods","linear attention","state space modeling","linear RNNs","significant efficiency improvements"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Mixture":3.0,"MoM":2.0,"Linear Sequence Modeling":2.0,"arXiv250213685v3":1.0,"Announce Type":1.0,"Linear sequence modeling methods":1.0,"linear attention":1.0,"state space modeling":1.0,"linear RNNs":1.0,"significant efficiency improvements":1.0}},"age_hours":2.779750953888889,"is_recent":true,"quality_score":1.0,"sentiment_score":8.2985,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6597,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.872,"joy":0.0047,"surprise":0.0153,"sadness":0.0088,"fear":0.0352,"anger":0.035,"disgust":0.0289},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new linear sequence modeling architecture, MoM, which improves performance on recall-intensive tasks. While the code is released, there is no evidence of deployment or real-world application. The potential climate impact is indirect, as more efficient models could reduce energy consumption in AI training and inference, but this is not quantified.","key_impact_metrics":["Linear complexity during training","Constant complexity during inference"],"technology_tags":["Linear Sequence Modeling","Mixture of Experts","AI Efficiency"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:20.068868Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9b04d674bfa0","title":"Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?","content":"arXiv:2502.13925v2 Announce Type: replace Abstract: Large Multimodal Models (LMMs) have achieved remarkable success across various visual-language tasks. However, existing benchmarks predominantly focus on single-image understanding, leaving the analysis of image sequences largely unexplored. To address this limitation, we introduce StripCipher, a comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend and reason over sequential images. StripCipher comprises a human-annotated dataset and three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal narrative reordering. Our evaluation of 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a significant performance gap compared to human capabilities, particularly in tasks that require reordering shuffled sequential images. For instance, GPT-4o achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower than human performance. Further quantitative analysis discuss several factors, such as input format of images, affecting the performance of LLMs in sequential understanding, underscoring the fundamental challenges that remain in the development of LMMs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.13925","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.421360","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":152,"author":"Xiaochen Wang, Heming Xia, Jialin Song, Longyu Guan, Yixin Yang, Qingxiu Dong, Weiyao Luo, Yifan Pu, Yiru Wang, Xiangdi Meng, Wenjie Li, Zhifang Sui","raw_content_length":1215,"priority":7,"update_frequency":1,"reading_time_minutes":0.76,"robust_parsing_used":true,"entities":{"organizations":["Comprehend Temporal","Beyond Single Frames","StripCipher"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1214,"language_detected":"en","key_concepts":{"key_phrases":["LMMs","Single Frames","Temporal and Contextual Narratives","Image Sequences","StripCipher","arXiv250213925v2 Announce Type","Large Multimodal Models","remarkable success","various visual-language tasks","existing benchmarks"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"LMMs":4.0,"Single Frames":2.0,"Temporal and Contextual Narratives":2.0,"Image Sequences":2.0,"StripCipher":2.0,"arXiv250213925v2 Announce Type":1.0,"Large Multimodal Models":1.0,"remarkable success":1.0,"various visual-language tasks":1.0,"existing benchmarks":1.0}},"age_hours":2.7797666622222224,"is_recent":true,"quality_score":1.0,"sentiment_score":8.8915,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7783,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8427,"joy":0.0132,"surprise":0.1094,"sadness":0.0052,"fear":0.01,"anger":0.0136,"disgust":0.006},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on improving Large Multimodal Models' ability to understand image sequences. While not directly related to climate change, improved AI could indirectly contribute to sustainability efforts in the future. The research is in the early stages, with no deployment or economic viability demonstrated.","key_impact_metrics":["GPT-4o accuracy in reordering subtask: 23.93%","Human performance in reordering subtask: 80%"],"technology_tags":["Large Multimodal Models","Artificial Intelligence","Image Recognition"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:22.876661Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7e8353350e56","title":"TransMamba: Fast Universal Architecture Adaption from Transformers to Mamba","content":"arXiv:2502.15130v2 Announce Type: replace Abstract: Transformer-based architectures have become the backbone of both uni-modal and multi-modal foundation models, largely due to their scalability via attention mechanisms, resulting in a rich ecosystem of publicly available pre-trained models such as LLaVA, CLIP, and DeiT, etc. In parallel, emerging sub-quadratic architectures like Mamba offer promising efficiency gains by enabling global context modeling with linear complexity. However, training these architectures from scratch remains resource-intensive (e.g., in terms of data and time). Motivated by this challenge, we explore a cross-architecture knowledge transfer paradigm, termed TransMamba, that facilitates the reuse of Transformer pre-trained knowledge. We propose a two-stage framework to accelerate the training of Mamba-based models, ensuring their effectiveness across both uni-modal and multi-modal tasks. The first stage leverages pre-trained Transformer models to initialize critical components of the Mamba architecture. To bridge architectural and dimensional gaps, we develop a selective weight subcloning strategy and a layered initialization scheme that prioritizes the early $n$ layers. Building on this initialization, the second stage introduces an adaptive multi-directional knowledge distillation method. This mechanism employs layer-wise adaptive scaling factors to align Mamba representations with their Transformer counterparts, while accommodating the scanning order variations inherent to multi-modal Mamba architectures. Despite operating with a reduced training dataset and a more compact model architecture, TransMamba consistently outperforms baseline approaches across diverse mamba-based backbones (e.g., PlainMamba, Vmamba, ViM and VideoMamba) and downstream tasks (e.g., image classification, visual question answering, text-video retrieval and multimodal reasoning). All code and implementation details will be released.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.15130","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.422225","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":248,"author":"Xiuwei Chen, Wentao Hu, Xiao Dong, Sihao Lin, Zisheng Chen, Meng Cao, Yina Zhuang, Jianhua Han, Hang Xu, Xiaodan Liang","raw_content_length":1967,"priority":7,"update_frequency":1,"reading_time_minutes":1.24,"robust_parsing_used":true,"entities":{"organizations":["Transformer","TransMamba","Mamba","DeiT","CLIP"],"persons":["Mamba arXiv:2502.15130v2 Announce Type"],"locations":[],"monetary":[]},"char_count":1966,"language_detected":"en","key_concepts":{"key_phrases":["Mamba","TransMamba","Fast Universal Architecture Adaption","Transformers","arXiv250215130v2","Announce Type","Abstract","Transformer-based architectures","the backbone","both uni-modal and multi-modal foundation models"],"filter_categories":{"ai_ml":["Transformers"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Mamba":3.0,"TransMamba":2.0,"Fast Universal Architecture Adaption":2.0,"Transformers":2.0,"arXiv250215130v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Transformer-based architectures":1.0,"the backbone":1.0,"both uni-modal and multi-modal foundation models":1.0}},"age_hours":2.7797972377777778,"is_recent":true,"quality_score":1.0,"sentiment_score":9.568,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9136,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8356,"joy":0.049,"surprise":0.0905,"sadness":0.0037,"fear":0.0055,"anger":0.0117,"disgust":0.0039},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a method (TransMamba) to accelerate the training of Mamba-based models by transferring knowledge from pre-trained Transformer models. The concrete action is the development of a two-stage framework with selective weight subcloning and adaptive multi-directional knowledge distillation. The evidence is the reported outperformance of baseline approaches across diverse tasks, but this is still in the research phase without deployed units or independent verification.","key_impact_metrics":["Reduced training dataset size","More compact model architecture"],"technology_tags":["Mamba architecture","Transformer models","Knowledge distillation"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:54:26.031897Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_fe8e4f0c5b35","title":"The Euclidean $k$","content":"arXiv:2502.15660v3 Announce Type: replace Abstract: Let $G$ be a complete edge-weighted graph on $n$ vertices. To each subset of vertices of $G$ assign the cost of the minimum spanning tree of the subset as its weight. Suppose that $n$ is a multiple of some fixed positive integer $k$. The $k$-matching problem is the problem of finding a partition of the vertices of $G$ into $k$-sets, that minimizes the sum of the weights of the $k$-sets. The case $k=3$ has been shown to be NP-hard [Johnsson et al.,1998]. In the Euclidean version, the vertices of $G$ are points in the plane and the weight of an edge is the Euclidean distance between its endpoints. We call this problem the Euclidean $k$-matching problem. We show that, for every fixed $k \\ge 3$, the Euclidean $k$-matching is NP-hard. This resolves an open problem in the literature and provides the first theoretical justification for the use of known heuristic methods in the case $k=3$. We also show that the problem remains NP-hard if the trees are required to be paths.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.15660","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.422631","language":"en","tags":["cscg","research","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":176,"author":"Jos\\'e-Miguel D\\'iaz-B\\'a\\~nez, Ruy Fabila-Monroy, Jos\\'e-Manuel Higes-L\\'opez, Nestaly Mar\\'in, Miguel-Angel P\\'erez-Cuti\\~no, Pablo P\\'erez-Lantero","raw_content_length":1032,"priority":7,"update_frequency":1,"reading_time_minutes":0.88,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type","Johnsson"],"locations":[],"monetary":["k=3$.","$k=3$"]},"char_count":1031,"language_detected":"en","key_concepts":{"key_phrases":["The Euclidean","arXiv250215660v3","Announce Type","Abstract","a complete edge-weighted graph","n vertices","each subset","vertices","the cost","the minimum spanning tree"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"The Euclidean":2.0,"arXiv250215660v3":1.0,"Announce Type":1.0,"Abstract":1.0,"a complete edge-weighted graph":1.0,"n vertices":1.0,"each subset":1.0,"vertices":1.0,"the cost":1.0,"the minimum spanning tree":1.0}},"age_hours":2.7798148150000004,"is_recent":true,"quality_score":1.0,"sentiment_score":3.9884999999999997,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2023,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9083,"joy":0.0069,"surprise":0.047,"sadness":0.0081,"fear":0.0051,"anger":0.0164,"disgust":0.0082},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents a theoretical result in computational complexity, showing that a specific optimization problem is NP-hard. While the problem itself could potentially be related to resource allocation, there are no concrete actions or measurable outcomes related to sustainability in the article. It is purely theoretical research.","key_impact_metrics":[],"technology_tags":[],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:54:28.412411Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_bfd90b9744e9","title":"DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks","content":"arXiv:2502.17157v3 Announce Type: replace Abstract: This paper's primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs.\\ 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained model's prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the model's performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the model's ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models. Code and Model: https://github.com/aim-uofa/Diception","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.17157","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.423042","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":228,"author":"Canyu Zhao, Yanlong Sun, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen","raw_content_length":1762,"priority":7,"update_frequency":1,"reading_time_minutes":1.14,"robust_parsing_used":true,"entities":{"organizations":["DICEPTION"],"persons":["DICEPTION","SAM-vit-h"],"locations":[],"monetary":[]},"char_count":1761,"language_detected":"en","key_concepts":{"key_phrases":["DICEPTION","A Generalist Diffusion Model","Visual Perceptual Tasks","arXiv250217157v3 Announce Type","Abstract","This papers primary objective","a robust generalist perception model","multiple tasks","constraints","computational resources"],"filter_categories":{"ai_ml":["constraints"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"DICEPTION":3.0,"A Generalist Diffusion Model":2.0,"Visual Perceptual Tasks":2.0,"arXiv250217157v3 Announce Type":1.0,"Abstract":1.0,"This papers primary objective":1.0,"a robust generalist perception model":1.0,"multiple tasks":1.0,"constraints":1.0,"computational resources":1.0}},"age_hours":2.779829325,"is_recent":true,"quality_score":1.0,"sentiment_score":8.5015,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7003,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8951,"joy":0.0397,"surprise":0.0334,"sadness":0.0038,"fear":0.0076,"anger":0.0145,"disgust":0.0059},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel diffusion model for visual perception tasks, demonstrating significant data efficiency compared to existing models (achieving results on par with SAM-vit-h using only 0.06% of their data). While the model itself doesn't directly address climate change, its potential to reduce computational costs associated with training complex AI models could indirectly contribute to energy savings. The research is at the applied research stage, with code and model available but no real-world deployments mentioned.","key_impact_metrics":["0.06% data usage compared to SAM-vit-h","fine-tuning on as few as 50 images"],"technology_tags":["diffusion models","visual perception","artificial intelligence"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:54:31.750358Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d595a3f4452c","title":"Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models","content":"arXiv:2502.19982v3 Announce Type: replace Abstract: In this paper, we investigate knowledge forgetting in large language models with a focus on its generalisation, ensuring that models forget not only specific training samples but also related implicit knowledge. To this end, we begin by identifying a broader unlearning scope that includes both target data and logically associated samples, including rephrased, subject-replaced, relation-reversed, and one-hop reasoned data. We then conduct a rigorous evaluation of 15 state-of-the-art methods across three datasets, revealing that unlearned models still recall paraphrased answers and retain target facts in their intermediate layers. This motivates us to take a preliminary step toward more generalised implicit knowledge forgetting by proposing PerMU, a novel probability perturbation-based unlearning paradigm. PerMU simulates adversarial unlearning samples to eliminate fact-related tokens from the logit distribution, collectively reducing the probabilities of all answer-associated tokens. Experiments are conducted on a diverse range of datasets, including TOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B in scale. The results demonstrate that PerMU delivers up to a 50.40% improvement in unlearning vanilla target data while maintaining a 40.73% boost in forgetting implicit knowledge. Our code can be found in https://github.com/MaybeLizzy/PERMU.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2502.19982","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.423482","language":"en","tags":["cscl","computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao","raw_content_length":1442,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1441,"language_detected":"en","key_concepts":{"key_phrases":["Remembering","Implicit Knowledge Forgetting","Large Language Models","arXiv250219982v3 Announce Type","Abstract","this paper","knowledge","large language models","a focus","its generalisation"],"filter_categories":{"ai_ml":["Large Language Models","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Remembering":2.0,"Implicit Knowledge Forgetting":2.0,"Large Language Models":2.0,"arXiv250219982v3 Announce Type":1.0,"Abstract":1.0,"this paper":1.0,"knowledge":1.0,"large language models":1.0,"a focus":1.0,"its generalisation":1.0}},"age_hours":2.779844333611111,"is_recent":true,"quality_score":1.0,"sentiment_score":5.1290000000000004,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.898,"joy":0.0055,"surprise":0.0309,"sadness":0.0133,"fear":0.0102,"anger":0.0185,"disgust":0.0235},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (PerMU) for improving knowledge unlearning in large language models. The concrete action is the development and testing of this method, with measurable outcomes reported as a percentage improvement in unlearning performance (up to 50.40% improvement in unlearning vanilla target data). The stage of deployment is applied research, with experiments conducted on various datasets and model sizes.","key_impact_metrics":["50.40% improvement in unlearning vanilla target data","40.73% boost in forgetting implicit knowledge"],"technology_tags":["Large Language Models","Knowledge Unlearning","Probability Perturbation"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:54:34.971057Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e3978237cc9b","title":"Instructor","content":"arXiv:2503.00566v4 Announce Type: replace Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.00566","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.423951","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":168,"author":"Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li","raw_content_length":1157,"priority":7,"update_frequency":1,"reading_time_minutes":0.84,"robust_parsing_used":true,"entities":{"organizations":["Instructor","Worker","the Digital Twin Building"],"persons":[],"locations":["Los Angeles"],"monetary":["more than 250 billion dollars"]},"char_count":1156,"language_detected":"en","key_concepts":{"key_phrases":["Instructor","arXiv250300566v4","Announce Type","Abstract","The Los Angeles wildfires","January","more than 250 billion dollars","damage","nearly an entire month","containment"],"filter_categories":{"ai_ml":["containment"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Instructor":2.0,"arXiv250300566v4":1.0,"Announce Type":1.0,"Abstract":1.0,"The Los Angeles wildfires":1.0,"January":1.0,"more than 250 billion dollars":1.0,"damage":1.0,"nearly an entire month":1.0,"containment":1.0}},"age_hours":2.7798595405555555,"is_recent":true,"quality_score":1.0,"sentiment_score":3.634,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2732,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9126,"joy":0.0058,"surprise":0.0367,"sadness":0.0188,"fear":0.0061,"anger":0.0144,"disgust":0.0057},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a multi-agent LLM system for analyzing air quality data during the Los Angeles wildfires. While it uses real-world data (air quality during wildfires), it's primarily a research project focused on data analysis and policy recommendation capabilities, not a deployed solution with measurable environmental outcomes. The system's ability to provide health recommendations based on air quality is a concrete action, but the impact on climate change mitigation or adaptation is indirect and not quantified.","key_impact_metrics":["Damage caused by wildfires: $250 billion","Wildfire duration: 1 month"],"technology_tags":["Large Language Models","Digital Twins","Air Quality Monitoring"],"sdg_alignment":[3,11,13],"analyzed_at":"2025-10-28T20:54:38.081061Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e83d6ca4a36f","title":"Uncertainty Comes for Free: Human-in","content":"arXiv:2503.01876v3 Announce Type: replace Abstract: Human-in-the-loop (HitL) robot deployment has gained significant attention in both academia and industry as a semi-autonomous paradigm that enables human operators to intervene and adjust robot behaviors at deployment time, improving success rates. However, continuous human monitoring and intervention can be highly labor-intensive and impractical when deploying a large number of robots. To address this limitation, we propose a method that allows diffusion policies to actively seek human assistance only when necessary, reducing reliance on constant human oversight. To achieve this, we leverage the generative process of diffusion policies to compute an uncertainty-based metric based on which the autonomous agent can decide to request operator assistance at deployment time, without requiring any operator interaction during training. Additionally, we show that the same method can be used for efficient data collection for fine-tuning diffusion policies in order to improve their autonomous performance. Experimental results from simulated and real-world environments demonstrate that our approach enhances policy performance during deployment for a variety of scenarios.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.01876","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.425120","language":"en","tags":["csro","computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Zhanpeng He, Yifeng Cao, Matei Ciocarlie","raw_content_length":1232,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1231,"language_detected":"en","key_concepts":{"key_phrases":["Uncertainty","Free Human","arXiv250301876v3 Announce Type","Abstract","the-loop","significant attention","both academia","industry","a semi-autonomous paradigm","human operators"],"filter_categories":{"ai_ml":["Uncertainty"],"research_academic":["both academia"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Uncertainty":2.0,"Free Human":2.0,"arXiv250301876v3 Announce Type":1.0,"Abstract":1.0,"the-loop":1.0,"significant attention":1.0,"both academia":1.0,"industry":1.0,"a semi-autonomous paradigm":1.0,"human operators":1.0}},"age_hours":2.7799071383333334,"is_recent":true,"quality_score":0.7,"sentiment_score":9.3125,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8625,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6465,"joy":0.0367,"surprise":0.0191,"sadness":0.0213,"fear":0.2515,"anger":0.0151,"disgust":0.0097},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":4,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":true},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"The article describes a method to reduce human intervention in robot deployment, potentially leading to more efficient use of resources and reduced energy consumption in robotic operations. The approach is validated in simulated and real-world environments, providing some evidence of its effectiveness. However, the specific climate impact is not quantified, and the economic viability is unclear at this stage.","key_impact_metrics":["Reduced reliance on constant human oversight","Enhanced policy performance during deployment"],"technology_tags":["Human-in-the-loop robotics","Diffusion policies"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T20:54:40.644060Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f936f83d4526","title":"Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling","content":"arXiv:2503.02233v4 Announce Type: replace Abstract: Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.02233","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.425532","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":164,"author":"Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu","raw_content_length":1375,"priority":7,"update_frequency":1,"reading_time_minutes":0.82,"robust_parsing_used":true,"entities":{"organizations":["the Explicit Knowledge Boundary Modeling","EKBM","Explicit Knowledge Boundary Modeling arXiv:2503.02233v4"],"persons":[],"locations":[],"monetary":[]},"char_count":1374,"language_detected":"en","key_concepts":{"key_phrases":["LLM Reliability","Explicit Knowledge Boundary Modeling","arXiv250302233v4 Announce Type","Large language models","LLMs","misaligned self-awareness","their knowledge boundaries","existing mitigation strategies","uncertainty estimation","query rejection mechanisms"],"filter_categories":{"ai_ml":["LLM Reliability","Large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"LLM Reliability":2.0,"Explicit Knowledge Boundary Modeling":2.0,"arXiv250302233v4 Announce Type":1.0,"Large language models":1.0,"LLMs":1.0,"misaligned self-awareness":1.0,"their knowledge boundaries":1.0,"existing mitigation strategies":1.0,"uncertainty estimation":1.0,"query rejection mechanisms":1.0}},"age_hours":2.7799210694444447,"is_recent":true,"quality_score":1.0,"sentiment_score":1.9379999999999997,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6124,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.619,"joy":0.0038,"surprise":0.0172,"sadness":0.0401,"fear":0.2389,"anger":0.0185,"disgust":0.0625},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article proposes a framework (EKBM) to improve LLM reliability, which could indirectly contribute to sustainability by improving the accuracy of information used in climate modeling or resource management. The framework uses a fast and slow reasoning system and a hybrid training pipeline. Evaluations on dialogue state tracking tasks demonstrate superior model reliability over uncertainty-based baselines, and refinement boosts accuracy while maintaining low computational overhead.","key_impact_metrics":["Accuracy improvement","Computational overhead reduction"],"technology_tags":["Large Language Models","Artificial Intelligence","Knowledge Boundary Modeling"],"sdg_alignment":[9,13],"analyzed_at":"2025-10-28T20:54:43.374919Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_adef47155a62","title":"Teaching Your Models to Understand Code via Focal Preference Alignment","content":"arXiv:2503.02783v4 Announce Type: replace Abstract: Preference learning extends the performance of Code LLMs beyond traditional supervised fine-tuning by leveraging relative quality comparisons. In existing approaches, a set of n candidate solutions is evaluated based on test case success rates, with the candidate demonstrating a higher pass rate being labeled as positive and its counterpart with a lower pass rate as negative. However, because this approach aligns entire failing code blocks rather than pinpointing specific errors, it lacks the granularity necessary to capture meaningful error-correction relationships. As a result, the model is unable to learn more informative error-correction patterns. To address these issues, we propose Target-DPO, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. Target-DPO explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To facilitate it, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with Target-DPO achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that Target-DPO yields fewer errors. Code, model and datasets are in: https://github.com/JieWu02/Target-DPO.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.02783","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.425933","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Jie Wu, Haoling Li, Xin Zhang, Xiao Liu, Yangyu Huang, Jianwen Luo, Yizhen Zhang, Zuchao Li, Ruihang Chu, Yujiu Yang, Scarlett Li","raw_content_length":1458,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["Target","Focal Preference Alignment arXiv:2503.02783v4"],"persons":[],"locations":[],"monetary":[]},"char_count":1457,"language_detected":"en","key_concepts":{"key_phrases":["Your Models","Code","Focal Preference Alignment","arXiv250302783v4","Announce Type","Abstract","Preference learning","the performance","Code LLMs","traditional supervised fine-tuning"],"filter_categories":{"ai_ml":["Code LLMs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Your Models":2.0,"Code":2.0,"Focal Preference Alignment":2.0,"arXiv250302783v4":1.0,"Announce Type":1.0,"Abstract":1.0,"Preference learning":1.0,"the performance":1.0,"Code LLMs":1.0,"traditional supervised fine-tuning":1.0}},"age_hours":2.7799358350000003,"is_recent":true,"quality_score":1.0,"sentiment_score":7.786999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5574,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8542,"joy":0.0144,"surprise":0.0114,"sadness":0.0058,"fear":0.0345,"anger":0.0365,"disgust":0.0432},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a new preference alignment framework (Target-DPO) for Code LLMs that improves code generation and reduces errors. The framework is validated through extensive experiments showing performance gains on code generation tasks. However, it is still in the research phase, with no deployed units or customer contracts, hence the low deployment readiness and economic viability scores.","key_impact_metrics":["Significant performance gains in code generation","Fewer errors"],"technology_tags":["Code LLMs","Preference Alignment","Iterative Debugging"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:54:45.883693Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f211315260ce","title":"Spiking Meets Attention: Efficient Remote Sensing Image Super","content":"arXiv:2503.04223v3 Announce Type: replace Abstract: Spiking neural networks (SNNs) are emerging as a promising alternative to traditional artificial neural networks (ANNs), offering biological plausibility and energy efficiency. Despite these merits, SNNs are frequently hampered by limited capacity and insufficient representation power, yet remain underexplored in remote sensing super-resolution (SR) tasks. In this paper, we first observe that spiking signals exhibit drastic intensity variations across diverse textures, highlighting an active learning state of the neurons. This observation motivates us to apply SNNs for efficient SR of RSIs. Inspired by the success of attention mechanisms in representing salient information, we devise the spiking attention block (SAB), a concise yet effective component that optimizes membrane potentials through inferred attention weights, which, in turn, regulates spiking activity for superior feature representation. Our key contributions include: 1) we bridge the independent modulation between temporal and channel dimensions, facilitating joint feature correlation learning, and 2) we access the global self-similar patterns in large-scale remote sensing imagery to infer spatial attention weights, incorporating effective priors for realistic and faithful reconstruction. Building upon SAB, we proposed SpikeSR, which achieves state-of-the-art performance across various remote sensing benchmarks such as AID, DOTA, and DIOR, while maintaining high computational efficiency. Code of SpikeSR will be available at https://github.com/XY-boy/SpikeSR.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.04223","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.426332","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Yi Xiao, Qiangqiang Yuan, Kui Jiang, Wenke Huang, Qiang Zhang, Tingting Zheng, Chia-Wen Lin, Liangpei Zhang","raw_content_length":1599,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["SAB"],"persons":[],"locations":[],"monetary":[]},"char_count":1598,"language_detected":"en","key_concepts":{"key_phrases":["Spiking Meets Attention","Efficient Remote Sensing Image Super","SNNs","arXiv250304223v3 Announce Type","Abstract","neural networks","a promising alternative","traditional artificial neural networks","ANNs","biological plausibility"],"filter_categories":{"ai_ml":["neural networks","traditional artificial neural networks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Spiking Meets Attention":2.0,"Efficient Remote Sensing Image Super":2.0,"SNNs":2.0,"arXiv250304223v3 Announce Type":1.0,"Abstract":1.0,"neural networks":1.0,"a promising alternative":1.0,"traditional artificial neural networks":1.0,"ANNs":1.0,"biological plausibility":1.0}},"age_hours":2.779950675277778,"is_recent":true,"quality_score":1.0,"sentiment_score":9.351,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8702,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7946,"joy":0.0299,"surprise":0.0543,"sadness":0.0304,"fear":0.0483,"anger":0.0251,"disgust":0.0174},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to remote sensing image super-resolution using spiking neural networks (SNNs), which are potentially more energy-efficient than traditional ANNs. The claim of computational efficiency is supported by achieving state-of-the-art performance on remote sensing benchmarks, but there is no mention of deployed units or real-world operational data, placing it at the applied research stage.","key_impact_metrics":["High computational efficiency","State-of-the-art performance across various remote sensing benchmarks"],"technology_tags":["Spiking Neural Networks","Remote Sensing","Super-Resolution","Attention Mechanisms"],"sdg_alignment":[9,11,13],"analyzed_at":"2025-10-28T20:54:48.734683Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c9081bcc648b","title":"LLM Applications: Current Paradigms and the Next Frontier","content":"arXiv:2503.04596v2 Announce Type: replace Abstract: The development of large language models (LLMs) has given rise to four major application paradigms: LLM app stores, LLM agents, self-hosted LLM services, and LLM-powered devices. Each has its advantages but also shares common challenges. LLM app stores lower the barrier to development but lead to platform lock-in; LLM agents provide autonomy but lack a unified communication mechanism; self-hosted LLM services enhance control but increase deployment complexity; and LLM-powered devices improve privacy and real-time performance but are limited by hardware. This paper reviews and analyzes these paradigms, covering architecture design, application ecosystem, research progress, as well as the challenges and open problems they face. Based on this, we outline the next frontier of LLM applications, characterizing them through three interconnected layers: infrastructure, protocol, and application. We describe their responsibilities and roles of each layer and demonstrate how to mitigate existing fragmentation limitations and improve security and scalability. Finally, we discuss key future challenges, identify opportunities such as protocol-driven cross-platform collaboration and device integration, and propose a research roadmap for openness, security, and sustainability.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.04596","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.426743","language":"en","tags":["csse","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":178,"author":"Xinyi Hou, Yanjie Zhao, Haoyu Wang","raw_content_length":1335,"priority":7,"update_frequency":1,"reading_time_minutes":0.89,"robust_parsing_used":true,"entities":{"organizations":["LLM","the Next Frontier arXiv:2503.04596v2 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1334,"language_detected":"en","key_concepts":{"key_phrases":["LLM Applications","Current Paradigms","the Next Frontier","LLM app stores","LLM agents","self-hosted LLM services","Announce Type","Abstract","The development","large language models"],"filter_categories":{"ai_ml":["LLM Applications","large language models"],"engineering":["The development"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"LLM Applications":2.0,"Current Paradigms":2.0,"the Next Frontier":2.0,"LLM app stores":2.0,"LLM agents":2.0,"self-hosted LLM services":2.0,"Announce Type":1.0,"Abstract":1.0,"The development":1.0,"large language models":1.0}},"age_hours":2.7799659699999997,"is_recent":true,"quality_score":1.0,"sentiment_score":8.9525,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7905,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8843,"joy":0.0126,"surprise":0.074,"sadness":0.0132,"fear":0.005,"anger":0.0069,"disgust":0.004},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":2,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper analyzes LLM application paradigms and proposes a future research roadmap. While it discusses sustainability as a future challenge, it lacks concrete actions or measurable outcomes related to climate impact. The research is at an early stage, with no deployed technology or quantified environmental benefits.","key_impact_metrics":[],"technology_tags":["Large Language Models","AI"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T20:54:51.224924Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_5830c0b843a7","title":"DiMA: An LLM","content":"arXiv:2503.04768v3 Announce Type: replace Abstract: On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed urban transportation, offering unmatched convenience and flexibility. In this paper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in DiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond through a natural and efficient conversational interface under dynamic and complex spatiotemporal urban contexts. To achieve this, we propose a spatiotemporal-aware order planning module that leverages external tools for precise spatiotemporal reasoning and progressive order planning. Additionally, we develop a cost-effective dialogue system that integrates multi-type dialog repliers with cost-aware LLM configurations to handle diverse conversation goals and trade-off response quality and latency. Furthermore, we introduce a continual fine-tuning scheme that utilizes real-world interactions and simulated dialogues to align the assistant's behavior with human preferred decision-making processes. Since its deployment in the DiDi application, DiMA has demonstrated exceptional performance, achieving 93% accuracy in order planning and 92% in response generation during real-world interactions. Offline experiments further validate DiMA capabilities, showing improvements of up to 70.23% in order planning and 321.27% in response generation compared to three state-of-the-art agent frameworks, while reducing latency by $0.72\\times$ to $5.47\\times$. These results establish DiMA as an effective, efficient, and intelligent mobile assistant for ride-hailing services. Our project is released at https://github.com/usail-hkust/DiMA and we also release the MCP service (https://mcp.didichuxing.com/api) to foster the ride-hailing research community.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.04768","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.427154","language":"en","tags":["computer-science","preprints","cshc","cscl","cscy","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":227,"author":"Yansong Ning, Shuowei Cai, Wei Li, Jun Fang, Naiqiang Tan, Hua Chai, Hao Liu","raw_content_length":1813,"priority":7,"update_frequency":1,"reading_time_minutes":1.135,"robust_parsing_used":true,"entities":{"organizations":["LLM","DiDi, Uber"],"persons":["DiMA"],"locations":["DiDi Chuxing"],"monetary":[]},"char_count":1812,"language_detected":"en","key_concepts":{"key_phrases":["An LLM","Announce Type","Abstract","demand","DiDi","Uber","Lyft","urban transportation","unmatched convenience","flexibility"],"filter_categories":{"ai_ml":["An LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"An LLM":2.0,"Announce Type":1.0,"Abstract":1.0,"demand":1.0,"DiDi":1.0,"Uber":1.0,"Lyft":1.0,"urban transportation":1.0,"unmatched convenience":1.0,"flexibility":1.0}},"age_hours":2.779980783888889,"is_recent":true,"quality_score":1.0,"sentiment_score":9.4425,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8885,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7825,"joy":0.1268,"surprise":0.0528,"sadness":0.0029,"fear":0.0147,"anger":0.0143,"disgust":0.006},"emotion_method":"local"},"sustainability_analysis":{"content_type":"technology_deployment","innovation_stage":"commercial","climate_impact_potential":5,"technical_credibility":7,"economic_viability":6,"deployment_readiness":7,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":true,"has_metrics":true,"has_peer_review":true,"has_deployment":true},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"DiMA is an LLM-powered ride-hailing assistant deployed in DiDi Chuxing, demonstrating real-world usage. The article provides specific performance metrics, such as 93% accuracy in order planning and 92% in response generation, and compares its performance against state-of-the-art frameworks. The deployment and measured outcomes provide evidence of its effectiveness, though the climate impact is indirect through potential optimization of ride-sharing.","key_impact_metrics":["93% accuracy in order planning","92% accuracy in response generation"],"technology_tags":["LLM","Ride-hailing","Spatiotemporal reasoning"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-28T20:54:54.009548Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_73915215494c","title":"Finding a Fair Scoring Function for Top","content":"arXiv:2503.11575v3 Announce Type: replace Abstract: Selecting a subset of the $k$ \"best\" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the rise of automated decision-making software, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a fair linear scoring function for top-$k$ selection. The function computes a score for each item as a weighted sum of its (numerical) attribute values, and must ensure that the selected subset includes adequate representation of a minority or historically disadvantaged group. Existing algorithms do not scale efficiently, particularly in higher dimensions. Our hardness analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size, and the computational complexity is likely to increase rapidly with dimensionality. However, the hardness results also provide key insights guiding algorithm design, leading to our two-pronged solution: (1) For small values of $k$, our hardness analysis reveals a gap in the hardness barrier. By addressing various engineering challenges, including achieving efficient parallelism, we turn this potential of efficiency into an optimized algorithm delivering substantial practical performance gains. (2) For large values of $k$, where the hardness is robust, we employ a practically efficient algorithm which, despite being theoretically worse, achieves superior real-world performance. Experimental evaluations on real-world datasets then explore scenarios where worst-case behavior does not manifest, identifying areas critical to practical performance. Our solution achieves speed-ups of up to several orders of magnitude compared to SOTA, an efficiency made possible through a tight integration of hardness analysis, algorithm design, practical engineering, and empirical evaluation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.11575","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.427969","language":"en","tags":["csdb","csds","csdc","computer-science","preprints","cscy","cscc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":278,"author":"Guangya Cai","raw_content_length":1959,"priority":7,"update_frequency":1,"reading_time_minutes":1.39,"robust_parsing_used":true,"entities":{"organizations":["top-$k$ selection"],"persons":["top-$k$"],"locations":[],"monetary":[]},"char_count":1958,"language_detected":"en","key_concepts":{"key_phrases":["a Fair Scoring Function","Top","top-k selection","Announce Type","Abstract","a subset","the k best items","a dataset","n items","a scoring function"],"filter_categories":{"ai_ml":["a Fair Scoring Function"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"a Fair Scoring Function":2.0,"Top":2.0,"top-k selection":2.0,"Announce Type":1.0,"Abstract":1.0,"a subset":1.0,"the k best items":1.0,"a dataset":1.0,"n items":1.0,"a scoring function":1.0}},"age_hours":2.780010401111111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.375,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.875,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8781,"joy":0.0112,"surprise":0.0115,"sadness":0.0053,"fear":0.0368,"anger":0.0352,"disgust":0.0219},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":7,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a new algorithm for fair top-k selection, aiming to ensure adequate representation of minority groups in automated decision-making. The research includes hardness analysis, algorithm design, and experimental evaluations on real-world datasets, achieving speed-ups compared to state-of-the-art. While the algorithm itself doesn't directly reduce GHG emissions, its focus on fair resource allocation could indirectly impact climate justice by ensuring equitable access to climate mitigation and adaptation resources.","key_impact_metrics":["Speed-ups of up to several orders of magnitude compared to SOTA"],"technology_tags":["Fairness algorithm","Automated decision-making"],"sdg_alignment":[5,10,16],"analyzed_at":"2025-10-28T20:54:56.877357Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_447359426dba","title":"Uncertainty","content":"arXiv:2503.15742v2 Announce Type: replace Abstract: Reconstructing 3D scenes from a single image is a fundamentally ill-posed task due to the severely under-constrained nature of the problem. Consequently, when the scene is rendered from novel camera views, existing single image to 3D reconstruction methods render incoherent and blurry views. This problem is exacerbated when the unseen regions are far away from the input camera. In this work, we address these inherent limitations in existing single image-to-3D scene feedforward networks. To alleviate the poor performance due to insufficient information beyond the input image's view, we leverage a strong generative prior in the form of a pre-trained latent video diffusion model, for iterative refinement of a coarse scene represented by optimizable Gaussian parameters. To ensure that the style and texture of the generated images align with that of the input image, we incorporate on-the-fly Fourier-style transfer between the generated images and the input image. Additionally, we design a semantic uncertainty quantification module that calculates the per-pixel entropy and yields uncertainty maps used to guide the refinement process from the most confident pixels while discarding the remaining highly uncertain ones. We conduct extensive experiments on real-world scene datasets, including in-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach can provide more realistic and high-fidelity novel view synthesis results compared to existing state-of-the-art methods.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.15742","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.428361","language":"en","tags":["research","cscv","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":219,"author":"Sarosij Bose, Arindam Dutta, Sayak Nag, Junge Zhang, Jiachen Li, Konstantinos Karydis, Amit K. Roy Chowdhury","raw_content_length":1555,"priority":7,"update_frequency":1,"reading_time_minutes":1.095,"robust_parsing_used":true,"entities":{"organizations":["Uncertainty arXiv:2503.15742v2 Announce Type: replace Abstract"],"persons":["Fourier"],"locations":[],"monetary":[]},"char_count":1554,"language_detected":"en","key_concepts":{"key_phrases":["Uncertainty","Announce Type","Abstract","3D scenes","a single image","a fundamentally ill-posed task","the severely under-constrained nature","the problem","the scene","novel camera views"],"filter_categories":{"ai_ml":["Uncertainty"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Uncertainty":2.0,"Announce Type":1.0,"Abstract":1.0,"3D scenes":1.0,"a single image":1.0,"a fundamentally ill-posed task":1.0,"the severely under-constrained nature":1.0,"the problem":1.0,"the scene":1.0,"novel camera views":1.0}},"age_hours":2.7800252658333333,"is_recent":true,"quality_score":1.0,"sentiment_score":0.8200000000000002,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.836,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.3944,"joy":0.0041,"surprise":0.0203,"sadness":0.079,"fear":0.1357,"anger":0.0786,"disgust":0.2879},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for 3D scene reconstruction, but its direct impact on sustainability is limited. The research is in the applied research stage, with experiments conducted on datasets but no real-world deployment or quantified environmental benefits. The vaporware risk is present as it is an early-stage concept with no deployed units.","key_impact_metrics":[],"technology_tags":["3D reconstruction","Generative AI","Diffusion Models"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:55:01.730801Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_259a5186f688","title":"Unified Cross","content":"arXiv:2503.16278v3 Announce Type: replace Abstract: 3D structure modeling is essential across scales, enabling applications from fluid simulation and 3D reconstruction to protein folding and molecular docking. Yet, despite shared 3D spatial patterns, current approaches remain fragmented, with models narrowly specialized for specific domains and unable to generalize across tasks or scales. We propose Uni-3DAR, a unified autoregressive framework for cross-scale 3D generation and understanding. At its core is a coarse-to-fine tokenizer based on octree data structures, which compresses diverse 3D structures into compact 1D token sequences. We further propose a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. To address the challenge of dynamically varying token positions introduced by compression, we introduce a masked next-token prediction strategy that ensures accurate positional modeling, significantly boosting model performance. Extensive experiments across multiple 3D generation and understanding tasks, including small molecules, proteins, polymers, crystals, and macroscopic 3D objects, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.16278","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.429291","language":"en","tags":["cond-matmtrl-sci","q-biobm","computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":181,"author":"Shuqi Lu, Haowei Lin, Lin Yao, Zhifeng Gao, Xiaohong Ji, Yitao Liang, Weinan E, Linfeng Zhang, Guolin Ke","raw_content_length":1386,"priority":7,"update_frequency":1,"reading_time_minutes":0.905,"robust_parsing_used":true,"entities":{"organizations":["Unified Cross"],"persons":[],"locations":[],"monetary":[]},"char_count":1385,"language_detected":"en","key_concepts":{"key_phrases":["Unified Cross","scales","Announce Type","Abstract","3D structure modeling","applications","fluid simulation","3D reconstruction","protein folding","molecular docking"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Unified Cross":2.0,"scales":2.0,"Announce Type":1.0,"Abstract":1.0,"3D structure modeling":1.0,"applications":1.0,"fluid simulation":1.0,"3D reconstruction":1.0,"protein folding":1.0,"molecular docking":1.0}},"age_hours":2.7800538705555558,"is_recent":true,"quality_score":1.0,"sentiment_score":7.439,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4878,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.847,"joy":0.0085,"surprise":0.0229,"sadness":0.0101,"fear":0.0512,"anger":0.0364,"disgust":0.024},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework for 3D structure modeling across scales, with potential applications in areas like protein folding and material design. The concrete action is the development of a new autoregressive framework (Uni-3DAR) and a coarse-to-fine tokenizer. Evidence includes experimental results showing improved performance and inference speeds, but it is still in the research phase with no deployed technology.","key_impact_metrics":["256% relative improvement","21.8x faster inference speeds"],"technology_tags":["3D modeling","autoregressive models","octree data structures"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:55:07.673575Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c8f770528d43","title":"Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act","content":"arXiv:2503.18156v3 Announce Type: replace Abstract: AI-generated images have become so good in recent years that individuals often cannot distinguish them any more from \"real\" images. This development, combined with the rapid spread of AI-generated content online, creates a series of societal risks. Watermarking, a technique that involves embedding information within images and other content to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated content. Indeed, watermarking and AI labelling measures are now becoming a legal requirement in many jurisdictions, including under the 2024 European Union AI Act. Despite the widespread use of AI image generation systems, the practical implications and the current status of implementation of these measures remain largely unexamined. The present paper therefore provides both an empirical and a legal analysis of these measures. In our legal analysis, we identify four categories of generative AI deployment scenarios and outline how the legal obligations could apply in each category. In our empirical analysis, we find that only a minority number of AI image generators currently implement adequate watermarking (38%) and deep fake labelling (18%) practices. In response, we suggest a range of avenues of how the implementation of these legally mandated techniques can be improved, and publicly share our tooling for the detection of watermarks in images.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.18156","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.430504","language":"en","tags":["computer-science","csai","preprints","cscy","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":217,"author":"Bram Rijsbosch, Gijs van Dijck, Konrad Kollnig","raw_content_length":1472,"priority":7,"update_frequency":1,"reading_time_minutes":1.085,"robust_parsing_used":true,"entities":{"organizations":["European Union AI Act","Practice"],"persons":[],"locations":[],"monetary":[]},"char_count":1471,"language_detected":"en","key_concepts":{"key_phrases":["Watermarking","Adoption","Generative AI Systems","Practice","Implications","the new EU AI Act","arXiv250318156v3 Announce Type","Abstract","AI-generated images","recent years"],"filter_categories":{"ai_ml":["Generative AI Systems"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Watermarking":3.0,"Adoption":2.0,"Generative AI Systems":2.0,"Practice":2.0,"Implications":2.0,"the new EU AI Act":2.0,"arXiv250318156v3 Announce Type":1.0,"Abstract":1.0,"AI-generated images":1.0,"recent years":1.0}},"age_hours":2.780097218611111,"is_recent":true,"quality_score":1.0,"sentiment_score":7.8885000000000005,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5777,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.5301,"joy":0.0084,"surprise":0.0138,"sadness":0.0101,"fear":0.3183,"anger":0.0656,"disgust":0.0537},"emotion_method":"local"},"sustainability_analysis":{"content_type":"policy_action","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":true},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article discusses the EU AI Act and its requirement for watermarking AI-generated content. The empirical analysis shows only 38% of AI image generators implement adequate watermarking, indicating limited deployment. The paper shares tooling for watermark detection, but this is still in the applied research stage.","key_impact_metrics":["watermarking implementation rate 38%","deep fake labelling 18%"],"technology_tags":["watermarking","AI labelling","deep fake detection"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-28T20:55:10.786413Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_8ea8e15af1f5","title":"More Bang for the Buck: Process Reward Modeling with Entropy","content":"arXiv:2503.22233v3 Announce Type: replace Abstract: We introduce the Entropy-Driven Uncertainty Process Reward Model (EDU-PRM), a novel entropy-driven training framework for process reward modeling that enables dynamic, uncertainty-aligned segmentation of complex reasoning steps, eliminating the need for costly manual step annotations. Unlike previous Process Reward Models (PRMs) that rely on static partitioning and human labeling, EDU-PRM automatically anchors step boundaries at tokens with high predictive entropy, effectively capturing intrinsic logical transitions and facilitating efficient exploration of diverse reasoning paths. On the ProcessBench benchmark, EDU-PRM outperforms strong public PRM baselines, such as Math-Shepherd PRM and Omega PRM, and EDU-PRM achieves comparable results with SOTA models while only using 1.5% training data. Furthermore, by leveraging our proposed EDU sampling strategy, we observe accuracy boosts from 64.7% to 67.3% for generative reasoning tasks, accompanied by a reduction of 32% in token usage. These findings underscore the potential of EDU-PRM as a scalable and annotation-efficient paradigm for process supervision in mathematical reasoning, paving the way for more efficient and robust approaches to complex mathematical problem solving.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.22233","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.431271","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Lang Cao, Renhong Chen, Yingtian Zou, Chao Peng, Huacong Xu, Yuxian Wang, Wu Ning, Qian Chen, Mofan Peng, Zijie Chen, Peishuo Su, Sirui Han, Yitong Li","raw_content_length":1295,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["PRM","ProcessBench","the Entropy-Driven Uncertainty Process Reward Model","Math-Shepherd PRM","Process Reward Models","EDU"],"persons":["Omega PRM"],"locations":[],"monetary":[]},"char_count":1294,"language_detected":"en","key_concepts":{"key_phrases":["the Buck","Process","Modeling","Entropy","EDU-PRM","arXiv250322233v3 Announce Type","Abstract","the Entropy-Driven Uncertainty Process Reward Model","a novel entropy-driven training framework","process reward modeling"],"filter_categories":{"ai_ml":["Process"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"the Buck":2.0,"Process":2.0,"Modeling":2.0,"Entropy":2.0,"EDU-PRM":2.0,"arXiv250322233v3 Announce Type":1.0,"Abstract":1.0,"the Entropy-Driven Uncertainty Process Reward Model":1.0,"a novel entropy-driven training framework":1.0,"process reward modeling":1.0}},"age_hours":2.7801259755555554,"is_recent":true,"quality_score":1.0,"sentiment_score":9.7545,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9509,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8963,"joy":0.0161,"surprise":0.0414,"sadness":0.0031,"fear":0.0184,"anger":0.0182,"disgust":0.0065},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new method (EDU-PRM) for training AI models to solve mathematical problems more efficiently. The concrete action is the development of a new algorithm and its testing on a benchmark dataset. The evidence supporting the claims is the reported accuracy boosts and reduction in token usage, but it is still in the applied research stage with no real-world deployment.","key_impact_metrics":["accuracy boosts from 64.7% to 67.3%","reduction of 32% in token usage"],"technology_tags":["AI","Machine Learning","Process Reward Modeling"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:55:13.441254Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6d5d6f4dcc03","title":"Adaptive Layer","content":"arXiv:2503.23798v3 Announce Type: replace Abstract: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, limited attention has been paid to a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive computation in LLMs without modifying their original parameters. Applied to Llama-3-8B, it skips 8 out of 32 layers while maintaining full benchmark performance. Our experiments reveal that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Despite the computational savings, FlexiDepth does not yet achieve wall-clock speedup due to varied skipping patterns and I/O overhead. To inspire future work and advance research on practical speedup, we open-sourced FlexiDepth and a dataset documenting its layer allocation patterns.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2503.23798","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.431681","language":"en","tags":["cscl","computer-science","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":169,"author":"Xuan Luo, Weizhi Wang, Xifeng Yan","raw_content_length":1235,"priority":7,"update_frequency":1,"reading_time_minutes":0.845,"robust_parsing_used":true,"entities":{"organizations":["FlexiDepth","Transformer"],"persons":["Adaptive Layer arXiv:2503.23798v3 Announce Type"],"locations":[],"monetary":[]},"char_count":1234,"language_detected":"en","key_concepts":{"key_phrases":["Adaptive Layer","FlexiDepth","arXiv250323798v3 Announce Type","Abstract","Various layer-skipping methods","token generation","large language models","LLMs","limited attention","a fundamental question"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Adaptive Layer":2.0,"FlexiDepth":2.0,"arXiv250323798v3 Announce Type":1.0,"Abstract":1.0,"Various layer-skipping methods":1.0,"token generation":1.0,"large language models":1.0,"LLMs":1.0,"limited attention":1.0,"a fundamental question":1.0}},"age_hours":2.7801406833333333,"is_recent":true,"quality_score":1.0,"sentiment_score":6.1315,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2263,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9052,"joy":0.0147,"surprise":0.0612,"sadness":0.0035,"fear":0.0033,"anger":0.0087,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents FlexiDepth, a method to dynamically adjust the number of Transformer layers used in text generation, skipping 8 out of 32 layers in Llama-3-8B while maintaining benchmark performance. While it shows computational savings, it does not yet achieve wall-clock speedup due to I/O overhead. The open-sourced code and dataset documenting layer allocation patterns are positive steps, but it's still in the applied research phase with no deployed units.","key_impact_metrics":["8 layers skipped","Maintained benchmark performance"],"technology_tags":["Large Language Models","Transformer Architecture","Adaptive Computation"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T20:55:16.333464Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a93f9ff4b7d9","title":"$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi","content":"arXiv:2504.00218v2 Announce Type: replace Abstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.00218","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.432077","language":"en","tags":["csma","computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":177,"author":"Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Fleming, Tianlong Chen","raw_content_length":1518,"priority":7,"update_frequency":1,"reading_time_minutes":0.885,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":["\\textit{permutation"]},"char_count":1517,"language_detected":"en","key_concepts":{"key_phrases":["textitAgents","Breaking Pragmatic Multi","arXiv250400218v2 Announce Type","Abstract","Most discussions","Large Language Model LLM safety","single-agent settings","multi-agent LLM systems","novel adversarial risks","their behavior"],"filter_categories":{"ai_ml":["Large Language Model LLM safety"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"textitAgents":2.0,"Breaking Pragmatic Multi":2.0,"arXiv250400218v2 Announce Type":1.0,"Abstract":1.0,"Most discussions":1.0,"Large Language Model LLM safety":1.0,"single-agent settings":1.0,"multi-agent LLM systems":1.0,"novel adversarial risks":1.0,"their behavior":1.0}},"age_hours":2.780155241666667,"is_recent":true,"quality_score":1.0,"sentiment_score":1.9705,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6059,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8038,"joy":0.0205,"surprise":0.0458,"sadness":0.0085,"fear":0.0643,"anger":0.0505,"disgust":0.0065},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper focuses on adversarial attacks on multi-agent LLM systems, exposing vulnerabilities in safety mechanisms. While it doesn't directly address climate change, it highlights the importance of robust AI safety, which is indirectly relevant as AI is increasingly used in climate modeling and mitigation strategies. The research quantifies attack success rates, providing measurable outcomes.","key_impact_metrics":["Attack success rate increased by up to 7x"],"technology_tags":["Large Language Models","AI Safety","Adversarial Attacks"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-28T20:55:19.264432Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_15992f4982c9","title":"Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Cultural Intelligence with CQ","content":"arXiv:2504.01127v2 Announce Type: replace Abstract: Cultural Intelligence (CQ) refers to the ability to understand unfamiliar cultural contexts, a crucial skill for large language models (LLMs) to effectively engage with globally diverse users. Existing studies often focus on explicitly stated cultural norms, but fail to capture the subtle, implicit values that are common in daily conversation. To address this gap, we introduce CQBench, a benchmark specifically designed to assess LLMs' capability to infer implicit cultural values from natural conversational contexts. CQBench consists of multi character conversation based stories using values from the World Value Survey and the GlobalOpinions, with topics including ethical, religious, social, etc. Our automatic dataset construction pipeline integrates rigorous validation procedures (incorporation, consistency, and implicitness checks), achieving a 94.5% human model agreement in the final validation. To leverage CQBench data, we design three tasks of increasing complexity: attitude detection, value selection, and value extraction. These tasks evaluate whether models can detect attitude and recognize values embedded within natural dialogues rather than relying on explicit cultural knowledge. We find that while frontier models like o1 reach human level performance in value selection (0.809 F1), they still fall short in nuanced attitude detection (0.622 F1). Notably, finetuning a smaller LLaMA-3.2-3B on only 500 culturally rich examples improves performance by over 10%, even outperforming o3-mini in some cases. Using CQ-Bench, we provide insights into the current challenges in LLMs' CQ research and suggest practical pathways for enhancing LLMs' cross-cultural reasoning abilities.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.01127","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.432503","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":240,"author":"Ziyi Liu, Priyanka Dey, Jen-tse Huang, Zhenyu Zhao, Bowen Jiang, Rahul Gupta, Yang Liu, Yao Du, Jieyu Zhao","raw_content_length":1755,"priority":7,"update_frequency":1,"reading_time_minutes":1.2,"robust_parsing_used":true,"entities":{"organizations":["GlobalOpinions","CQBench","the World Value Survey"],"persons":["arXiv:2504.01127v2 Announce Type:"],"locations":[],"monetary":[]},"char_count":1754,"language_detected":"en","key_concepts":{"key_phrases":["Can LLMs Grasp Implicit Cultural Values","Benchmarking LLMs Cultural Intelligence","arXiv250401127v2 Announce Type","Abstract","Cultural Intelligence CQ","the ability","unfamiliar cultural contexts","a crucial skill","large language models","LLMs"],"filter_categories":{"ai_ml":["Can LLMs Grasp Implicit Cultural Values","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Can LLMs Grasp Implicit Cultural Values":2.0,"Benchmarking LLMs Cultural Intelligence":2.0,"arXiv250401127v2 Announce Type":1.0,"Abstract":1.0,"Cultural Intelligence CQ":1.0,"the ability":1.0,"unfamiliar cultural contexts":1.0,"a crucial skill":1.0,"large language models":1.0,"LLMs":1.0}},"age_hours":2.780169933611111,"is_recent":true,"quality_score":1.0,"sentiment_score":8.6135,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7227,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9301,"joy":0.0025,"surprise":0.0233,"sadness":0.0078,"fear":0.0124,"anger":0.0118,"disgust":0.0121},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":5,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving LLMs' cultural intelligence, which could indirectly support sustainability efforts by enabling more effective communication and collaboration across diverse groups. However, it's currently in the applied research phase with no deployed technology or measurable environmental outcomes. The technical credibility is relatively high due to the rigorous validation procedures and the use of established datasets.","key_impact_metrics":["F1 score in value selection: 0.809","F1 score in attitude detection: 0.622"],"technology_tags":["Large Language Models","Cultural Intelligence","Natural Language Processing"],"sdg_alignment":[4,16,17],"analyzed_at":"2025-10-28T20:55:22.462942Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_90a6d6b95e48","title":"PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization","content":"arXiv:2504.01444v4 Announce Type: replace Abstract: Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.01444","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.433261","language":"en","tags":["computer-science","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":164,"author":"Aofan Liu, Lulu Tang, Ting Pan, Yuguo Yin, Bin Wang, Ao Yang","raw_content_length":1239,"priority":7,"update_frequency":1,"reading_time_minutes":0.82,"robust_parsing_used":true,"entities":{"organizations":["Pictorial Code Contextualization arXiv:2504.01444v4 Announce Type:","PiCo","Large Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1238,"language_detected":"en","key_concepts":{"key_phrases":["PiCo","Multimodal Large Language Models","Pictorial Code Contextualization","arXiv250401444v4 Announce Type","Abstract","MLLMs","which","vision","other modalities","Large Language Models"],"filter_categories":{"ai_ml":["Multimodal Large Language Models","vision","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"PiCo":3.0,"Multimodal Large Language Models":3.0,"Pictorial Code Contextualization":2.0,"arXiv250401444v4 Announce Type":1.0,"Abstract":1.0,"MLLMs":1.0,"which":1.0,"vision":1.0,"other modalities":1.0,"Large Language Models":1.0}},"age_hours":2.7801980627777776,"is_recent":true,"quality_score":1.0,"sentiment_score":4.8709999999999996,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.852,"joy":0.0168,"surprise":0.0613,"sadness":0.01,"fear":0.029,"anger":0.0211,"disgust":0.0097},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel jailbreaking framework (PiCo) for multimodal large language models. While the research is technically credible and innovative, it focuses on security vulnerabilities rather than direct sustainability actions. The concrete action is the development and testing of the PiCo framework, and the evidence is the reported Attack Success Rate (ASR) on Gemini-Pro Vision and GPT-4. It's at the basic research stage.","key_impact_metrics":["Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision","Attack Success Rate (ASR) of 52.66% on GPT-4"],"technology_tags":["Multimodal Large Language Models","AI Security","Jailbreaking"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:55:25.412424Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2e375af125cf","title":"Identifying and Evaluating Inactive Heads in Pretrained LLMs","content":"arXiv:2504.03889v3 Announce Type: replace Abstract: Attention is foundational to large language models (LLMs), enabling different heads to have diverse focus on relevant input tokens. However, learned behaviors like attention sinks, where the first token receives the most attention despite limited semantic importance, suggest some heads may be inactive, and point to a significant source of computational redundancy. To analyze this phenomenon, we propose a taxonomy of 13 score functions that measure different ways a head can be inactive. Thresholding these scores allows us to analyze different sets of potentially inactive attention heads. We evaluate whether identified heads are inactive through model interventions, finding that more than 12% of attention heads are inactive on average, and can be ablated in specific contexts while maintaining MMLU accuracy to within 1% of the pretrained LLM. Across 3 model families, our score functions that measure the average norm of a head's output consistently identify inactive heads that would not have been found by score functions that rely solely on attention weights. We establish that relying on a score function that measures a first token attention sink would underestimate the prevalence of inactive heads, failing to identify more than 7% of inactive heads on average. We also show how measuring score distributions can provide insights into attention behavior. For instance, we find evidence that finetuning causes little to no change in attention behavior, and that even within the same model family, large model scales present markedly different attention behaviors.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.03889","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.434144","language":"en","tags":["research","preprints","computer-science","cslg","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":244,"author":"Pedro Sandoval-Segura, Xijun Wang, Ashwinee Panda, Micah Goldblum, Ronen Basri, Tom Goldstein, David Jacobs","raw_content_length":1631,"priority":7,"update_frequency":1,"reading_time_minutes":1.22,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1630,"language_detected":"en","key_concepts":{"key_phrases":["Evaluating","Inactive Heads","Pretrained LLMs","arXiv250403889v3 Announce Type","Abstract","Attention","large language models","LLMs","different heads","diverse focus"],"filter_categories":{"ai_ml":["Pretrained LLMs","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Evaluating":2.0,"Inactive Heads":2.0,"Pretrained LLMs":2.0,"arXiv250403889v3 Announce Type":1.0,"Abstract":1.0,"Attention":1.0,"large language models":1.0,"LLMs":1.0,"different heads":1.0,"diverse focus":1.0}},"age_hours":2.7802267252777777,"is_recent":true,"quality_score":1.0,"sentiment_score":7.4005,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4801,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8789,"joy":0.0033,"surprise":0.0132,"sadness":0.0072,"fear":0.034,"anger":0.025,"disgust":0.0384},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research identifies and evaluates inactive heads in pretrained LLMs, finding that more than 12% of attention heads are inactive and can be ablated while maintaining MMLU accuracy to within 1%. This could lead to reduced computational redundancy and energy consumption in LLMs. However, it is still in the research phase and has not been deployed.","key_impact_metrics":["12% inactive attention heads","1% MMLU accuracy maintained"],"technology_tags":["large language models","attention mechanisms","model optimization"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-28T20:55:28.281143Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ea06bb422050","title":"Utility","content":"arXiv:2504.05220v5 Announce Type: replace Abstract: This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.05220","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.434944","language":"en","tags":["computer-science","preprints","csai","csir","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":165,"author":"Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng","raw_content_length":1164,"priority":7,"update_frequency":1,"reading_time_minutes":0.825,"robust_parsing_used":true,"entities":{"organizations":["LLM","MS MARCO QA","BEIR","MS MARCO","RAG"],"persons":["RAG","arXiv:2504.05220v5 Announce Type"],"locations":[],"monetary":[]},"char_count":1163,"language_detected":"en","key_concepts":{"key_phrases":["Utility","LLMs","document utility","Announce Type","Abstract","This paper","the use","large language models","training retrieval","retrieval-augmented generation RAG systems"],"filter_categories":{"ai_ml":["LLMs","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Utility":2.0,"LLMs":2.0,"document utility":2.0,"Announce Type":1.0,"Abstract":1.0,"This paper":1.0,"the use":1.0,"large language models":1.0,"training retrieval":1.0,"retrieval-augmented generation RAG systems":1.0}},"age_hours":2.7802553541666666,"is_recent":true,"quality_score":1.0,"sentiment_score":8.634500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7269,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9169,"joy":0.0184,"surprise":0.036,"sadness":0.0048,"fear":0.0037,"anger":0.0124,"disgust":0.0079},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper explores using LLMs to improve retrieval and RAG systems, potentially reducing reliance on human annotation. The concrete action is the annotation of document utility using the Qwen-2.5-32B model on the MS MARCO dataset. Results show improved out-of-domain retrieval and RAG outcomes compared to human annotations, but it's still in the research phase with no deployed technology.","key_impact_metrics":["20% human labels achieves comparable performance","Improved out-of-domain retrieval performance"],"technology_tags":["Large Language Models","Retrieval-Augmented Generation"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:55:31.885371Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_553c97101e6a","title":"SQL","content":"arXiv:2504.08600v5 Announce Type: replace Abstract: Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the reasoning performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning~(SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments~(e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning~(RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start and synthetic data on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6\\% and 67.1\\% on the benchmark Spider and BIRD, respectively. The code is available at https://github.com/IDEA-FinAI/SQL-R1 .","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.08600","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.435330","language":"en","tags":["csdb","research","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":181,"author":"Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, Jian Guo","raw_content_length":1389,"priority":7,"update_frequency":1,"reading_time_minutes":0.905,"robust_parsing_used":true,"entities":{"organizations":["SQL-R1","SQL"],"persons":[],"locations":["NL2SQL"],"monetary":[]},"char_count":1388,"language_detected":"en","key_concepts":{"key_phrases":["SQL","arXiv250408600v5 Announce Type","Abstract","Natural Language","NL2SQL","intuitive interactions","databases","natural language queries","structured SQL statements","recent advancements"],"filter_categories":{"ai_ml":["Natural Language"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"SQL":3.0,"arXiv250408600v5 Announce Type":1.0,"Abstract":1.0,"Natural Language":1.0,"NL2SQL":1.0,"intuitive interactions":1.0,"databases":1.0,"natural language queries":1.0,"structured SQL statements":1.0,"recent advancements":1.0}},"age_hours":2.780268544444444,"is_recent":true,"quality_score":0.7,"sentiment_score":8.9225,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7845,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9146,"joy":0.0124,"surprise":0.0518,"sadness":0.0034,"fear":0.0075,"anger":0.0068,"disgust":0.0035},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel NL2SQL reasoning model (SQL-R1) trained by reinforcement learning to improve database interaction. While the technology shows promise with reported execution accuracy of 88.6% on Spider and 67.1% on BIRD benchmarks, it is currently in the applied research stage with no evidence of real-world deployment or impact on energy consumption or emissions. The vaporware flag is set because it's a prototype with no deployed units or customer contracts.","key_impact_metrics":["execution accuracy on Spider 88.6%","execution accuracy on BIRD 67.1%"],"technology_tags":["Natural Language Processing","Reinforcement Learning","Database Management"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:55:35.217900Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_836668f71f7d","title":"Science Hierarchography: Hierarchical Organization of Science Literature","content":"arXiv:2504.13834v4 Announce Type: replace Abstract: Scientific knowledge is growing rapidly, making it difficult to track progress and high-level conceptual links across broad disciplines. While tools like citation networks and search engines help retrieve related papers, they lack the abstraction needed to capture the needed to represent the density and structure of activity across subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that spans multiple levels of abstraction -- from broad domains to specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve this goal, we develop a hybrid approach that combines efficient embedding-based clustering with LLM-based prompting, striking a balance between scalability and semantic precision. Compared to LLM-heavy methods like iterative tree construction, our approach achieves superior quality-speed trade-offs. Our hierarchies capture different dimensions of research contributions, reflecting the interdisciplinary and multifaceted nature of modern science. We evaluate its utility by measuring how effectively an LLM-based agent can navigate the hierarchy to locate target papers. Results show that our method improves interpretability and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo are available: https://github.com/JHU-CLSP/science-hierarchography","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.13834","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.436150","language":"en","tags":["research","preprints","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":199,"author":"Muhan Gao, Jash Shah, Weiqi Wang, Daniel Khashabi","raw_content_length":1549,"priority":7,"update_frequency":1,"reading_time_minutes":0.995,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1546,"language_detected":"en","key_concepts":{"key_phrases":["Science Hierarchography","Hierarchical Organization","Science Literature","arXiv250413834v4 Announce Type","Abstract","Scientific knowledge","progress","high-level conceptual links","broad disciplines","tools"],"filter_categories":{"research_academic":["Science Hierarchography"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Science Hierarchography":2.0,"Hierarchical Organization":2.0,"Science Literature":2.0,"arXiv250413834v4 Announce Type":1.0,"Abstract":1.0,"Scientific knowledge":1.0,"progress":1.0,"high-level conceptual links":1.0,"broad disciplines":1.0,"tools":1.0}},"age_hours":2.780296685277778,"is_recent":true,"quality_score":1.0,"sentiment_score":8.7895,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7579,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7289,"joy":0.0085,"surprise":0.0145,"sadness":0.0111,"fear":0.1229,"anger":0.0871,"disgust":0.027},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on organizing scientific literature to identify well-explored and under-explored fields. While it doesn't directly reduce GHG emissions, it could indirectly accelerate climate research by improving access to relevant information. The method is evaluated by measuring how effectively an LLM-based agent can navigate the hierarchy to locate target papers, but there are no concrete deployments or economic viability data.","key_impact_metrics":["Quality-speed trade-offs","Effectiveness of LLM agent in locating papers"],"technology_tags":["Machine Learning","Natural Language Processing","Knowledge Organization"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:55:38.621906Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_99fb6042419f","title":"Fast Online Adaptive Neural MPC via Meta","content":"arXiv:2504.16369v5 Announce Type: replace Abstract: Data-driven model predictive control (MPC) has demonstrated significant potential for improving robot control performance in the presence of model uncertainties. However, existing approaches often require extensive offline data collection and computationally intensive training, limiting their ability to adapt online. To address these challenges, this paper presents a fast online adaptive MPC framework that leverages neural networks integrated with Model-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot adaptation of residual dynamics - capturing the discrepancy between nominal and true system behavior - using minimal online data and gradient steps. By embedding these meta-learned residual models into a computationally efficient L4CasADi-based MPC pipeline, the proposed method enables rapid model correction, enhances predictive accuracy, and improves real-time control performance. We validate the framework through simulation studies on a Van der Pol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show significant gains in adaptation speed and prediction accuracy over both nominal MPC and nominal MPC augmented with a freshly initialized neural network, underscoring the effectiveness of our approach for real-time adaptive robot control.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.16369","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.436575","language":"en","tags":["csro","cssy","computer-science","eesssy","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":176,"author":"Yu Mei, Xinyu Zhou, Shuyang Yu, Vaibhav Srivastava, Xiaobo Tan","raw_content_length":1332,"priority":7,"update_frequency":1,"reading_time_minutes":0.88,"robust_parsing_used":true,"entities":{"organizations":["Meta arXiv:2504.16369v5 Announce Type: replace Abstract","MPC","L4CasADi","Model-Agnostic Meta-Learning"],"persons":[],"locations":[],"monetary":[]},"char_count":1331,"language_detected":"en","key_concepts":{"key_phrases":["Fast Online Adaptive Neural MPC","Meta","arXiv250416369v5 Announce Type","Abstract","Data-driven model predictive control","MPC","significant potential","robot control performance","the presence","model uncertainties"],"filter_categories":{"ai_ml":["model uncertainties"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Fast Online Adaptive Neural MPC":2.0,"Meta":2.0,"arXiv250416369v5 Announce Type":1.0,"Abstract":1.0,"Data-driven model predictive control":1.0,"MPC":1.0,"significant potential":1.0,"robot control performance":1.0,"the presence":1.0,"model uncertainties":1.0}},"age_hours":2.7803110949999996,"is_recent":true,"quality_score":1.0,"sentiment_score":7.553000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5106,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9091,"joy":0.016,"surprise":0.0275,"sadness":0.0063,"fear":0.0267,"anger":0.0111,"disgust":0.0033},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to robot control using meta-learning for faster adaptation. While the simulation results show improved prediction accuracy and adaptation speed, it's still in the simulation stage with no real-world deployment or economic viability demonstrated. The potential climate impact is indirect, as improved robot control could lead to more efficient resource utilization in various sectors.","key_impact_metrics":["adaptation speed gain","prediction accuracy gain"],"technology_tags":["model predictive control","neural networks","meta-learning","robot control"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:55:41.390548Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_038b1d845e3e","title":"Realization of Temporally Connected Graphs Based on Degree Sequences","content":"arXiv:2504.17743v2 Announce Type: replace Abstract: Given an undirected graph $G$, the problem of deciding whether $G$ admits a simple and proper time-labeling that makes it temporally connected is known to be NP-hard (G\\\"obel et al., 1991). In this article, we relax this problem and ask whether a given degree sequence can be realized as a temporally connected graph. Our main results are a complete characterization of the feasible cases, and a recognition algorithm that runs in $O(n)$ time for graphical degree sequences (realized as simple temporal graphs) and in $O(n+m)$ time for multigraphical degree sequences (realized as non-simple temporal graphs, where the number of time labels on an edge corresponds to the multiplicity of the edge in the multigraph). In fact, these algorithms can be made constructive at essentially no cost. Namely, we give a constructive $O(n+m)$ time algorithm that outputs, for a given (multi)graphical degree sequence $\\mathbf{d}$, a temporally connected graph whose underlying (multi)graph is a realization of $\\mathbf{d}$, if one exists.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.17743","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.437340","language":"en","tags":["research","csds","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Arnaud Casteigts, Michelle D\\\"oring, Nils Morawietz","raw_content_length":1079,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":["Degree Sequences"],"persons":["Announce Type"],"locations":[],"monetary":["O(n)$","O(n+m)$"]},"char_count":1078,"language_detected":"en","key_concepts":{"key_phrases":["Realization","Temporally Connected Graphs","Degree Sequences","arXiv250417743v2","Announce Type","Abstract","an undirected graph","the problem","a simple and proper time-labeling","this article"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Realization":2.0,"Temporally Connected Graphs":2.0,"Degree Sequences":2.0,"arXiv250417743v2":1.0,"Announce Type":1.0,"Abstract":1.0,"an undirected graph":1.0,"the problem":1.0,"a simple and proper time-labeling":1.0,"this article":1.0}},"age_hours":2.7803435605555555,"is_recent":true,"quality_score":1.0,"sentiment_score":4.08,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.184,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8445,"joy":0.0249,"surprise":0.0156,"sadness":0.0055,"fear":0.048,"anger":0.0397,"disgust":0.0218},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents a novel algorithm for realizing temporally connected graphs from degree sequences. While the algorithm itself doesn't directly impact climate change, it could potentially be used in modeling and optimizing complex systems, including energy grids or supply chains, which could indirectly contribute to sustainability. The research is based on mathematical proofs and algorithms, indicating strong technical credibility, but it is at a very early stage of development with no clear path to economic viability or deployment.","key_impact_metrics":[],"technology_tags":["graph theory","algorithms","network analysis"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:55:44.797648Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e12fb0e96f22","title":"Evaluating Evaluation Metrics -","content":"arXiv:2504.18114v2 Announce Type: replace Abstract: Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.18114","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.437871","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":158,"author":"Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu","raw_content_length":1177,"priority":7,"update_frequency":1,"reading_time_minutes":0.79,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["GPT-4"],"locations":[],"monetary":[]},"char_count":1176,"language_detected":"en","key_concepts":{"key_phrases":["Evaluating Evaluation Metrics","arXiv250418114v2","Announce Type","Abstract","Hallucinations","a significant obstacle","the reliability","widespread adoption","language models","their accurate measurement"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Evaluating Evaluation Metrics":2.0,"arXiv250418114v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Hallucinations":1.0,"a significant obstacle":1.0,"the reliability":1.0,"widespread adoption":1.0,"language models":1.0,"their accurate measurement":1.0}},"age_hours":2.7803578925,"is_recent":true,"quality_score":1.0,"sentiment_score":6.806,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3612,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8981,"joy":0.0145,"surprise":0.0301,"sadness":0.0062,"fear":0.0288,"anger":0.0127,"disgust":0.0095},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper evaluates metrics for detecting hallucinations in language models. While it doesn't directly address climate change, improving the reliability of information could indirectly support better decision-making in sustainability. The research is in the applied research stage, evaluating existing metrics across various models and datasets, but lacks real-world deployment.","key_impact_metrics":["Alignment with human judgments","Inconsistent gains with parameter scaling"],"technology_tags":["Language Models","Hallucination Detection","Evaluation Metrics"],"sdg_alignment":[9,17],"analyzed_at":"2025-10-28T20:55:47.942221Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_134a6f45be79","title":"Efficient and Adaptable Overlapping for Computation and Communication via Signaling and Reordering","content":"arXiv:2504.19519v2 Announce Type: replace Abstract: Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency becomes an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, which utilizes a novel signaling mechanism: when part of the output finishes, the computation kernel sends a signal to trigger the communication of that part, while continuing the computation of the remaining part (interference-free computation). Consequently, the communication of the finished part and the computation of the remaining part can be overlapped. On top of the signaling mechanism, FlashOverlap comprises two key components: (1) the determination of the signaling timing to boost the overlap efficiency (tile-wise overlapping), and (2) a pre-communication reordering to create the contiguous address for finished data, enabling communication by simply calling NCCL APIs (communication agnosticism), and a post-communication reordering to correct the data order. Experiments show that FlashOverlap achieves up to 1.65x speedup through overlap, outperforming existing works in most cases. Code is available at https://github.com/infinigence/FlashOverlap.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2504.19519","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.438294","language":"en","tags":["csdc","computer-science","cslg","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":242,"author":"Ke Hong, Xiuhong Li, Minxu Liu, Qiuli Mao, Tianqi Wu, Zixiao Huang, Lufang Chen, Zhong Wang, Yichong Zhang, Zhenhua Zhu, Guohao Dai, Yu Wang","raw_content_length":1902,"priority":7,"update_frequency":1,"reading_time_minutes":1.21,"robust_parsing_used":true,"entities":{"organizations":["Inter-GPU","Signaling and Reordering arXiv:2504.19519v2"],"persons":[],"locations":[],"monetary":[]},"char_count":1901,"language_detected":"en","key_concepts":{"key_phrases":["Efficient and Adaptable Overlapping","Computation","Communication","Signaling","Reordering","Announce Type","Generative models","remarkable success","various applications","the demand"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Efficient and Adaptable Overlapping":2.0,"Computation":2.0,"Communication":2.0,"Signaling":2.0,"Reordering":2.0,"Announce Type":1.0,"Generative models":1.0,"remarkable success":1.0,"various applications":1.0,"the demand":1.0}},"age_hours":2.780371629722222,"is_recent":true,"quality_score":1.0,"sentiment_score":9.3445,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8689,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8034,"joy":0.0458,"surprise":0.0905,"sadness":0.0064,"fear":0.0143,"anger":0.0291,"disgust":0.0104},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel technique (FlashOverlap) to improve the efficiency of multi-GPU computing, which can indirectly reduce energy consumption by speeding up computations. The 1.65x speedup is a concrete metric, but the technology is still in the applied research stage with no deployment data. The open-source code increases transparency and potential for adoption.","key_impact_metrics":["1.65x speedup"],"technology_tags":["Multi-GPU computing","Parallel processing","Communication optimization"],"sdg_alignment":[7,9],"analyzed_at":"2025-10-28T20:55:51.298289Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ca73e8a5d452","title":"Position Paper: Towards Open Complex Human","content":"arXiv:2505.00018v2 Announce Type: replace Abstract: We propose a technology-agnostic, collaboration-ready stance for Human-AI Agents Collaboration Systems (HAACS) that closes long-standing gaps in prior stages (automation; flexible autonomy; agentic multi-agent collectives). Reading empirical patterns through a seven-dimension collaboration spine and human-agent contrasts, we identify missing pieces: principled budgeting of initiative, instantaneous and auditable reconfiguration, a system-wide knowledge backbone with an epistemic promotion gate, capacity-aware human interfaces; and, as a prerequisite to all of the above, unified definitions of agent and formal collaborative dynamics. We respond with (i) a boundary-centric ontology of agenthood synthesized with cybernetics; (ii) a Petri net family (colored and interpreted) that models ownership, cross-boundary interaction, concurrency, guards, and rates with collaboration transitions; and (iii) a three-level orchestration (meta, agent, execution) that governs behavior families via guard flips. On the knowledge side, we ground collaborative learning in Conversation Theory and SECI with teach-back gates and an evolving backbone; on the problem-solving side, we coordinate routine MEA-style control with practice-guided open-ended discovery. The result is the Hierarchical Exploration-Exploitation Net (HE2-Net): a policy-controlled stance that splits provisional from validated assets, promotes only after tests and peer checks, and budgets concurrent probing while keeping reuse fast and safe. We show interoperability with emerging agent protocols without ad hoc glue and sketch bio-cybernetic extensions (autopoiesis, autogenesis, evolving boundaries, synergetics, etc). Altogether, the framework keeps humans central to setting aims, justifying knowledge, and steering theory-practice dynamics, while scaling agents as reliable collaborators within audited governance.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.00018","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.438746","language":"en","tags":["csma","computer-science","csai","preprints","cshc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":241,"author":"Ju Wu, Calvin K. L. Or","raw_content_length":1939,"priority":7,"update_frequency":1,"reading_time_minutes":1.205,"robust_parsing_used":true,"entities":{"organizations":["Human-AI Agents Collaboration Systems"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1938,"language_detected":"en","key_concepts":{"key_phrases":["Position Paper","Towards","Open Complex Human","arXiv250500018v2 Announce Type","Abstract","a technology-agnostic collaboration-ready stance","Human-AI Agents Collaboration Systems","HAACS","long-standing gaps","prior stages"],"filter_categories":{"ai_ml":["Human-AI Agents Collaboration Systems"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Position Paper":2.0,"Towards":2.0,"Open Complex Human":2.0,"arXiv250500018v2 Announce Type":1.0,"Abstract":1.0,"a technology-agnostic collaboration-ready stance":1.0,"Human-AI Agents Collaboration Systems":1.0,"HAACS":1.0,"long-standing gaps":1.0,"prior stages":1.0}},"age_hours":2.7803868422222218,"is_recent":true,"quality_score":1.0,"sentiment_score":4.614,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0772,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7592,"joy":0.0102,"surprise":0.0101,"sadness":0.0081,"fear":0.1383,"anger":0.0435,"disgust":0.0305},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a framework for Human-AI collaboration, focusing on theoretical models and ontologies. There are no deployed technologies or measured outcomes related to sustainability. The impact is currently theoretical and at the basic research stage, with no clear path to economic viability or deployment readiness.","key_impact_metrics":[],"technology_tags":["Human-AI Collaboration","Agent-based Systems","Cybernetics"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:56:01.041292Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_921c250a3ebf","title":"Phantora: Maximizing Code Reuse in Simulation","content":"arXiv:2505.01616v3 Announce Type: replace Abstract: Modern machine learning (ML) training workloads place substantial demands on both computational and communication resources. Consequently, accurate performance estimation has become increasingly critical for guiding system design decisions, such as the selection of parallelization strategies, cluster configurations, and hardware provisioning. Existing simulation-based performance estimation requires reimplementing the ML framework in a simulator, which demands significant manual effort and is hard to maintain as ML frameworks evolve rapidly. This paper introduces Phantora, a hybrid GPU cluster simulator designed for performance estimation of ML training workloads. Phantora executes unmodified ML frameworks as is within a distributed, containerized environment. Each container emulates the behavior of a GPU server in a large-scale cluster, while Phantora intercepts and simulates GPU- and communication-related operations to provide high-fidelity performance estimation. We call this approach hybrid simulation of ML systems, in contrast to traditional methods that simulate static workloads. The primary advantage of hybrid simulation is that it allows direct reuse of ML framework source code in simulation, avoiding the need for reimplementation. Our evaluation shows that Phantora provides accuracy comparable to static workload simulation while supporting three state-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora operates on a single GPU, eliminating the need for the resource-intensive trace collection and workload extraction steps required by traditional trace-based simulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.01616","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.440565","language":"en","tags":["csdc","computer-science","cslg","preprints","cspf","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":220,"author":"Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Tianjun Yuan, Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang Zhuo","raw_content_length":1743,"priority":7,"update_frequency":1,"reading_time_minutes":1.1,"robust_parsing_used":true,"entities":{"organizations":["GPU"],"persons":["Phantora"],"locations":[],"monetary":[]},"char_count":1740,"language_detected":"en","key_concepts":{"key_phrases":["Phantora","Maximizing Code Reuse","Simulation","arXiv250501616v3 Announce Type","Modern machine learning","ML training","substantial demands","both computational and communication resources","accurate performance estimation","system design decisions"],"filter_categories":{"ai_ml":["Modern machine learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Phantora":2.0,"Maximizing Code Reuse":2.0,"Simulation":2.0,"arXiv250501616v3 Announce Type":1.0,"Modern machine learning":1.0,"ML training":1.0,"substantial demands":1.0,"both computational and communication resources":1.0,"accurate performance estimation":1.0,"system design decisions":1.0}},"age_hours":2.780400431666667,"is_recent":true,"quality_score":1.0,"sentiment_score":4.36,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.128,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9225,"joy":0.0106,"surprise":0.0315,"sadness":0.0042,"fear":0.0109,"anger":0.0151,"disgust":0.0052},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"Phantora allows for more efficient design of ML training workloads, potentially reducing energy consumption by optimizing resource allocation. The article mentions accuracy comparable to static workload simulation, which is a measurable outcome. It is currently in the prototype/pilot stage, operating on a single GPU.","key_impact_metrics":["Accuracy comparable to static workload simulation","Operates on a single GPU"],"technology_tags":["GPU cluster simulation","Machine learning training optimization"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-28T20:56:03.683045Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_cc1e79ef8410","title":"Estimating Dynamic Soft Continuum Robot States From Boundaries","content":"arXiv:2505.04491v2 Announce Type: replace Abstract: State estimation is one of the fundamental problems in robotics. For soft continuum robots, this task is particularly challenging because their states (poses, strains, internal wrenches, and velocities) are inherently infinite-dimensional functions due to their continuous deformability. Traditional sensing techniques, however, can only provide discrete measurements. Recently, a dynamic state estimation method known as a \\textit{boundary observer} was introduced, which uses Cosserat rod theory to recover all infinite-dimensional states by measuring only the tip velocity. In this work, we present a dual design that instead relies on measuring the internal wrench at the robot's base. Despite the duality, this new approach offers a key practical advantage: it requires only a force/torque (FT) sensor embedded at the base and eliminates the need for external motion capture systems. Both observer types are inspired by principles of energy dissipation and can be naturally combined to enhance performance. We conduct a Lyapunov-based analysis to study the convergence rate of these boundary observers and reveal a useful property: as the observer gains increase, the convergence rate initially improves and then degrades. This convex trend enables efficient tuning of the observer gains. We also identify special cases where linear and angular states are fully determined by each other, which further relaxes sensing requirements. Experimental studies using a tendon-driven continuum robot validate the convergence of all observer variants under fast dynamic motions, the existence of optimal gains, robustness against unknown external forces, and the algorithm's real-time computational performance.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.04491","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.441418","language":"en","tags":["research","csro","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":246,"author":"Tongjia Zheng, Jessica Burgner-Kahrs","raw_content_length":1759,"priority":7,"update_frequency":1,"reading_time_minutes":1.23,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Continuum Robot States"],"locations":[],"monetary":[]},"char_count":1758,"language_detected":"en","key_concepts":{"key_phrases":["Dynamic Soft Continuum Robot States","Boundaries","arXiv250504491v2 Announce Type","Abstract","State estimation","the fundamental problems","robotics","soft continuum robots","this task","their states"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Dynamic Soft Continuum Robot States":2.0,"Boundaries":2.0,"arXiv250504491v2 Announce Type":1.0,"Abstract":1.0,"State estimation":1.0,"the fundamental problems":1.0,"robotics":1.0,"soft continuum robots":1.0,"this task":1.0,"their states":1.0}},"age_hours":2.7804302127777776,"is_recent":true,"quality_score":1.0,"sentiment_score":6.4719999999999995,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2944,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6767,"joy":0.0214,"surprise":0.1732,"sadness":0.011,"fear":0.0848,"anger":0.0248,"disgust":0.0082},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":4,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new method for state estimation in soft continuum robots, validated through experiments. While it doesn't directly address climate change, improved robot control could lead to more efficient manufacturing or resource extraction, but this is theoretical. The technical credibility is high due to Lyapunov-based analysis and experimental validation, but deployment readiness is still at the pilot stage.","key_impact_metrics":["Convergence rate of boundary observers","Real-time computational performance"],"technology_tags":["Soft Continuum Robots","State Estimation","Boundary Observer"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:56:06.514776Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e1e9e172e6fe","title":"Understanding In","content":"arXiv:2505.05145v3 Announce Type: replace Abstract: To perform few-shot learning, language models extract signals from a few input-label pairs, aggregate these into a learned prediction rule, and apply this rule to new inputs. How is this implemented in the forward pass of modern transformer models? To explore this question, we study a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We introduce a novel optimization method that localizes the model's few-shot ability to only a few attention heads. We then perform an in-depth analysis of individual heads, via dimensionality reduction and decomposition. As an example, on Llama-3-8B-instruct, we reduce its mechanism on our tasks to just three attention heads with six-dimensional subspaces, where four dimensions track the unit digit with trigonometric functions at periods $2$, $5$, and $10$, and two dimensions track magnitude with low-frequency components. To deepen our understanding of the mechanism, we also derive a mathematical identity relating ``aggregation'' and ``extraction'' subspaces for attention heads, allowing us to track the flow of information from individual examples to a final aggregated concept. Using this, we identify a self-correction mechanism where mistakes learned from earlier demonstrations are suppressed by later demonstrations. Our results demonstrate how tracking low-dimensional subspaces of localized heads across a forward pass can provide insight into fine-grained computational structures in language models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.05145","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.442247","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":227,"author":"Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen","raw_content_length":1577,"priority":7,"update_frequency":1,"reading_time_minutes":1.135,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":["$10$","periods $2$, $5$"]},"char_count":1576,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250505145v3 Announce Type","Abstract","few-shot learning","language models","signals","a few input-label pairs","these","a learned prediction rule","this rule","new inputs"],"filter_categories":{"ai_ml":["a few input-label pairs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250505145v3 Announce Type":1.0,"Abstract":1.0,"few-shot learning":1.0,"language models":1.0,"signals":1.0,"a few input-label pairs":1.0,"these":1.0,"a learned prediction rule":1.0,"this rule":1.0,"new inputs":1.0}},"age_hours":2.7804614391666664,"is_recent":true,"quality_score":1.0,"sentiment_score":8.8585,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7717,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8301,"joy":0.016,"surprise":0.1264,"sadness":0.0044,"fear":0.0074,"anger":0.0122,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents research on understanding how language models perform few-shot learning. While the research is technically sound and published on arXiv, it is in the very early stages and doesn't directly translate to concrete climate impact or economic viability. The potential systemic impact is limited as it is fundamental research.","key_impact_metrics":[],"technology_tags":["language models","few-shot learning","transformer models","attention heads"],"sdg_alignment":[],"analyzed_at":"2025-10-28T20:56:09.472611Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d6d6e79777db","title":"Hakim: Farsi Text Embedding Model","content":"arXiv:2505.08435v3 Announce Type: replace Abstract: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.08435","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.443019","language":"en","tags":["computer-science","cslg","preprints","csai","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":157,"author":"Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman","raw_content_length":1244,"priority":7,"update_frequency":1,"reading_time_minutes":0.785,"robust_parsing_used":true,"entities":{"organizations":["FaMTEB","BERT"],"persons":["Hakim"],"locations":["Corpesia","Pairsia"],"monetary":[]},"char_count":1243,"language_detected":"en","key_concepts":{"key_phrases":["Hakim","Farsi Text Embedding Model","arXiv250508435v3 Announce Type","Abstract","Recent advancements","text","have significantly improved natural language understanding","many languages","large-scale embedding research","this paper"],"filter_categories":{"research_academic":["large-scale embedding research"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hakim":3.0,"Farsi Text Embedding Model":2.0,"arXiv250508435v3 Announce Type":1.0,"Abstract":1.0,"Recent advancements":1.0,"text":1.0,"have significantly improved natural language understanding":1.0,"many languages":1.0,"large-scale embedding research":1.0,"this paper":1.0}},"age_hours":2.7804866697222224,"is_recent":true,"quality_score":1.0,"sentiment_score":9.36,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.872,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7848,"joy":0.0619,"surprise":0.1228,"sadness":0.0052,"fear":0.0047,"anger":0.0144,"disgust":0.0063},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new Persian text embedding model, Hakim, with a claimed 8.5% performance improvement on a specific benchmark. While the model shows promise and introduces new datasets, it's still in the early stages of development with no mention of deployment or real-world impact. The climate impact is indirect, potentially improving efficiency of chatbots or RAG systems, but not directly reducing emissions.","key_impact_metrics":["8.5% performance improvement on FaMTEB benchmark"],"technology_tags":["text embedding","natural language processing","machine learning"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:56:13.636705Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3035593c1f0e","title":"Chisme: Fully Decentralized Differentiated Deep Learning for IoT Intelligence","content":"arXiv:2505.09854v2 Announce Type: replace Abstract: As end-user device capability increases and demand for intelligent services at the Internet's edge rise, distributed learning has emerged as a key enabling technology. Existing approaches like federated learning (FL) and decentralized FL (DFL) enable distributed learning among clients, while gossip learning (GL) approaches have emerged to address the potential challenges in resource-constrained, connectivity-challenged infrastructure-less environments. However, most distributed learning approaches assume largely homogeneous data distributions and may not consider or exploit the heterogeneity of clients and their underlying data distributions. This paper introduces Chisme, a novel fully decentralized distributed learning algorithm designed to address the challenges of implementing robust intelligence in network edge contexts characterized by heterogeneous data distributions, episodic connectivity, and sparse network infrastructure. Chisme leverages cosine similarity-based data affinity heuristics calculated from received model exchanges to inform how much influence received models have when merging into the local model. By doing so, it facilitates stronger merging influence between clients with more similar model learning progressions, enabling clients to strategically balance between broader collaboration to build more general knowledge and more selective collaboration to build specific knowledge. We evaluate Chisme against contemporary approaches using image recognition and time-series prediction scenarios while considering different network connectivity conditions, representative of real-world distributed intelligent systems. Our experiments demonstrate that Chisme outperforms state-of-the-art edge intelligence approaches in almost every case -- clients using Chisme exhibit faster training convergence, lower final loss after training, and lower performance disparity between clients.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.09854","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.443845","language":"en","tags":["csma","computer-science","cslg","preprints","cssi","cset","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":242,"author":"Harikrishna Kuttivelil, Katia Obraczka","raw_content_length":1970,"priority":7,"update_frequency":1,"reading_time_minutes":1.21,"robust_parsing_used":true,"entities":{"organizations":["Chisme","DFL"],"persons":[],"locations":[],"monetary":[]},"char_count":1969,"language_detected":"en","key_concepts":{"key_phrases":["Chisme","Fully Decentralized Differentiated Deep Learning","IoT Intelligence","arXiv250509854v2","Announce Type","Abstract","end-user device capability increases","demand","intelligent services","the Internets edge rise"],"filter_categories":{"ai_ml":["Fully Decentralized Differentiated Deep Learning"],"engineering":["IoT Intelligence"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Chisme":2.0,"Fully Decentralized Differentiated Deep Learning":2.0,"IoT Intelligence":2.0,"arXiv250509854v2":1.0,"Announce Type":1.0,"Abstract":1.0,"end-user device capability increases":1.0,"demand":1.0,"intelligent services":1.0,"the Internets edge rise":1.0}},"age_hours":2.7805164994444445,"is_recent":true,"quality_score":1.0,"sentiment_score":8.8585,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7717,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9003,"joy":0.0152,"surprise":0.0476,"sadness":0.0036,"fear":0.015,"anger":0.0132,"disgust":0.0052},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel decentralized learning algorithm (Chisme) and presents experimental results showing improved performance in image recognition and time-series prediction. While the algorithm shows promise for edge intelligence, it is still in the applied research stage with no mention of real-world deployments or economic viability. The climate impact is indirect, potentially enabling more efficient IoT devices, but not directly reducing emissions.","key_impact_metrics":["faster training convergence","lower final loss after training"],"technology_tags":["decentralized learning","edge intelligence","federated learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:56:16.568616Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d09263b3154c","title":"FairSHAP: Preprocessing for Fairness Through Attribution","content":"arXiv:2505.11111v2 Announce Type: replace Abstract: Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/youlei202/FairSHAP.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.11111","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.444239","language":"en","tags":["computer-science","cslg","csai","preprints","cscy","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":169,"author":"Lin Zhu, Yijun Bian, Lei You","raw_content_length":1368,"priority":7,"update_frequency":1,"reading_time_minutes":0.845,"robust_parsing_used":true,"entities":{"organizations":["Shapley"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1367,"language_detected":"en","key_concepts":{"key_phrases":["FairSHAP","Fairness","Attribution","arXiv250511111v2","Announce Type","Abstract","Ensuring fairness","machine learning models","high-stakes domains","biased decisions"],"filter_categories":{"ai_ml":["FairSHAP","machine learning models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"FairSHAP":3.0,"Fairness":2.0,"Attribution":2.0,"arXiv250511111v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Ensuring fairness":1.0,"machine learning models":1.0,"high-stakes domains":1.0,"biased decisions":1.0}},"age_hours":2.780529930277778,"is_recent":true,"quality_score":1.0,"sentiment_score":4.614,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0772,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.4546,"joy":0.0059,"surprise":0.0155,"sadness":0.0198,"fear":0.0232,"anger":0.3923,"disgust":0.0887},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":7,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel pre-processing framework (FairSHAP) that aims to improve fairness in machine learning models. It achieves fairness gains with minimal data perturbation and, in some cases, improved predictive performance. The code is available on GitHub, suggesting a pilot stage of deployment.","key_impact_metrics":["fairness gains","minimal data perturbation"],"technology_tags":["machine learning","fairness","Shapley value"],"sdg_alignment":[10,16],"analyzed_at":"2025-10-28T20:56:19.181633Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_fda357a5e02b","title":"LLINBO: Trustworthy LLM-in","content":"arXiv:2505.14756v2 Announce Type: replace Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.14756","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.445021","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":183,"author":"Chih-Yu Chang, Milad Azvar, Chinedum Okwudire, Raed Al Kontar","raw_content_length":1430,"priority":7,"update_frequency":1,"reading_time_minutes":0.915,"robust_parsing_used":true,"entities":{"organizations":["Gaussian Processes"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1429,"language_detected":"en","key_concepts":{"key_phrases":["LLINBO","Trustworthy LLM","LLMs","Announce Type","Abstract","Bayesian optimization","a sequential decision-making tool","expensive black-box functions","Large Language Models","remarkable adaptability"],"filter_categories":{"ai_ml":["Trustworthy LLM","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"LLINBO":2.0,"Trustworthy LLM":2.0,"LLMs":2.0,"Announce Type":1.0,"Abstract":1.0,"Bayesian optimization":1.0,"a sequential decision-making tool":1.0,"expensive black-box functions":1.0,"Large Language Models":1.0,"remarkable adaptability":1.0}},"age_hours":2.7805598994444445,"is_recent":true,"quality_score":1.0,"sentiment_score":9.7795,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9559,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6824,"joy":0.0613,"surprise":0.0294,"sadness":0.0073,"fear":0.1958,"anger":0.0173,"disgust":0.0066},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework (LLINBO) that combines LLMs with statistical surrogate experts for Bayesian optimization, with a proof-of-concept in 3D printing. While the approach is innovative and shows promise for optimizing processes, it is still in the early stages of development with no deployed units or real-world data demonstrating significant climate impact. The vaporware flag is raised due to the lack of deployed units and operational data.","key_impact_metrics":[],"technology_tags":["Large Language Models","Bayesian Optimization","3D Printing"],"sdg_alignment":[9],"analyzed_at":"2025-10-28T20:56:21.848139Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2eead87b563b","title":"WebAgent-R1: Training Web Agents via End-to","content":"arXiv:2505.16421v2 Announce Type: replace Abstract: While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.16421","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.445507","language":"en","tags":["cscl","computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":190,"author":"Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li","raw_content_length":1469,"priority":7,"update_frequency":1,"reading_time_minutes":0.95,"robust_parsing_used":true,"entities":{"organizations":["WebAgent-R1"],"persons":[],"locations":[],"monetary":[]},"char_count":1468,"language_detected":"en","key_concepts":{"key_phrases":["WebAgent-R1","Training Web Agents","End","arXiv250516421v2 Announce Type","Abstract","reinforcement learning","remarkable success","large language models","LLMs","single-turn tasks"],"filter_categories":{"ai_ml":["Training Web Agents","reinforcement learning","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"WebAgent-R1":3.0,"Training Web Agents":2.0,"End":2.0,"arXiv250516421v2 Announce Type":1.0,"Abstract":1.0,"reinforcement learning":1.0,"remarkable success":1.0,"large language models":1.0,"LLMs":1.0,"single-turn tasks":1.0}},"age_hours":2.7805757630555554,"is_recent":true,"quality_score":1.0,"sentiment_score":9.792000000000002,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9584,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8499,"joy":0.0501,"surprise":0.0489,"sadness":0.006,"fear":0.0295,"anger":0.0126,"disgust":0.0029},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel RL framework (WebAgent-R1) for training web agents, demonstrating improved task success rates on the WebArena-Lite benchmark. While the research shows promise, it's currently in the applied research stage with no real-world deployments. The climate impact is indirect, as it could potentially improve efficiency in web-based tasks, but this is not quantified.","key_impact_metrics":["Task success rate increase for Qwen-2.5-3B from 6.1% to 33.9%","Task success rate increase for Llama-3.1-8B from 8.5% to 44.8%"],"technology_tags":["Reinforcement Learning","Large Language Models","Web Agents"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-28T20:56:25.917949Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_74576f4f3914","title":"Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning","content":"arXiv:2505.16567v3 Announce Type: replace Abstract: Finetuning open-weight Large Language Models (LLMs) is standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets leads to predictable behaviors. In this paper, we demonstrate, for the first time, that an adversary can create compromised LLMs that are performant and benign, yet exhibit adversarial behaviors once finetuned by downstream users. To this end, we propose an attack, FAB (Finetuning-activated Adversarial Behaviors), which compromises an LLM via meta-learning techniques that simulate downstream finetuning, explicitly optimizing for the emergence of adversarial behaviors in the finetuned models. At the same time, the compromised LLM is regularized to retain general capabilities and to exhibit no adversarial behaviors prior to finetuning. As a result, when users finetune (e.g., instruction-tuning, distillation, DPO) the seemingly benign model on their own datasets, they unknowingly trigger its dormant adversarial behavior. We experimentally demonstrate the effectiveness of FAB across multiple LLMs and three commonly considered target behaviors: unsolicited advertising, jailbreakability, and over-refusal. We show that FAB-triggers are robust to various finetuning choices made by the user (e.g., dataset, number of steps, scheduler, post-training algorithm). Our findings challenge prevailing assumptions on the security of finetuning, revealing a critical attack vector.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.16567","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.445928","language":"en","tags":["computer-science","cslg","csai","preprints","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev","raw_content_length":1573,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["LLM","FAB","Dormant Adversarial Behaviors","Adversarial Behaviors"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1572,"language_detected":"en","key_concepts":{"key_phrases":["your steps","LLM Finetuning","arXiv250516567v3 Announce Type","Abstract","open-weight Large Language Models","LLMs","standard practice","task-specific performance improvements","finetuning","a controlled and secure process"],"filter_categories":{"ai_ml":["LLM Finetuning","open-weight Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"your steps":2.0,"LLM Finetuning":2.0,"arXiv250516567v3 Announce Type":1.0,"Abstract":1.0,"open-weight Large Language Models":1.0,"LLMs":1.0,"standard practice":1.0,"task-specific performance improvements":1.0,"finetuning":1.0,"a controlled and secure process":1.0}},"age_hours":2.780590686388889,"is_recent":true,"quality_score":1.0,"sentiment_score":7.929500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5859,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8345,"joy":0.0072,"surprise":0.0086,"sadness":0.0036,"fear":0.0727,"anger":0.0501,"disgust":0.0234},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel attack vector on LLMs, demonstrating how seemingly benign models can be compromised to exhibit adversarial behaviors after finetuning. While the research is technically sound and uses meta-learning techniques, it is currently in the basic research stage with no deployed units or real-world data related to climate impact. The impact on sustainability is indirect, as compromised AI could potentially be used to spread misinformation or delay climate action, but this is theoretical.","key_impact_metrics":["Effectiveness of FAB attack across multiple LLMs","Robustness of FAB-triggers to various finetuning choices"],"technology_tags":["Large Language Models","Adversarial Machine Learning","Finetuning"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-28T20:56:30.817923Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_bc54b7f7a867","title":"What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse","content":"arXiv:2505.16592v3 Announce Type: replace Abstract: Media framing refers to the emphasis on specific aspects of perceived reality to shape how an issue is defined and understood. Its primary purpose is to shape public perceptions often in alignment with the authors' opinions and stances. However, the interaction between stance and media frame remains largely unexplored. In this work, we apply an interdisciplinary approach to conceptualize and computationally explore this interaction with internet memes on climate change. We curate CLIMATEMEMES, the first dataset of climate-change memes annotated with both stance and media frames, inspired by research in communication science. CLIMATEMEMES includes 1,184 memes sourced from 47 subreddits, enabling analysis of frame prominence over time and communities, and sheds light on the framing preferences of different stance holders. We propose two meme understanding tasks: stance detection and media frame detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the corresponding results on their LLM backbone. Human captions consistently enhance performance. Synthetic captions and human-corrected OCR also help occasionally. Our findings highlight that VLMs perform well on stance, but struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs' limitations in handling nuanced frames and stance expressions on climate change internet memes.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2505.16592","published_date":"2025-10-10T04:00:00","collected_date":"2025-10-10T06:41:18.446324","language":"en","tags":["cscl","csmm","computer-science","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":201,"author":"Shijia Zhou, Siyao Peng, Simon M. Luebke, J\\\"org Ha{\\ss}ler, Mario Haim, Saif M. Mohammad, Barbara Plank","raw_content_length":1428,"priority":7,"update_frequency":1,"reading_time_minutes":1.005,"robust_parsing_used":true,"entities":{"organizations":["Dataset","CLIMATEMEMES"],"persons":[],"locations":[],"monetary":[]},"char_count":1427,"language_detected":"en","key_concepts":{"key_phrases":["What","Media Frames","Stance","Study","Memes","Climate Change Discourse","arXiv250516592v3 Announce Type","Abstract","Media","the emphasis"],"filter_categories":{"research_academic":["Study"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"What":2.0,"Media Frames":2.0,"Stance":2.0,"Study":2.0,"Memes":2.0,"Climate Change Discourse":2.0,"arXiv250516592v3 Announce Type":1.0,"Abstract":1.0,"Media":1.0,"the emphasis":1.0}},"age_hours":2.7806060108333335,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9192,"joy":0.0077,"surprise":0.034,"sadness":0.0065,"fear":0.0085,"anger":0.0121,"disgust":0.012},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":1,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This article describes a dataset and methodology for analyzing climate change memes. While it contributes to understanding public discourse, it does not directly result in concrete climate action or measurable environmental outcomes. The technical credibility is moderate due to the use of computational methods and a defined dataset, but economic viability and deployment readiness are very low as it is purely research-based.","key_impact_metrics":["Accuracy of stance detection with VLMs","Performance on media frame detection"],"technology_tags":["Natural Language Processing","Machine Learning","Image Recognition"],"sdg_alignment":[4,13],"analyzed_at":"2025-10-28T20:56:36.379312Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
