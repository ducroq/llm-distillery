{"id":"science_arxiv_cs_7ba042127bc8","title":"TAG","content":"arXiv:2506.23484v3 Announce Type: replace Abstract: AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2506.23484","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.219914","language":"en","tags":["eessiv","computer-science","preprints","cscv","csmm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":184,"author":"Yuzhuo Chen, Zehua Ma, Han Fang, Weiming Zhang, Nenghai Yu","raw_content_length":1467,"priority":7,"update_frequency":1,"reading_time_minutes":0.92,"robust_parsing_used":true,"entities":{"organizations":["WLR","Tamper-Aware","WaterMarking"],"persons":[],"locations":[],"monetary":[]},"char_count":1466,"language_detected":"en","key_concepts":{"key_phrases":["TAG","arXiv250623484v3","Announce Type","Abstract","AI-generated content","AIGC","efficient visual creation","copyright and authenticity risks","a common technique","integrity verification"],"filter_categories":{"ai_ml":["AI-generated content"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"TAG":2.0,"arXiv250623484v3":1.0,"Announce Type":1.0,"Abstract":1.0,"AI-generated content":1.0,"AIGC":1.0,"efficient visual creation":1.0,"copyright and authenticity risks":1.0,"a common technique":1.0,"integrity verification":1.0}},"age_hours":2.7680776197222223,"is_recent":true,"quality_score":0.7,"sentiment_score":8.6555,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7311,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.854,"joy":0.0146,"surprise":0.0128,"sadness":0.01,"fear":0.0773,"anger":0.0229,"disgust":0.0085},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel method for watermarking AI-generated images to address copyright and authenticity risks. The method, TAG-WM, demonstrates state-of-the-art performance in tampering robustness and localization capability while preserving generation quality. However, it is still in the applied research phase with no real-world deployment, limiting its immediate sustainability impact.","key_impact_metrics":["watermark capacity of 256 bits","lossless generation quality"],"technology_tags":["digital watermarking","AI-generated content","image authentication"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:15.842818Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_639e0dec829e","title":"LH2Face: Loss function for Hard High","content":"arXiv:2506.23555v3 Announce Type: replace Abstract: In current practical face authentication systems, most face recognition (FR) algorithms are based on cosine similarity with softmax classification. Despite its reliable classification performance, this method struggles with hard samples. A popular strategy to improve FR performance is incorporating angular or cosine margins. However, it does not take face quality or recognition hardness into account, simply increasing the margin value and thus causing an overly uniform training strategy. To address this problem, a novel loss function is proposed, named Loss function for Hard High-quality Face (LH2Face). Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution is stated, specifically focusing on the logarithm of the Probability Density Function (PDF), which represents the distance between a probability distribution and a vector. Then, an adaptive margin-based multi-classification method using softmax, called the Uncertainty-Aware Margin Function, is implemented in the article. Furthermore, proxy-based loss functions are used to apply extra constraints between the proxy and sample to optimize their representation space distribution. Finally, a renderer is constructed that optimizes FR through face reconstruction and vice versa. Our LH2Face is superior to similiar schemes on hard high-quality face datasets, achieving 49.39% accuracy on the IJB-B dataset, which surpasses the second-place method by 2.37%.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2506.23555","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.220414","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":205,"author":"Fan Xie, Yang Wang, Yikang Jiao, Zhenyu Yuan, Congxi Chen, Chuanxin Zhao","raw_content_length":1498,"priority":7,"update_frequency":1,"reading_time_minutes":1.025,"robust_parsing_used":true,"entities":{"organizations":["LH2Face","PDF","the Probability Density Function","multi-classificati"],"persons":["Loss"],"locations":[],"monetary":[]},"char_count":1497,"language_detected":"en","key_concepts":{"key_phrases":["LH2Face","Loss function","Hard High","arXiv250623555v3 Announce Type","Abstract","current practical face authentication systems","most face recognition"," algorithms","cosine similarity","softmax classification"],"filter_categories":{"hydrogen_energy":["LH2Face"],"ai_ml":[" algorithms"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"LH2Face":2.0,"Loss function":2.0,"Hard High":2.0,"arXiv250623555v3 Announce Type":1.0,"Abstract":1.0,"current practical face authentication systems":1.0,"most face recognition":1.0," algorithms":1.0,"cosine similarity":1.0,"softmax classification":1.0}},"age_hours":2.768092467222222,"is_recent":true,"quality_score":1.0,"sentiment_score":5.1290000000000004,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7581,"joy":0.0028,"surprise":0.0172,"sadness":0.0358,"fear":0.0782,"anger":0.0521,"disgust":0.0558},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a novel loss function for face recognition. While it achieves a 2.37% improvement in accuracy on the IJB-B dataset (49.39% accuracy), it doesn't directly address climate change or sustainability. The technology is in the basic research stage with no deployment.","key_impact_metrics":["49.39% accuracy on the IJB-B dataset","2.37% accuracy improvement"],"technology_tags":["Face Recognition","Loss Function"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:18.668012Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9c9494432c4c","title":"The Illusion of Progress? A Critical Look at Test","content":"arXiv:2506.24000v2 Announce Type: replace Abstract: Test-time adaptation (TTA) methods have gained significant attention for enhancing the performance of vision-language models (VLMs) such as CLIP during inference, without requiring additional labeled data. However, current TTA researches generally suffer from major limitations such as duplication of baseline results, limited evaluation metrics, inconsistent experimental settings, and insufficient analysis. These problems hinder fair comparisons between TTA methods and make it difficult to assess their practical strengths and weaknesses. To address these challenges, we introduce TTA-VLM, a comprehensive benchmark for evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7 online TTA methods within a unified and reproducible framework, and evaluates them across 15 widely used datasets. Unlike prior studies focused solely on CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA to assess generality. Beyond classification accuracy, TTA-VLM incorporates various evaluation metrics, including robustness, calibration, out-of-distribution detection, and stability, enabling a more holistic assessment of TTA methods. Through extensive experiments, we find that 1) existing TTA methods produce limited gains compared to the previous pioneering work; 2) current TTA methods exhibit poor collaboration with training-time fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced model trustworthiness. We release TTA-VLM to provide fair comparison and comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the community to develop more reliable and generalizable TTA strategies.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2506.24000","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.220855","language":"en","tags":["computer-science","cslg","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":242,"author":"Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan","raw_content_length":1796,"priority":7,"update_frequency":1,"reading_time_minutes":1.21,"robust_parsing_used":true,"entities":{"organizations":["The Illusion of Progress","TTA","CLIP"],"persons":[],"locations":[],"monetary":[]},"char_count":1795,"language_detected":"en","key_concepts":{"key_phrases":["The Illusion","Progress","A Critical Look","Test","arXiv250624000v2 Announce Type","Abstract","Test-time adaptation","TTA methods","significant attention","the performance"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"The Illusion":2.0,"Progress":2.0,"A Critical Look":2.0,"Test":2.0,"arXiv250624000v2 Announce Type":1.0,"Abstract":1.0,"Test-time adaptation":1.0,"TTA methods":1.0,"significant attention":1.0,"the performance":1.0}},"age_hours":2.7681070805555557,"is_recent":true,"quality_score":1.0,"sentiment_score":4.36,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.128,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7924,"joy":0.0057,"surprise":0.0478,"sadness":0.073,"fear":0.0321,"anger":0.0163,"disgust":0.0327},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a benchmark (TTA-VLM) for evaluating test-time adaptation methods on vision-language models. While it aims to improve the reliability and generalizability of these methods, it is currently in the research phase with no deployed technology or measured outcomes related to climate impact. The benchmark includes 15 datasets and various evaluation metrics, providing some evidence strength.","key_impact_metrics":["Accuracy gains","Robustness"],"technology_tags":["Vision-Language Models","Test-Time Adaptation"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:23.393400Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_10c865196b41","title":"Theoretical Modeling of LLM Self","content":"arXiv:2507.00075v3 Announce Type: replace Abstract: Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further show how to model the entire training trajectory. This framework allows quantifying the capability limit of self-improvement by fitting the theoretical model to the experiment results. We empirically validate the effectiveness of the theoretical framework on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.00075","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.221253","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":178,"author":"Yifan Sun, Yushan Liang, Zhen Zhang, Jiaye Teng","raw_content_length":1304,"priority":7,"update_frequency":1,"reading_time_minutes":0.89,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1303,"language_detected":"en","key_concepts":{"key_phrases":["Theoretical Modeling","LLM Self","Announce Type","Self-improvement","the most prominent techniques","the realm","large language models","LLM","the LLM performance","external data"],"filter_categories":{"ai_ml":["LLM Self","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Theoretical Modeling":2.0,"LLM Self":2.0,"Announce Type":1.0,"Self-improvement":1.0,"the most prominent techniques":1.0,"the realm":1.0,"large language models":1.0,"LLM":1.0,"the LLM performance":1.0,"external data":1.0}},"age_hours":2.7681218802777776,"is_recent":true,"quality_score":1.0,"sentiment_score":7.182500000000001,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4365,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8642,"joy":0.0429,"surprise":0.0369,"sadness":0.0056,"fear":0.0189,"anger":0.0193,"disgust":0.0122},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents a theoretical model for LLM self-improvement. While the model is validated empirically on various LLMs and datasets, it remains at the theoretical stage with no concrete deployment or measurable impact on real-world sustainability challenges. The potential for indirect impact exists if LLMs can be used to accelerate sustainability research, but this is not directly addressed or quantified.","key_impact_metrics":["Solver-verifier gap","Training trajectory"],"technology_tags":["Large Language Models","Self-improvement","Theoretical Modeling"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:41:26.227894Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c774037c5292","title":"PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning","content":"arXiv:2507.01271v2 Announce Type: replace Abstract: In recent years, unlearning techniques, which are methods for inducing a model to \"forget\" previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.01271","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.221669","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":187,"author":"Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa","raw_content_length":1402,"priority":7,"update_frequency":1,"reading_time_minutes":0.935,"robust_parsing_used":true,"entities":{"organizations":["Sustainability Evalua","Practical Evaluation Scenarios for Large Multimodal Model Unlearning arXiv:2507.01271v2 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1401,"language_detected":"en","key_concepts":{"key_phrases":["PULSE","Practical Evaluation Scenarios","Large Multimodal Model Unlearning","LLMs","LMMs","arXiv250701271v2 Announce Type","Abstract","recent years","unlearning techniques","which"],"filter_categories":{"ai_ml":["LLMs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"PULSE":2.0,"Practical Evaluation Scenarios":2.0,"Large Multimodal Model Unlearning":2.0,"LLMs":2.0,"LMMs":2.0,"arXiv250701271v2 Announce Type":1.0,"Abstract":1.0,"recent years":1.0,"unlearning techniques":1.0,"which":1.0}},"age_hours":2.7681372444444445,"is_recent":true,"quality_score":1.0,"sentiment_score":6.1315,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2263,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.483,"joy":0.0132,"surprise":0.0076,"sadness":0.0119,"fear":0.3656,"anger":0.0658,"disgust":0.0529},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new protocol (PULSE) for evaluating unlearning in large multimodal models (LMMs). The concrete action is the development of a benchmark and evaluation of existing unlearning methods. Evidence is provided through experimental results, but it's still in the research phase with no deployed technology.","key_impact_metrics":["Performance degradation after sequential unlearning","Success rate of unlearning pre-trained knowledge"],"technology_tags":["Machine Learning","Unlearning","Large Multimodal Models"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T12:41:28.937914Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_504824d65211","title":"TriVLA: A Triple-System-Based Unified Vision","content":"arXiv:2507.01424v3 Announce Type: replace Abstract: Recent advances in vision-language models (VLMs) have enabled robots to follow open-ended instructions and demonstrate impressive commonsense reasoning. However, current vision-language-action (VLA) frameworks primarily rely on static representations and limited temporal context, restricting agents to short-horizon, reactive behaviors and hindering robust generalization in dynamic embodied environments. Inspired by cognitive neuroscience theories of episodic memory, we propose, to our knowledge, one of the first formalized episodic world models in VLA, enabling embodied robots to accumulate, recall, and predict sequential experiences. As an instantiation of this concept, our unified TriVLA realizes the episodic world model through a triple-system architecture: integrating multimodal grounding from a pretrained VLM (System 2) and temporally rich dynamics perception from a video diffusion model (System 3). This enables the agent to accumulate and recall sequential experiences, interpret current contexts, and predict future environmental evolution. Guided by episodic representations that span both the past and anticipated future, the downstream policy (System 1) generates coherent, context-aware action sequences through flow-matching and cross-modal attention mechanisms. Experimental results show that TriVLA operates efficiently at approximately 36 Hz and consistently outperforms baseline models on standard benchmarks and challenging real-world manipulation tasks. It demonstrates strong long-horizon planning and open-ended intent understanding, showcasing the advantages of episodic world model-inspired reasoning for robust, generalizable robot intelligence. Project Page: https://zhenyangliu.github.io/TriVLA/.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.01424","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.222079","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":218,"author":"Zhenyang Liu, Yongchong Gu, Sixiao Zheng, Yanwei Fu, Xiangyang Xue, Yu-Gang Jiang","raw_content_length":1788,"priority":7,"update_frequency":1,"reading_time_minutes":1.09,"robust_parsing_used":true,"entities":{"organizations":["VLA","VLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1787,"language_detected":"en","key_concepts":{"key_phrases":["TriVLA","A Triple-System-Based Unified Vision","Announce Type","Abstract","Recent advances","vision-language models","VLMs","robots","open-ended instructions","impressive commonsense reasoning"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"TriVLA":2.0,"A Triple-System-Based Unified Vision":2.0,"Announce Type":1.0,"Abstract":1.0,"Recent advances":1.0,"vision-language models":1.0,"VLMs":1.0,"robots":1.0,"open-ended instructions":1.0,"impressive commonsense reasoning":1.0}},"age_hours":2.7681523430555557,"is_recent":true,"quality_score":1.0,"sentiment_score":9.511000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9022,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6332,"joy":0.054,"surprise":0.0819,"sadness":0.0146,"fear":0.1231,"anger":0.0794,"disgust":0.0139},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel AI architecture (TriVLA) for embodied robots, demonstrating improved performance on manipulation tasks. While the technology could potentially improve efficiency in various sectors, leading to indirect emissions reductions, there are no direct climate benefits or quantified environmental impacts at this stage. It's currently in the applied research phase, with experimental results but no real-world deployments.","key_impact_metrics":["36 Hz operating frequency","Outperforms baseline models on standard benchmarks"],"technology_tags":["Vision-Language-Action Models","Robotics","Artificial Intelligence","Episodic World Models"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:32.235933Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_026ffa14e4e1","title":"GradMetaNet: An Equivariant Architecture for Learning on Gradients","content":"arXiv:2507.01649v2 Announce Type: replace Abstract: Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. In this paper, we present a principled approach for designing architectures that process gradients. Our approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, we introduce GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. We prove universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.01649","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.222463","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":173,"author":"Yoav Gelberg (Moe), Yam Eitan (Moe), Aviv Navon (Moe), Aviv Shamsian (Moe),  Theo (Moe),  Putterman, Michael Bronstein, Haggai Maron","raw_content_length":1347,"priority":7,"update_frequency":1,"reading_time_minutes":0.865,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1346,"language_detected":"en","key_concepts":{"key_phrases":["Gradients","GradMetaNet An Equivariant Architecture","optimization","gradients","arXiv250701649v2 Announce Type","Abstract","neural networks","valuable information","editing","analysis"],"filter_categories":{"ai_ml":["neural networks"],"business_innovation":["analysis"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Gradients":3.0,"GradMetaNet An Equivariant Architecture":2.0,"optimization":2.0,"gradients":2.0,"arXiv250701649v2 Announce Type":1.0,"Abstract":1.0,"neural networks":1.0,"valuable information":1.0,"editing":1.0,"analysis":1.0}},"age_hours":2.7681670625,"is_recent":true,"quality_score":0.7,"sentiment_score":8.352500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6705,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.892,"joy":0.0109,"surprise":0.0642,"sadness":0.0065,"fear":0.0062,"anger":0.0127,"disgust":0.0075},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel architecture for learning on gradients, which could potentially improve the efficiency of machine learning models used in climate-related applications. However, it is still in the basic research phase and there are no deployed units or quantified climate impacts at this stage. The technical credibility is high due to the universality results and demonstration on gradient-based tasks.","key_impact_metrics":[],"technology_tags":["machine learning","neural networks","optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:35.698725Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c3c5c0027172","title":"DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy","content":"arXiv:2507.01738v2 Announce Type: replace Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment objects in an image based on natural language expressions. While prior studies have predominantly concentrated on improving vision-language interactions and achieving fine-grained localization, a systematic analysis of the fundamental bottlenecks in existing RIS frameworks remains underexplored. To bridge this gap, we propose DeRIS, a novel framework that decomposes RIS into two key components: perception and cognition. This modular decomposition facilitates a systematic analysis of the primary bottlenecks impeding RIS performance. Our findings reveal that the predominant limitation lies not in perceptual deficiencies, but in the insufficient multi-modal cognitive capacity of current models. To mitigate this, we propose a Loopback Synergy mechanism, which enhances the synergy between the perception and cognition modules, thereby enabling precise segmentation while simultaneously improving robust image-text comprehension. Additionally, we analyze and introduce a simple non-referent sample conversion data augmentation to address the long-tail distribution issue related to target existence judgement in general scenarios. Notably, DeRIS demonstrates inherent adaptability to both non- and multi-referents scenarios without requiring specialized architectural modifications, enhancing its general applicability. The codes and models are available at https://github.com/Dmmm1997/DeRIS.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.01738","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.222875","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Ming Dai, Wenxuan Cheng, Jiang-jiang Liu, Sen Yang, Wenxiao Cai, Yanpeng Sun, Wankou Yang","raw_content_length":1528,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["RIS","DeRIS"],"persons":["a Loopback Synergy"],"locations":[],"monetary":[]},"char_count":1527,"language_detected":"en","key_concepts":{"key_phrases":["Perception","Cognition","Enhanced Referring Image Segmentation","Loopback Synergy","Announce Type","Abstract","Image Segmentation","RIS","a challenging task","objects"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Perception":2.0,"Cognition":2.0,"Enhanced Referring Image Segmentation":2.0,"Loopback Synergy":2.0,"Announce Type":1.0,"Abstract":1.0,"Image Segmentation":1.0,"RIS":1.0,"a challenging task":1.0,"objects":1.0}},"age_hours":2.7681820913888893,"is_recent":true,"quality_score":1.0,"sentiment_score":8.548,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7096,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8298,"joy":0.0117,"surprise":0.0445,"sadness":0.0205,"fear":0.0605,"anger":0.0186,"disgust":0.0144},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel AI framework (DeRIS) for improved image segmentation. While the research is technically sound and addresses a bottleneck in AI, it is currently in the basic research stage with no deployed units or measured outcomes related to sustainability. The potential climate impact is indirect and speculative, as it could potentially improve the efficiency of AI systems used in climate-related applications, but this is not demonstrated.","key_impact_metrics":[],"technology_tags":["Artificial Intelligence","Image Segmentation","Computer Vision"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:40.610830Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6803a1de4cbf","title":"Understanding and Improving Length Generalization in Recurrent Models","content":"arXiv:2507.02782v2 Announce Type: replace Abstract: Recently, recurrent models such as state space models and linear attention have become popular due to their linear complexity in the sequence length. Thanks to their recurrent nature, in principle they can process arbitrarily long sequences, but their performance sometimes drops considerably beyond their training context lengths-i.e. they fail to length generalize. In this work, we provide comprehensive empirical and theoretical analysis to support the unexplored states hypothesis, which posits that models fail to length generalize when during training they are only exposed to a limited subset of the distribution of all attainable states (i.e. states that would be attained if the recurrence was applied to long sequences). Furthermore, we investigate simple training interventions that aim to increase the coverage of the states that the model is trained on, e.g. by initializing the state with Gaussian noise or with the final state of a different input sequence. With only 500 post-training steps ($\\sim 0.1\\%$ of the pre-training budget), these interventions enable length generalization for sequences that are orders of magnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and show improved performance in long context tasks, thus presenting a simple and efficient way to enable robust length generalization in general recurrent models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.02782","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.223652","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Ricardo Buitrago Ruiz, Albert Gu","raw_content_length":1424,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["Improving Length Generalization"],"persons":[],"locations":[],"monetary":[]},"char_count":1423,"language_detected":"en","key_concepts":{"key_phrases":["Improving","Length Generalization","Recurrent Models","Announce Type","Abstract","recurrent models","state space models","linear attention","their linear complexity","the sequence length"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Improving":2.0,"Length Generalization":2.0,"Recurrent Models":2.0,"Announce Type":1.0,"Abstract":1.0,"recurrent models":1.0,"state space models":1.0,"linear attention":1.0,"their linear complexity":1.0,"the sequence length":1.0}},"age_hours":2.768212063055555,"is_recent":true,"quality_score":1.0,"sentiment_score":3.75,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.25,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8076,"joy":0.0083,"surprise":0.062,"sadness":0.0315,"fear":0.0165,"anger":0.0416,"disgust":0.0324},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents research on improving the length generalization of recurrent models, which could potentially improve the efficiency of AI models used in climate modeling or other sustainability-related applications. The research shows that with only 500 post-training steps, the models can generalize to sequences orders of magnitude longer than the training context (e.g. 2k -> 128k). However, this is still in the basic research phase and has not been deployed in any real-world applications.","key_impact_metrics":["2k -> 128k sequence length generalization","0.1% pre-training budget"],"technology_tags":["recurrent models","state space models","linear attention","machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:41:43.684807Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_35fd0c3fd89c","title":"Investigating VLM Hallucination from a Cognitive Psychology Perspective: A First Step Toward Interpretation with Intriguing Observations","content":"arXiv:2507.03123v2 Announce Type: replace Abstract: Hallucination is a long-standing problem that has been actively investigated in Vision-Language Models (VLMs). Existing research commonly attributes hallucinations to technical limitations or sycophancy bias, where the latter means the models tend to generate incorrect answers to align with user expectations. However, these explanations primarily focus on technical or externally driven factors, and may have neglected the possibility that hallucination behaviours might mirror cognitive biases observed in human psychology. In this work, we introduce a psychological taxonomy, categorizing VLMs' cognitive biases that lead to hallucinations, including sycophancy, logical inconsistency, and a newly identified VLMs behaviour: appeal to authority. To systematically analyze these behaviours, we design AIpsych, a scalable benchmark that reveals psychological tendencies in model response patterns. Leveraging this benchmark, we investigate how variations in model architecture and parameter size influence model behaviour when responding to strategically manipulated questions. Our experiments reveal that as model size increases, VLMs exhibit stronger sycophantic tendencies but reduced authority bias, suggesting increasing competence but a potential erosion of response integrity. A human subject study further validates our hypotheses and highlights key behavioural differences between VLMs and human respondents. This work suggests a new perspective for understanding hallucination in VLMs and highlights the importance of integrating psychological principles into model evaluation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.03123","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.224421","language":"en","tags":["computer-science","cslg","preprints","cscv","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":211,"author":"Xiangrui Liu, Man Luo, Agneet Chatterjee, Hua Wei, Chitta Baral, Yezhou Yang","raw_content_length":1642,"priority":7,"update_frequency":1,"reading_time_minutes":1.055,"robust_parsing_used":true,"entities":{"organizations":["Vision-Language Models","AIpsych"],"persons":[],"locations":[],"monetary":[]},"char_count":1641,"language_detected":"en","key_concepts":{"key_phrases":["VLM Hallucination","a Cognitive Psychology Perspective","Interpretation","Intriguing Observations","arXiv250703123v2","Announce Type","Abstract","Hallucination","a long-standing problem","Vision-Language Models"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"VLM Hallucination":2.0,"a Cognitive Psychology Perspective":2.0,"Interpretation":2.0,"Intriguing Observations":2.0,"arXiv250703123v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Hallucination":1.0,"a long-standing problem":1.0,"Vision-Language Models":1.0}},"age_hours":2.768241370277778,"is_recent":true,"quality_score":1.0,"sentiment_score":3.9884999999999997,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2023,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.5883,"joy":0.0036,"surprise":0.0349,"sadness":0.0308,"fear":0.2492,"anger":0.0462,"disgust":0.0471},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper investigates the cognitive biases in VLMs that lead to hallucinations using a newly designed benchmark. While it doesn't directly address climate change, improving AI's accuracy could indirectly benefit sustainability efforts by reducing errors in climate modeling or resource management. The research is at a basic research stage with no deployed technology or measurable environmental outcomes.","key_impact_metrics":["Stronger sycophantic tendencies with increased model size","Reduced authority bias with increased model size"],"technology_tags":["Vision-Language Models","AI","Cognitive Psychology"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:41:46.628804Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1aab04be6a3d","title":"Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?","content":"arXiv:2507.04632v4 Announce Type: replace Abstract: Recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs). The optimization process often requires numerous iterations to achieve satisfactory performance, resulting in high computational costs due to the need for frequent prompt evaluations under intensive LLM interactions and repeated policy updates. Appropriate online prompt selection methods reduce iteration steps by prioritizing informative prompts during training, while the pipeline's reliance on exhaustive prompt evaluation and subset selection for optimization still incurs substantial computational overhead due to frequent LLM inference calls. Distinguished from these direct evaluate-then-select schemes, this work investigates iterative approximate evaluation for arbitrary prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions. Technically, MoPPS models each prompt's success rate as a latent variable, performs streaming Bayesian inference, and employs posterior sampling in a constructed multi-armed bandit machine, enabling sample efficient and adaptive prompt selection. Extensive experiments across mathematics, planning, and vision-based geometry tasks show that MoPPS reliably predicts prompt difficulty and accelerates training with significantly reduced LLM rollouts. Our code is available at https://github.com/thu-rllab/MoPPS.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.04632","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.225207","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":194,"author":"Yun Qu, Qi Wang, Yixiu Mao, Vincent Tao Hu, Bj\\\"orn Ommer, Xiangyang Ji","raw_content_length":1594,"priority":7,"update_frequency":1,"reading_time_minutes":0.97,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1593,"language_detected":"en","key_concepts":{"key_phrases":["Prompt Difficulty","RL Finetuning","Reasoning Models","Announce Type","Abstract","Recent advances","the effectiveness","reinforcement learning","the reasoning capabilities","large language models"],"filter_categories":{"ai_ml":["reinforcement learning","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Prompt Difficulty":2.0,"RL Finetuning":2.0,"Reasoning Models":2.0,"Announce Type":1.0,"Abstract":1.0,"Recent advances":1.0,"the effectiveness":1.0,"reinforcement learning":1.0,"the reasoning capabilities":1.0,"large language models":1.0}},"age_hours":2.7682694144444446,"is_recent":true,"quality_score":1.0,"sentiment_score":7.009499999999999,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4019,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8813,"joy":0.0034,"surprise":0.0811,"sadness":0.0079,"fear":0.0105,"anger":0.0122,"disgust":0.0036},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the efficiency of reinforcement learning (RL) finetuning for large language models (LLMs), which indirectly reduces computational costs and energy consumption associated with training these models. The concrete action is the development of a new algorithm (MoPPS) that predicts prompt difficulty, leading to fewer LLM rollouts during training. The evidence supporting this claim comes from experiments across various tasks, showing that MoPPS reliably predicts prompt difficulty and accelerates training.","key_impact_metrics":["Reduced LLM rollouts","Accelerated training"],"technology_tags":["Reinforcement Learning","Large Language Models","Bayesian Inference"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-29T12:41:51.788149Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a4bffe9b27bc","title":"Train","content":"arXiv:2507.05195v2 Announce Type: replace Abstract: Existing language model benchmarks provide contradictory model rankings, even for benchmarks that aim to capture similar skills. This dilemma of conflicting rankings hampers model selection, clouds model comparisons, and adds confusion to a growing ecosystem of competing models. In this paper, we take a different perspective on model comparison: instead of relying on out-of-the-box performance via direct evaluation, we compare model potential by providing each model with identical benchmark-specific fine-tuning before evaluation. We call this approach train-before-test. Our primary contribution is a comprehensive empirical evaluation of model potential across 24 benchmarks and 61 models. First, we demonstrate that model potential rankings obtained through train-before-test exhibit remarkable consistency across all benchmarks. Whereas traditional rankings demonstrate little external validity under direct evaluation, they enjoy a significant degree of external validity when applying train-before-test: model potential rankings transfer gracefully from one benchmark to another. Second, train-before-test restores the connection between perplexity and downstream task performance, lost under direct evaluation. Remarkably, even pre-finetuning perplexity of a base model predicts post-finetuning downstream performance, suggesting that ranking consistency reflects inherent model potential rather than fine-tuning artifacts. Finally, train-before-test reduces the model-score matrix to essentially rank one, indicating that model potential is dominated by one latent factor, uncovered by train-before-test. Our work supports the recommendation to make train-before-test a default component of LLM benchmarking.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.05195","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.225657","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":222,"author":"Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt","raw_content_length":1774,"priority":7,"update_frequency":1,"reading_time_minutes":1.11,"robust_parsing_used":true,"entities":{"organizations":["Train arXiv:2507.05195v2 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1773,"language_detected":"en","key_concepts":{"key_phrases":["Train","arXiv250705195v2 Announce Type","Abstract","Existing language model benchmarks","contradictory model rankings","benchmarks","similar skills","This dilemma","conflicting rankings","clouds model comparisons"],"filter_categories":{"ai_ml":["Train"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Train":2.0,"arXiv250705195v2 Announce Type":1.0,"Abstract":1.0,"Existing language model benchmarks":1.0,"contradictory model rankings":1.0,"benchmarks":1.0,"similar skills":1.0,"This dilemma":1.0,"conflicting rankings":1.0,"clouds model comparisons":1.0}},"age_hours":2.768284367222222,"is_recent":true,"quality_score":0.7,"sentiment_score":1.2195,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7561,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9327,"joy":0.006,"surprise":0.022,"sadness":0.0063,"fear":0.0094,"anger":0.0159,"disgust":0.0077},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper focuses on improving the benchmarking of large language models (LLMs) through a 'train-before-test' methodology. While improved LLM efficiency *could* indirectly reduce energy consumption, there are no concrete actions or measurable outcomes related to direct climate impact. The research is at a basic research stage, with no deployment or operational data.","key_impact_metrics":["Ranking consistency across 24 benchmarks","Predicts post-finetuning downstream performance"],"technology_tags":["Large Language Models","Benchmarking","Fine-tuning"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T12:41:54.618493Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4a8074fcd0bf","title":"PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs","content":"arXiv:2507.05444v3 Announce Type: replace Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most methods still rely on direct IPA-based phonetic matching or employ LLMs without phonological guidance. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that performs IPA-based phonological adaptation and syllable-aware alignment to retrieve L1 keyword sequence and uses LLMs to generate verbal cues. We evaluate PhoniTale through automated metrics and a short-term recall test with human participants, comparing its output to human-written and prior automated mnemonics. Our findings show that PhoniTale consistently outperforms previous automated approaches and achieves quality comparable to human-written mnemonics.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.05444","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.226080","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":150,"author":"Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh","raw_content_length":1159,"priority":7,"update_frequency":1,"reading_time_minutes":0.75,"robust_parsing_used":true,"entities":{"organizations":["Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs arXiv:2507.05444v3 Announce Type:","IPA","PhoniTale"],"persons":[],"locations":[],"monetary":[]},"char_count":1158,"language_detected":"en","key_concepts":{"key_phrases":["PhoniTale","Phonologically Grounded Mnemonic Generation","Typologically Distant Language Pairs","arXiv250705444v3 Announce Type","Abstract","Vocabulary acquisition","a significant challenge","second-language L2 learners","typologically distant languages","English"],"filter_categories":{"ai_ml":["Typologically Distant Language Pairs"],"business_innovation":["Vocabulary acquisition"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"PhoniTale":2.0,"Phonologically Grounded Mnemonic Generation":2.0,"Typologically Distant Language Pairs":2.0,"arXiv250705444v3 Announce Type":1.0,"Abstract":1.0,"Vocabulary acquisition":1.0,"a significant challenge":1.0,"second-language L2 learners":1.0,"typologically distant languages":1.0,"English":1.0}},"age_hours":2.7682995369444447,"is_recent":true,"quality_score":1.0,"sentiment_score":6.3660000000000005,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2732,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8134,"joy":0.0109,"surprise":0.0896,"sadness":0.0111,"fear":0.0466,"anger":0.0208,"disgust":0.0076},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":1,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel mnemonic generation system (PhoniTale) for language learning. While it shows promise in improving vocabulary acquisition, it lacks direct connection to sustainability or climate change mitigation. The evaluation includes automated metrics and a short-term recall test, providing some evidence of its effectiveness, but it is still in the early stages of development.","key_impact_metrics":["Recall test performance","Automated metric scores"],"technology_tags":["Natural Language Processing","Machine Learning","Education Technology"],"sdg_alignment":[4],"analyzed_at":"2025-10-29T12:42:01.492418Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b6f4e2c18884","title":"SenseShift6D: Multimodal RGB","content":"arXiv:2507.05751v2 Announce Type: replace Abstract: Recent advances on 6D object-pose estimation have achieved high performance on representative benchmarks such as LM-O, YCB-V, and T-Less. However, these datasets were captured under fixed illumination and camera settings, leaving the impact of real-world variations in illumination, exposure, gain or depth-sensor mode-and the potential of test-time sensor control to mitigate such variations-largely unexplored. To bridge this gap, we introduce SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures, 9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels. For five common household objects (spray, pringles, tincase, sandwich, and mouse), we acquire 166.4k RGB and 16.7k depth images, which can provide 1,380 unique sensor-lighting permutations per object pose. Experiments with state-of-the-art models on our dataset demonstrate that applying multimodal sensor control at test time yields substantial performance gains, achieving a 19.5 pp improvement on pretrained generalizable models. It also enhances robustness precisely where those models tend to fail. Moreover, even instance-level pose estimators, where train and test set share identical object and background, performance still varies under environmental and sensor change, demonstrating that test-time sensor control remains effective compared to costly expansions in the quantity and diversity of real-world training data, without any additional training. SenseShift6D extends the object pose evaluation paradigm from data-centered to sensor-aware robustness, laying a foundation for adaptive, self-tuning perception systems capable of operating robustly in uncertain real-world environments.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.05751","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.226895","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":231,"author":"Yegyu Han, Taegyoon Yoon, Dayeon Woo, Sojeong Kim, Hyung-Sin Kim","raw_content_length":1755,"priority":7,"update_frequency":1,"reading_time_minutes":1.155,"robust_parsing_used":true,"entities":{"organizations":["Multimodal RGB arXiv:2507.05751v2 Announce Type: replace Abstract","RGB"],"persons":[],"locations":[],"monetary":[]},"char_count":1754,"language_detected":"en","key_concepts":{"key_phrases":["SenseShift6D","Multimodal RGB","arXiv250705751v2 Announce Type","Abstract","Recent advances","6D object-pose estimation","high performance","representative benchmarks","LM-O","YCB-V"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"SenseShift6D":2.0,"Multimodal RGB":2.0,"arXiv250705751v2 Announce Type":1.0,"Abstract":1.0,"Recent advances":1.0,"6D object-pose estimation":1.0,"high performance":1.0,"representative benchmarks":1.0,"LM-O":1.0,"YCB-V":1.0}},"age_hours":2.7683321066666666,"is_recent":true,"quality_score":1.0,"sentiment_score":7.6335,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5267,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.93,"joy":0.0052,"surprise":0.0222,"sadness":0.0067,"fear":0.015,"anger":0.013,"disgust":0.0078},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new dataset and methodology for improving 6D object-pose estimation in variable real-world conditions. While it demonstrates a 19.5 percentage point improvement in model performance, the direct climate impact is indirect, potentially improving the efficiency of systems that rely on object recognition (e.g., robotics in manufacturing or logistics). It is still in the applied research phase, with no deployment data.","key_impact_metrics":["19.5 pp improvement on pretrained generalizable models","1,380 unique sensor-lighting permutations per object pose"],"technology_tags":["6D object-pose estimation","RGB-D dataset","sensor control"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:05.478284Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_8176d5257599","title":"Automatic Synthesis of High","content":"arXiv:2507.05970v3 Announce Type: replace Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.05970","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.227289","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":208,"author":"Haiwen Li, Delong Liu, Zhaohui Hou, Zhicheng Zhao, Fei Su","raw_content_length":1531,"priority":7,"update_frequency":1,"reading_time_minutes":1.04,"robust_parsing_used":true,"entities":{"organizations":["CIRHS","CIR","Composed Image Retrieval","Synthetic Triplets","Hybrid Contextual Alignment (CoAlign"],"persons":["Composed Image Retrieval"],"locations":[],"monetary":[]},"char_count":1530,"language_detected":"en","key_concepts":{"key_phrases":["Automatic Synthesis","High","Announce Type","Abstract","a challenging vision-language VL task","Composed Image Retrieval","CIR","target images","multimodal imagetext queries","many existing CIR methods"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Automatic Synthesis":2.0,"High":2.0,"Announce Type":1.0,"Abstract":1.0,"a challenging vision-language VL task":1.0,"Composed Image Retrieval":1.0,"CIR":1.0,"target images":1.0,"multimodal imagetext queries":1.0,"many existing CIR methods":1.0}},"age_hours":2.7683475386111107,"is_recent":true,"quality_score":1.0,"sentiment_score":7.202,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4404,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9216,"joy":0.0057,"surprise":0.0207,"sadness":0.0136,"fear":0.0122,"anger":0.018,"disgust":0.0082},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for automatic generation of training data for image retrieval models. While the method itself doesn't directly reduce GHG emissions, it could potentially improve the efficiency of AI models used in various sustainability applications (e.g., identifying sustainable materials, optimizing energy consumption), but this is theoretical. The research is at the applied research stage, with no deployed units or real-world data yet, but it does show improved performance on benchmarks.","key_impact_metrics":["Zero-shot performance on three benchmarks","Outperforms state-of-the-art supervised CIR approaches"],"technology_tags":["Artificial Intelligence","Image Recognition","Large Language Models"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:11.579146Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7861503c1e14","title":"On the Surprising Effectiveness of a Single Global Merging in Decentralized Learning","content":"arXiv:2507.06542v2 Announce Type: replace Abstract: Decentralized learning provides a scalable alternative to parameter-server-based training, yet its performance is often hindered by limited peer-to-peer communication. In this paper, we study how communication should be scheduled over time to improve global generalization, including determining when and how frequently devices synchronize. Counterintuitive empirical results show that concentrating communication budgets in the later stages of decentralized training remarkably improves global generalization. Surprisingly, we uncover that fully connected communication at the final step, implemented by a single global merging, can significant improve the generalization performance of decentralized learning under serve high data heterogeneity. Our theoretical contributions, which explains these phenomena, are first to establish that the globally merged model of decentralized SGD can match the convergence rate of parallel SGD. Technically, we reinterpret part of the discrepancy among local models, which were previously considered as detrimental noise, as constructive components essential for matching this rate. This work provides promising results that decentralized learning is able to generalize under high data heterogeneity and limited communication, while offering broad new avenues for model merging research. The code will be made publicly available.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.06542","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.227706","language":"en","tags":["statml","cslg","csma","preprints","csdc","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":185,"author":"Tongtian Zhu, Tianyu Zhang, Mingze Wang, Zhanpeng Zhou, Can Wang","raw_content_length":1421,"priority":7,"update_frequency":1,"reading_time_minutes":0.925,"robust_parsing_used":true,"entities":{"organizations":["the Surprising Effectiveness","Decentralized Learning arXiv:2507.06542v2 Announce Type:"],"persons":[],"locations":[],"monetary":[]},"char_count":1420,"language_detected":"en","key_concepts":{"key_phrases":["the Surprising Effectiveness","a Single Global Merging","Decentralized Learning","Announce Type","Abstract","Decentralized learning","a scalable alternative","parameter-server-based training","its performance","peer"],"filter_categories":{"ai_ml":["parameter-server-based training"],"research_academic":["peer"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"the Surprising Effectiveness":2.0,"a Single Global Merging":2.0,"Decentralized Learning":2.0,"Announce Type":1.0,"Abstract":1.0,"Decentralized learning":1.0,"a scalable alternative":1.0,"parameter-server-based training":1.0,"its performance":1.0,"peer":1.0}},"age_hours":2.7683616433333333,"is_recent":true,"quality_score":1.0,"sentiment_score":7.383500000000001,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4767,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.3688,"joy":0.0507,"surprise":0.4997,"sadness":0.0127,"fear":0.0302,"anger":0.0244,"disgust":0.0135},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents theoretical contributions and empirical results on decentralized learning, showing that a single global merging step can improve generalization performance. While the research is peer-reviewed and provides a theoretical basis, it is still in the early stages of development with no deployed units or real-world data. The potential climate impact is indirect, as it could improve the efficiency of training models for climate-related applications, but this is not explicitly quantified.","key_impact_metrics":[],"technology_tags":["decentralized learning","federated learning","machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:14.567113Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f3fb89bca505","title":"Simulating Three","content":"arXiv:2507.08972v2 Announce Type: replace Abstract: Turbulent fluid flows are among the most computationally demanding problems in science, requiring enormous computational resources that become prohibitive at high flow speeds. Physics-informed neural networks (PINNs) represent a radically different approach that trains neural networks directly from physical equations rather than data, offering the potential for continuous, mesh-free solutions. Here we show that appropriately designed PINNs can successfully simulate fully turbulent flows in both two and three dimensions, directly learning solutions to the fundamental fluid equations without traditional computational grids or training data. Our approach combines several algorithmic innovations including adaptive network architectures, causal training, and advanced optimization methods to overcome the inherent challenges of learning chaotic dynamics. Through rigorous validation on challenging turbulence problems, we demonstrate that PINNs accurately reproduce key flow statistics including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our results demonstrate that neural equation solvers can handle complex chaotic systems, opening new possibilities for continuous turbulence modeling that transcends traditional computational limitations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.08972","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.228471","language":"en","tags":["physicsflu-dyn","cslg","physicscomp-ph","csai","preprints","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Sifan Wang, Shyam Sankaran, Xiantao Fan, Panos Stinis, Paris Perdikaris","raw_content_length":1324,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["causal training"],"locations":[],"monetary":[]},"char_count":1323,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250708972v2 Announce Type","Abstract","Turbulent fluid flows","the most computationally demanding problems","science","enormous computational resources","high flow speeds","Physics-informed neural networks","PINNs","a radically different approach"],"filter_categories":{"ai_ml":["science","Physics-informed neural networks"],"research_academic":["science"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250708972v2 Announce Type":1.0,"Abstract":1.0,"Turbulent fluid flows":1.0,"the most computationally demanding problems":1.0,"science":1.0,"enormous computational resources":1.0,"high flow speeds":1.0,"Physics-informed neural networks":1.0,"PINNs":1.0,"a radically different approach":1.0}},"age_hours":2.768390205,"is_recent":true,"quality_score":1.0,"sentiment_score":2.333,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5334,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8641,"joy":0.0239,"surprise":0.0849,"sadness":0.0033,"fear":0.0067,"anger":0.0129,"disgust":0.0042},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes a novel approach to simulating turbulent fluid flows using physics-informed neural networks (PINNs). The concrete action is the development and validation of these PINNs. Evidence is provided by demonstrating that PINNs accurately reproduce key flow statistics, including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. The technology is at the applied research stage, with validation on challenging turbulence problems but no mention of real-world deployment.","key_impact_metrics":["Energy spectra","Kinetic energy"],"technology_tags":["Physics-informed neural networks","Turbulence modeling"],"sdg_alignment":[7,9],"analyzed_at":"2025-10-29T12:42:18.025915Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d44da4321605","title":"Learning Diffusion Models with Flexible Representation Guidance","content":"arXiv:2507.08980v2 Announce Type: replace Abstract: Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.08980","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.228906","language":"en","tags":["computer-science","cslg","csai","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":196,"author":"Chenyu Wang, Cai Zhou, Sharut Gupta, Zongyu Lin, Stefanie Jegelka, Stephen Bates, Tommi Jaakkola","raw_content_length":1510,"priority":7,"update_frequency":1,"reading_time_minutes":0.98,"robust_parsing_used":true,"entities":{"organizations":["Learning Diffusion Models with"],"persons":[],"locations":[],"monetary":[]},"char_count":1509,"language_detected":"en","key_concepts":{"key_phrases":["Learning Diffusion Models","Flexible Representation Guidance","arXiv250708980v2 Announce Type","Abstract","Diffusion models","additional guidance","more effective representations","input","prior empirical work","internal representations"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Learning Diffusion Models":2.0,"Flexible Representation Guidance":2.0,"arXiv250708980v2 Announce Type":1.0,"Abstract":1.0,"Diffusion models":1.0,"additional guidance":1.0,"more effective representations":1.0,"input":1.0,"prior empirical work":1.0,"internal representations":1.0}},"age_hours":2.7684039480555556,"is_recent":true,"quality_score":1.0,"sentiment_score":9.4025,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8805,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.871,"joy":0.0328,"surprise":0.01,"sadness":0.0048,"fear":0.0263,"anger":0.0328,"disgust":0.0223},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper presents a new method for improving diffusion models, which could potentially accelerate the design of sustainable materials like proteins and molecules. The claim of 23.3x faster training than SiT-XL on ImageNet is a concrete metric, but the impact on sustainability is indirect and depends on the applications of the generated materials. It's still in the research phase with no deployed applications.","key_impact_metrics":["23.3 times faster training than the original SiT-XL","four times speedup over the state-of-the-art method REPA"],"technology_tags":["diffusion models","machine learning","generative AI","protein design","molecule design"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:21.369678Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_48dc72273850","title":"RoHOI: Robustness Benchmark for Human","content":"arXiv:2507.09111v3 Announce Type: replace Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate predictions. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusions, and noise. Our benchmark, RoHOI, includes 20 corruption types based on the HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the HOI field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, thus dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show that our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code are available at https://github.com/KratosWen/RoHOI.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.09111","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.229301","language":"en","tags":["eessiv","preprints","cscv","research","cshc","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":159,"author":"Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen","raw_content_length":1263,"priority":7,"update_frequency":1,"reading_time_minutes":0.795,"robust_parsing_used":true,"entities":{"organizations":["Semantic-Aware Masking","Progressive Learning","HOI"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1262,"language_detected":"en","key_concepts":{"key_phrases":["Robustness Benchmark","Human","arXiv250709111v3 Announce Type","Abstract","HOI","robot-human assistance","context-aware support","However models","real-world conditions","unforeseen corruptions"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Robustness Benchmark":2.0,"Human":2.0,"arXiv250709111v3 Announce Type":1.0,"Abstract":1.0,"HOI":1.0,"robot-human assistance":1.0,"context-aware support":1.0,"However models":1.0,"real-world conditions":1.0,"unforeseen corruptions":1.0}},"age_hours":2.768418971388889,"is_recent":true,"quality_score":1.0,"sentiment_score":7.1075,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4215,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.877,"joy":0.0034,"surprise":0.0196,"sadness":0.0103,"fear":0.0193,"anger":0.0327,"disgust":0.0376},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research introduces a robustness benchmark (RoHOI) for Human-Object Interaction detection, aiming to improve the reliability of AI systems in real-world conditions. While the research itself doesn't directly reduce GHG emissions, it could indirectly contribute by enabling more reliable robotic assistance in various sectors, potentially leading to efficiency gains. The technical credibility is supported by the benchmark dataset and a proposed improvement strategy (SAMPL), with performance metrics provided, but it is still in the applied research stage with no deployment.","key_impact_metrics":["Performance drops under corruptions","Performance improvement with SAMPL strategy"],"technology_tags":["Human-Object Interaction","Robustness Benchmark","Semantic-Aware Masking"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:28.164148Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1d79f0620143","title":"Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically","content":"arXiv:2507.09279v4 Announce Type: replace Abstract: Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/prompt4trust.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.09279","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.229735","language":"en","tags":["computer-science","csai","preprints","cscv","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":240,"author":"Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel","raw_content_length":1867,"priority":7,"update_frequency":1,"reading_time_minutes":1.2,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1866,"language_detected":"en","key_concepts":{"key_phrases":["arXiv250709279v4 Announce Type","Abstract","Multimodal large language models","MLLMs","considerable promise","applications","healthcare","their deployment","safety-critical settings","two key limitations"],"filter_categories":{"ai_ml":["Multimodal large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv250709279v4 Announce Type":1.0,"Abstract":1.0,"Multimodal large language models":1.0,"MLLMs":1.0,"considerable promise":1.0,"applications":1.0,"healthcare":1.0,"their deployment":1.0,"safety-critical settings":1.0,"two key limitations":1.0}},"age_hours":2.768433715,"is_recent":true,"quality_score":1.0,"sentiment_score":9.2395,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8479,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8444,"joy":0.0044,"surprise":0.0201,"sadness":0.0324,"fear":0.0491,"anger":0.0277,"disgust":0.0219},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a reinforcement learning framework (Prompt4Trust) for improving the trustworthiness of multimodal large language models (MLLMs) in healthcare. The concrete action is the development and testing of this framework on a medical visual question answering benchmark (PMC-VQA), achieving state-of-the-art performance. However, it is still in the applied research stage with no real-world deployment, and the climate impact is indirect, related to potential efficiencies in healthcare resource allocation.","key_impact_metrics":["State-of-the-art medical VQA performance on PMC-VQA benchmark"],"technology_tags":["Reinforcement Learning","Prompt Augmentation","Multimodal Large Language Models","Medical Visual Question Answering"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T12:42:32.084201Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_287c9ad7fa58","title":"RedOne: Revealing Domain","content":"arXiv:2507.10605v2 Announce Type: replace Abstract: As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.10605","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.230134","language":"en","tags":["computer-science","cslg","csai","preprints","research","cssi","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":212,"author":"Fei Zhao, Chonggang Lu, Yue Wang, Zheyong Xie, Ziyan Liu, Haofu Qian, JianZhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, Mingqian He, Xinze Lyu, Yiming Lu, Ziyang Xiang, Zheyu Ye, Chengqiang Lu, Zhe Xu, Yi Wu, Yao Hu, Yan Gao, Jun Fan, Xiaolong Jiang, Weiting Liu, Boyang Wang, Shaosheng Cao","raw_content_length":1603,"priority":7,"update_frequency":1,"reading_time_minutes":1.06,"robust_parsing_used":true,"entities":{"organizations":["LLM","SNS"],"persons":[],"locations":[],"monetary":[]},"char_count":1602,"language_detected":"en","key_concepts":{"key_phrases":["Domain","which","arXiv250710605v2 Announce Type","Abstract","a primary medium","modern information dissemination","social networking services","SNS","rapid growth","significant challenges"],"filter_categories":{"ai_ml":["Domain"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Domain":2.0,"which":2.0,"arXiv250710605v2 Announce Type":1.0,"Abstract":1.0,"a primary medium":1.0,"modern information dissemination":1.0,"social networking services":1.0,"SNS":1.0,"rapid growth":1.0,"significant challenges":1.0}},"age_hours":2.7684477558333334,"is_recent":true,"quality_score":1.0,"sentiment_score":8.478,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6956,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7378,"joy":0.0231,"surprise":0.1423,"sadness":0.0382,"fear":0.0179,"anger":0.0278,"disgust":0.0129},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a domain-specific LLM (RedOne) and its performance improvements on SNS tasks. While it mentions a reduction in harmful content exposure and improved click-through rates, the connection to direct climate impact or sustainability is weak. The technology is still in the early stages of deployment, lacking real-world operational data and independent verification.","key_impact_metrics":["harmful content detection by 11.23%","click page rate in post-view search by 14.95%"],"technology_tags":["large language models","artificial intelligence","content moderation"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T12:42:35.303416Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1c32084b2cc3","title":"A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF","content":"arXiv:2507.10864v3 Announce Type: replace Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a crucial role in diagnosing and preventing colorectal cancer, a major cause of mortality worldwide. This study introduces a new, lightweight, and efficient framework for polyp detection that combines the Local Outlier Factor (LOF) algorithm for filtering noisy data with the YOLO-v11n deep learning model. Study design: An experimental study leveraging deep learning and outlier removal techniques across multiple public datasets. Methods: The proposed approach was tested on five diverse and publicly available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene. Since these datasets originally lacked bounding box annotations, we converted their segmentation masks into suitable detection labels. To enhance the robustness and generalizability of our model, we apply 5-fold cross-validation and remove anomalous samples using the LOF method configured with 30 neighbors and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a fast and resource-efficient object detection architecture optimized for real-time applications. We train the model using a combination of modern augmentation strategies to improve detection accuracy under diverse conditions. Results: Our approach significantly improves polyp localization performance, achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods, our model demonstrates enhanced accuracy and efficiency. Conclusions: These results suggest that the proposed method is well-suited for real-time colonoscopy support in clinical settings. Overall, the study underscores how crucial data preprocessing and model efficiency are when designing effective AI systems for medical imaging.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.10864","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.230566","language":"en","tags":["computer-science","csai","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":253,"author":"Saadat Behzadi, Danial Sharifrazi, Bita Mesbahzadeh, Javad Hassannataj Joloudari, Roohallah Alizadehsani","raw_content_length":1890,"priority":7,"update_frequency":1,"reading_time_minutes":1.265,"robust_parsing_used":true,"entities":{"organizations":["LOF","ETIS","EndoScene"],"persons":["Kvasir-SEG"],"locations":[],"monetary":[]},"char_count":1881,"language_detected":"en","key_concepts":{"key_phrases":["LOF","A Lightweight and Robust Framework","Real-Time Colorectal Polyp Detection","arXiv250710864v3 Announce Type","Abstract","Objectives","Timely and accurate detection","colorectal polyps","a crucial role","colorectal cancer"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"LOF":3.0,"A Lightweight and Robust Framework":2.0,"Real-Time Colorectal Polyp Detection":2.0,"arXiv250710864v3 Announce Type":1.0,"Abstract":1.0,"Objectives":1.0,"Timely and accurate detection":1.0,"colorectal polyps":1.0,"a crucial role":1.0,"colorectal cancer":1.0}},"age_hours":2.768463556111111,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8651,"joy":0.0386,"surprise":0.0332,"sadness":0.0072,"fear":0.032,"anger":0.0122,"disgust":0.0117},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for polyp detection using deep learning. While it improves accuracy and efficiency compared to previous methods, it's still in the research phase with no deployed units or real-world operational data. The impact on climate is indirect, through improved healthcare and potentially reduced cancer rates, but not directly related to emissions reduction or carbon sequestration.","key_impact_metrics":["precision of 95.83%","recall of 91.85%"],"technology_tags":["deep learning","medical imaging","YOLO-v11n","Local Outlier Factor"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T12:42:38.580589Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6e71f26d2b84","title":"Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification","content":"arXiv:2507.11620v2 Announce Type: replace Abstract: Event time series are sequences of discrete events occurring at irregular time intervals, each associated with a domain-specific observational modality. They are common in domains such as high-energy astrophysics, computational social science, cybersecurity, finance, healthcare, neuroscience, and seismology. Their unstructured and irregular structure poses significant challenges for extracting meaningful patterns and identifying salient phenomena using conventional techniques. We propose novel two- and three-dimensional tensor representations for event time series, coupled with sparse autoencoders that learn physically meaningful latent representations. These embeddings support a variety of downstream tasks, including anomaly detection, similarity-based retrieval, semantic clustering, and unsupervised classification. We demonstrate our approach on a real-world dataset from X-ray astronomy, showing that these representations successfully capture temporal and spectral signatures and isolate diverse classes of X-ray transients. Our framework offers a flexible, scalable, and generalizable solution for analyzing complex, irregular event time series across scientific and industrial domains.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.11620","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.230953","language":"en","tags":["cslg","csai","astro-phhe","preprints","research","computer-science","astro-phim","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":151,"author":"Steven Dillmann, Juan Rafael Mart\\'inez-Galarza","raw_content_length":1256,"priority":7,"update_frequency":1,"reading_time_minutes":0.755,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Sparse Autoencoders for Anomaly Detection"],"locations":["anomaly"],"monetary":[]},"char_count":1255,"language_detected":"en","key_concepts":{"key_phrases":["Representations","Event Time Series","Sparse Autoencoders","Anomaly Detection","Similarity Search","Unsupervised Classification","arXiv250711620v2 Announce Type","Abstract","Event time series","sequences"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Representations":2.0,"Event Time Series":2.0,"Sparse Autoencoders":2.0,"Anomaly Detection":2.0,"Similarity Search":2.0,"Unsupervised Classification":2.0,"arXiv250711620v2 Announce Type":1.0,"Abstract":1.0,"Event time series":1.0,"sequences":1.0}},"age_hours":2.7684783475,"is_recent":true,"quality_score":1.0,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8412,"joy":0.0059,"surprise":0.0975,"sadness":0.0083,"fear":0.024,"anger":0.0158,"disgust":0.0073},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for analyzing event time series data, which could potentially be applied to various sustainability-related domains like climate monitoring or grid management. However, the article focuses on the methodology and demonstrates it on X-ray astronomy data, lacking concrete deployment or quantified environmental benefits. It is still in the applied research stage, with no evidence of real-world deployment or economic viability.","key_impact_metrics":[],"technology_tags":["machine learning","anomaly detection","time series analysis"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:42.447581Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_166c3dffcad9","title":"{S\\textsuperscript{2}M\\textsuperscript{2}}: Scalable Stereo Matching Model for Reliable Depth Estimation","content":"arXiv:2507.13229v4 Announce Type: replace Abstract: The pursuit of a generalizable stereo matching model, capable of performing well across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. However, global matching architectures, while theoretically more robust, have historically been rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with {S\\textsuperscript{2}M\\textsuperscript{2}}: a global matching architecture that achieves state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. {S\\textsuperscript{2}M\\textsuperscript{2}} establishes a new state of the art on Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods in most metrics while reconstructing high-quality details with competitive efficiency.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.13229","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.232107","language":"en","tags":["computer-science","csai","preprints","cscv","research","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Junhong Min, Youngpil Jeon, Jimin Kim, Minyong Choi","raw_content_length":1347,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1346,"language_detected":"en","key_concepts":{"key_phrases":["Stextsuperscript2Mtextsuperscript2","Scalable Stereo Matching Model","Reliable Depth Estimation","arXiv250713229v4 Announce Type","Abstract","The pursuit","a generalizable stereo matching model","varying resolutions","dataset-specific fine-tuning","a fundamental trade-off"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Stextsuperscript2Mtextsuperscript2":2.0,"Scalable Stereo Matching Model":2.0,"Reliable Depth Estimation":2.0,"arXiv250713229v4 Announce Type":1.0,"Abstract":1.0,"The pursuit":1.0,"a generalizable stereo matching model":1.0,"varying resolutions":1.0,"dataset-specific fine-tuning":1.0,"a fundamental trade-off":1.0}},"age_hours":2.7685218052777776,"is_recent":true,"quality_score":1.0,"sentiment_score":6.423,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2846,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8105,"joy":0.0273,"surprise":0.0677,"sadness":0.0206,"fear":0.0211,"anger":0.033,"disgust":0.0198},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel stereo matching model for depth estimation. While it achieves state-of-the-art accuracy on benchmarks, it is still in the research phase with no deployed units or real-world operational data. The potential climate impact is indirect, as improved depth estimation could potentially improve efficiency in robotics or autonomous vehicles, but this is not quantified.","key_impact_metrics":["Accuracy on Middlebury v3 benchmark","Accuracy on ETH3D benchmark"],"technology_tags":["stereo matching","depth estimation","computer vision","transformer networks"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:42:49.827837Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_66f5ae5dfeb5","title":"Online MMS Allocation for Chores","content":"arXiv:2507.14039v2 Announce Type: replace Abstract: We study the problem of fair division of indivisible chores among $n$ agents in an online setting, where items arrive sequentially and must be allocated irrevocably upon arrival. The goal is to produce an $\\alpha$-MMS allocation at the end. Several recent works have investigated this model, but have only succeeded in obtaining non-trivial algorithms under restrictive assumptions, such as the two-agent bi-valued special case (Wang and Wei, 2025), or by assuming knowledge of the total disutility of each agent (Zhou, Bai, and Wu, 2023). For the general case, the trivial $n$-MMS guarantee remains the best known, while the strongest lower bound is still only $2$. We close this gap on the negative side by proving that for any fixed $n$ and $\\varepsilon$, no algorithm can guarantee an $(n - \\varepsilon)$-MMS allocation. Notably, this lower bound holds precisely for every $n$, without hiding constants in big-$O$ notation, thereby exactly matching the trivial upper bound. Despite this strong impossibility result, we also present positive results. We provide an online algorithm that applies in the general case, guaranteeing a $\\min\\{n, O(k), O(\\log D)\\}$-MMS allocation, where $k$ is the maximum number of distinct disutilities across all agents and $D$ is the maximum ratio between the largest and smallest disutilities for any agent. This bound is reasonable across a broad range of scenarios and, for example, implies that we can achieve an $O(1)$-MMS allocation whenever $k$ is constant. Moreover, to optimize the constant in the important personalized bi-valued case, we show that if each agent has at most two distinct disutilities, our algorithm guarantees a $(2 + \\sqrt{3}) \\approx 3.7$-MMS allocation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.14039","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.232529","language":"en","tags":["research","csgt","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":271,"author":"Jiaxin Song, Biaoshuai Tao, Wenqian Wang, Yuhao Zhang","raw_content_length":1775,"priority":7,"update_frequency":1,"reading_time_minutes":1.355,"robust_parsing_used":true,"entities":{"organizations":["Wang"],"persons":["Wei","Zhou","\\varepsilon$","n$-MMS","Bai"],"locations":[],"monetary":["only $2$.","$\\alpha$-MMS allocation"]},"char_count":1770,"language_detected":"en","key_concepts":{"key_phrases":["Online MMS Allocation","Chores","arXiv250714039v2 Announce Type","Abstract","the problem","fair division","indivisible chores","n agents","an online setting","items"],"filter_categories":{"ai_ml":["fair division"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Online MMS Allocation":2.0,"Chores":2.0,"arXiv250714039v2 Announce Type":1.0,"Abstract":1.0,"the problem":1.0,"fair division":1.0,"indivisible chores":1.0,"n agents":1.0,"an online setting":1.0,"items":1.0}},"age_hours":2.768537709166667,"is_recent":true,"quality_score":1.0,"sentiment_score":8.9675,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7935,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8999,"joy":0.0182,"surprise":0.0538,"sadness":0.0073,"fear":0.005,"anger":0.0121,"disgust":0.0037},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":1,"systemic_impact":3,"justice_equity":7,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents theoretical research on fair allocation of chores. While it doesn't have direct climate impact, it addresses resource allocation, which could indirectly impact sustainability by optimizing resource use and promoting equitable distribution of burdens. The research is peer-reviewed, increasing its credibility, but it's in the early stages of development with no deployment.","key_impact_metrics":[],"technology_tags":["fair_division","online_algorithms","resource_allocation"],"sdg_alignment":[10,12],"analyzed_at":"2025-10-29T12:42:58.388625Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_67feb9b723c1","title":"Gait Transitions in Load","content":"arXiv:2507.14727v3 Announce Type: replace Abstract: Quadrupedal animals employ diverse galloping strategies to optimize speed, stability, and energy efficiency. However, the biomechanical mechanisms that enable adaptive gait transitions during high-speed locomotion under load remain poorly understood. In this study, we present new empirical and modeling insights into the biomechanics of load-pulling quadrupeds, using sprint sled dogs as a model system. High-speed video and force recordings reveal that sled dogs often switch between rotary and transverse galloping gaits within just a few strides and without any observable changes in speed, stride duration, or terrain, providing clear evidence of locomotor multistability during high-speed load-pulling. To investigate the mechanical basis of these transitions, a physics-based quadrupedal Spring-Loaded Inverted Pendulum model with hybrid dynamics and prescribed footfall sequences to reproduce the asymmetric galloping patterns observed in racing sled dogs. Through trajectory optimization, we replicate experimentally observed gait sequences and identify swing-leg stiffness modulation as a key control mechanism for inducing transitions. This work provides a much-needed biomechanical perspective on high-speed animal draft and establishes a modeling framework for studying locomotion in pulling quadrupeds, with implications for both biological understanding and the design of adaptive legged systems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.14727","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.232933","language":"en","tags":["eesssy","cssy","preprints","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":191,"author":"Jiayu Ding, Benjamin Seleb, Heather J. Huson, Saad Bhamla, Zhenyu Gan","raw_content_length":1464,"priority":7,"update_frequency":1,"reading_time_minutes":0.955,"robust_parsing_used":true,"entities":{"organizations":["Gait Transitions","Spring-Loaded Inverted Pendulum"],"persons":[],"locations":[],"monetary":[]},"char_count":1463,"language_detected":"en","key_concepts":{"key_phrases":["Gait Transitions","Load","arXiv250714727v3 Announce Type","Abstract","Quadrupedal animals","diverse galloping strategies","speed","stability","energy efficiency","the biomechanical mechanisms"],"filter_categories":{"ai_ml":["Gait Transitions"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Gait Transitions":2.0,"Load":2.0,"arXiv250714727v3 Announce Type":1.0,"Abstract":1.0,"Quadrupedal animals":1.0,"diverse galloping strategies":1.0,"speed":1.0,"stability":1.0,"energy efficiency":1.0,"the biomechanical mechanisms":1.0}},"age_hours":2.768553055,"is_recent":true,"quality_score":1.0,"sentiment_score":8.8915,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7783,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9209,"joy":0.0081,"surprise":0.0316,"sadness":0.0083,"fear":0.0084,"anger":0.0106,"disgust":0.0122},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":1,"deployment_readiness":2,"systemic_impact":1,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on quadrupedal locomotion and gait transitions, using sled dogs as a model system. While the research is scientifically sound and provides insights into biomechanics, it does not directly address climate change or sustainability in a concrete way. The work is at the applied research stage, focusing on modeling and experimentation but without any deployed technology or measurable environmental outcomes.","key_impact_metrics":["stride duration","swing-leg stiffness modulation"],"technology_tags":["biomechanics","robotics","locomotion modeling"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:43:01.583473Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1b717a7a3e21","title":"Adaptive Network Security Policies via Belief Aggregation and Rollout","content":"arXiv:2507.15163v3 Announce Type: replace Abstract: Evolving security vulnerabilities and shifting operational conditions require frequent updates to network security policies. These updates include adjustments to incident response procedures and modifications to access controls, among others. Reinforcement learning methods have been proposed for automating such policy adaptations, but most of the methods in the research literature lack performance guarantees and adapt slowly to changes. In this paper, we address these limitations and present a method for computing security policies that is scalable, offers theoretical guarantees, and adapts quickly to changes. It assumes a model or simulator of the system and comprises three components: belief estimation through particle filtering, offline policy computation through aggregation, and online policy adaptation through rollout. Central to our method is a new feature-based aggregation technique, which improves scalability and flexibility. We analyze the approximation error of aggregation and show that rollout efficiently adapts policies to changes under certain conditions. Simulations and testbed results demonstrate that our method outperforms state-of-the-art methods on several benchmarks, including CAGE-2.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.15163","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.233311","language":"en","tags":["eesssy","cssy","preprints","research","cscr","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Kim Hammar, Yuchao Li, Tansu Alpcan, Emil C. Lupu, Dimitri Bertsekas","raw_content_length":1275,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":["Adaptive Network Security Policies","Belief Aggregation"],"persons":[],"locations":[],"monetary":[]},"char_count":1274,"language_detected":"en","key_concepts":{"key_phrases":["Adaptive Network Security Policies","Belief Aggregation","Rollout","arXiv250715163v3 Announce Type","Abstract","security vulnerabilities","operational conditions","frequent updates","security policies","These updates"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Adaptive Network Security Policies":2.0,"Belief Aggregation":2.0,"Rollout":2.0,"arXiv250715163v3 Announce Type":1.0,"Abstract":1.0,"security vulnerabilities":1.0,"operational conditions":1.0,"frequent updates":1.0,"security policies":1.0,"These updates":1.0}},"age_hours":2.7685668619444446,"is_recent":true,"quality_score":1.0,"sentiment_score":4.8065,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0387,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8742,"joy":0.0028,"surprise":0.0099,"sadness":0.0168,"fear":0.0356,"anger":0.0284,"disgust":0.0324},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new method for adaptive network security policies using reinforcement learning. It includes simulations and testbed results, suggesting a proof-of-concept stage. While it aims to improve security and scalability, the direct climate impact is indirect, and economic viability and deployment readiness are still low.","key_impact_metrics":["Approximation error of aggregation","Performance on CAGE-2 benchmark"],"technology_tags":["Reinforcement Learning","Network Security","Policy Adaptation"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T12:43:05.036303Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f15cec33871a","title":"TinyIO: Lightweight Reparameterized Inertial Odometry","content":"arXiv:2507.15293v2 Announce Type: replace Abstract: Inertial localization is regarded as a promising positioning solution for consumer-grade IoT devices due to its cost-effectiveness and independence from external infrastructure. However, data-driven inertial localization methods often rely on increasingly complex network architectures to improve accuracy, which challenges the limited computational resources of IoT devices. Moreover, these methods frequently overlook the importance of modeling long-term dependencies in inertial measurements - a critical factor for accurate trajectory reconstruction - thereby limiting localization performance. To address these challenges, we propose a reparameterized inertial localization network that uses a multi-branch structure during training to enhance feature extraction. At inference time, this structure is transformed into an equivalent single-path architecture to improve parameter efficiency. To further capture long-term dependencies in motion trajectories, we introduce a temporal-scale sparse attention mechanism that selectively emphasizes key trajectory segments while suppressing noise. Additionally, a gated convolutional unit is incorporated to effectively integrate long-range dependencies with local fine-grained features. Extensive experiments on public benchmarks demonstrate that our method achieves a favorable trade-off between accuracy and model compactness. For example, on the RoNIN dataset, our approach reduces the Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while reducing the number of parameters by 3.86%.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.15293","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.233719","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":201,"author":"Shanshan Zhang, Siyue Wang, Liqin Wu, Qi Zhang, Tianshui Wen, Ziheng Zhou, Ao Peng, Xuemin Hong, Lingxiang Zheng, Yu Yang","raw_content_length":1605,"priority":7,"update_frequency":1,"reading_time_minutes":1.005,"robust_parsing_used":true,"entities":{"organizations":["IoT","Lightweight Reparameterized Inertial Odometry arXiv:2507.15293v2 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1604,"language_detected":"en","key_concepts":{"key_phrases":["Lightweight Reparameterized Inertial Odometry","Announce Type","Abstract","Inertial localization","a promising positioning solution","consumer-grade IoT devices","its cost-effectiveness","independence","external infrastructure","data-driven inertial localization methods"],"filter_categories":{"engineering":["consumer-grade IoT devices"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Lightweight Reparameterized Inertial Odometry":2.0,"Announce Type":1.0,"Abstract":1.0,"Inertial localization":1.0,"a promising positioning solution":1.0,"consumer-grade IoT devices":1.0,"its cost-effectiveness":1.0,"independence":1.0,"external infrastructure":1.0,"data-driven inertial localization methods":1.0}},"age_hours":2.768580694444444,"is_recent":true,"quality_score":1.0,"sentiment_score":8.715,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.743,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9119,"joy":0.0291,"surprise":0.0325,"sadness":0.0061,"fear":0.0055,"anger":0.0075,"disgust":0.0075},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel inertial odometry method that improves accuracy and reduces model complexity for IoT devices. The method is validated on a public benchmark (RoNIN dataset) with a reported 2.59% reduction in Absolute Trajectory Error (ATE) and a 3.86% reduction in parameters. However, it is still in the applied research stage with no mention of real-world deployment or economic viability beyond the benchmark results.","key_impact_metrics":["ATE reduction 2.59%","Parameter reduction 3.86%"],"technology_tags":["inertial odometry","IoT","localization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:43:09.078334Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d5c50ecd9236","title":"Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks","content":"arXiv:2507.16043v2 Announce Type: replace Abstract: We investigate the extent to which Spiking Neural Networks (SNNs) trained with Surrogate Gradient Descent (Surrogate GD), with and without delay learning, can learn from precise spike timing beyond firing rates. We first design synthetic tasks isolating intra-neuron inter-spike intervals and cross-neuron synchrony under matched spike counts. On more complex spike-based speech recognition datasets (Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC), we construct variants where spike count information is eliminated and only timing information remains, and show that Surrogate GD-trained SNNs are able to perform significantly above chance whereas purely rate-based models perform at chance level. We further evaluate robustness under biologically inspired perturbations -- including Gaussian jitter per spike or per-neuron, and spike deletion -- revealing consistent but perturbation-specific degradation. Networks show a sharp performance drop when spike sequences are reversed in time, with a larger drop in performance from SNNs trained with delays, indicating that these networks are more human-like in terms of behaviour. To facilitate further studies of temporal coding, we have released our modified SHD and SSC datasets.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.16043","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.234110","language":"en","tags":["csai","preprints","research","csne","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":179,"author":"Ziqiao Yu, Pengfei Sun, Dan F. M. Goodman","raw_content_length":1300,"priority":7,"update_frequency":1,"reading_time_minutes":0.895,"robust_parsing_used":true,"entities":{"organizations":["SSC","Spiking Neural Networks","Surrogate Gradient Descent"],"persons":[],"locations":["Surrogate GD"],"monetary":[]},"char_count":1299,"language_detected":"en","key_concepts":{"key_phrases":["Spiking Neural Networks","Rate Coding","Surrogate Gradients","Spike Timing Learning","arXiv250716043v2 Announce Type","Abstract","the extent","which","SNNs","Surrogate Gradient Descent"],"filter_categories":{"ai_ml":["Spiking Neural Networks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Spiking Neural Networks":3.0,"Rate Coding":2.0,"Surrogate Gradients":2.0,"Spike Timing Learning":2.0,"arXiv250716043v2 Announce Type":1.0,"Abstract":1.0,"the extent":1.0,"which":1.0,"SNNs":1.0,"Surrogate Gradient Descent":1.0}},"age_hours":2.7685964097222224,"is_recent":true,"quality_score":1.0,"sentiment_score":4.438,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1124,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9046,"joy":0.0152,"surprise":0.055,"sadness":0.0038,"fear":0.0062,"anger":0.0108,"disgust":0.0045},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents research on spiking neural networks and their ability to learn from precise spike timing. While the research is technically credible and innovative, it is still in the basic research stage and lacks concrete deployment or measurable climate impact. The potential for energy efficiency gains in AI is there, but not yet realized.","key_impact_metrics":["Performance above chance on spike-based speech recognition datasets","Performance drop when spike sequences are reversed"],"technology_tags":["Spiking Neural Networks","Surrogate Gradient Descent","AI","Machine Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:43:12.408357Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3984573faa16","title":"Efficient Compositional Multi","content":"arXiv:2507.16083v2 Announce Type: replace Abstract: Adapter parameters provide a mechanism to modify the behavior of machine learning models and have gained significant popularity in the context of large language models (LLMs) and generative AI. These parameters can be merged to support multiple tasks via a process known as task merging. However, prior work on merging in LLMs, particularly in natural language processing, has been limited to scenarios where each test example addresses only a single task. In this paper, we focus on on-device settings and study the problem of text-based compositional multi-tasking, where each test example involves the simultaneous execution of multiple tasks. For instance, generating a translated summary of a long text requires solving both translation and summarization tasks concurrently. To facilitate research in this setting, we propose a benchmark comprising four practically relevant compositional tasks. We also present an efficient method (Learnable Calibration) tailored for on-device applications, where computational resources are limited, emphasizing the need for solutions that are both resource-efficient and high-performing. Our contributions lay the groundwork for advancing the capabilities of LLMs in real-world multi-tasking scenarios, expanding their applicability to complex, resource-constrained use cases.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.16083","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.234507","language":"en","tags":["computer-science","cslg","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":188,"author":"Ondrej Bohdal, Mete Ozay, Jijoong Moon, Kyeng-Hun Lee, Hyeonmok Ko, Umberto Michieli","raw_content_length":1371,"priority":7,"update_frequency":1,"reading_time_minutes":0.94,"robust_parsing_used":true,"entities":{"organizations":["Efficient Compositional Multi arXiv:2507.16083v2 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1370,"language_detected":"en","key_concepts":{"key_phrases":["Efficient Compositional Multi","LLMs","arXiv250716083v2 Announce Type","Abstract","Adapter parameters","a mechanism","the behavior","machine learning models","significant popularity","the context"],"filter_categories":{"ai_ml":["LLMs","machine learning models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Efficient Compositional Multi":2.0,"LLMs":2.0,"arXiv250716083v2 Announce Type":1.0,"Abstract":1.0,"Adapter parameters":1.0,"a mechanism":1.0,"the behavior":1.0,"machine learning models":1.0,"significant popularity":1.0,"the context":1.0}},"age_hours":2.768611003611111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.582999999999998,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9166,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9005,"joy":0.0176,"surprise":0.0606,"sadness":0.0055,"fear":0.004,"anger":0.0085,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article proposes a new method for efficient multi-tasking in LLMs, specifically for on-device applications. While this could potentially reduce energy consumption by optimizing resource use, the impact is theoretical and not yet quantified. The research is at an early stage, with no deployed units or operational data available, hence the vaporware flag.","key_impact_metrics":[],"technology_tags":["Large Language Models","Adapter Parameters","On-device Machine Learning"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-29T12:43:15.249202Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4186d03eece2","title":"StarIO: A Lightweight Inertial Odometry for Nonlinear Motion","content":"arXiv:2507.16121v2 Announce Type: replace Abstract: Inertial odometry (IO) directly estimates the position of a carrier from inertial sensor measurements and serves as a core technology for the widespread deployment of consumer grade localization systems. While existing IO methods can accurately reconstruct simple and near linear motion trajectories, they often fail to account for drift errors caused by complex motion patterns such as turning. This limitation significantly degrades localization accuracy and restricts the applicability of IO systems in real world scenarios. To address these challenges, we propose a lightweight IO framework. Specifically, inertial data is projected into a high dimensional implicit nonlinear feature space using the Star Operation method, enabling the extraction of complex motion features that are typically overlooked. We further introduce a collaborative attention mechanism that jointly models global motion dynamics across both channel and temporal dimensions. In addition, we design Multi Scale Gated Convolution Units to capture fine grained dynamic variations throughout the motion process, thereby enhancing the model's ability to learn rich and expressive motion representations. Extensive experiments demonstrate that our proposed method consistently outperforms SOTA baselines across six widely used inertial datasets. Compared to baseline models on the RoNIN dataset, it achieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a new benchmark in the field.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.16121","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.234905","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":212,"author":"Shanshan Zhang, Siyue Wang, Qi Zhang Liqin Wu, Tianshui Wen, Ziheng Zhou, Xuemin Hong, Lingxiang Zheng, Yu Yang","raw_content_length":1532,"priority":7,"update_frequency":1,"reading_time_minutes":1.06,"robust_parsing_used":true,"entities":{"organizations":["A Lightweight Inertial Odometry for Nonlinear Motion arXiv:2507.16121v2","linear motion trajectories"],"persons":[],"locations":[],"monetary":[]},"char_count":1531,"language_detected":"en","key_concepts":{"key_phrases":["StarIO","A Lightweight Inertial Odometry","Nonlinear Motion","arXiv250716121v2 Announce Type","Abstract","Inertial odometry","the position","a carrier","inertial sensor measurements","a core technology"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"StarIO":2.0,"A Lightweight Inertial Odometry":2.0,"Nonlinear Motion":2.0,"arXiv250716121v2 Announce Type":1.0,"Abstract":1.0,"Inertial odometry":1.0,"the position":1.0,"a carrier":1.0,"inertial sensor measurements":1.0,"a core technology":1.0}},"age_hours":2.768624651388889,"is_recent":true,"quality_score":1.0,"sentiment_score":1.452,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7096,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8327,"joy":0.0048,"surprise":0.0247,"sadness":0.0381,"fear":0.0286,"anger":0.0317,"disgust":0.0394},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel inertial odometry framework that improves localization accuracy, potentially enabling more efficient navigation for various applications. While the technology is promising and shows performance improvements on benchmark datasets, it is still in the research phase and lacks real-world deployment data. The reduction in ATE ranging from 2.26% to 65.78% compared to baseline models on the RoNIN dataset is a measurable outcome.","key_impact_metrics":["ATE reduction on RoNIN dataset: 2.26% to 65.78%"],"technology_tags":["Inertial Odometry","Localization","Nonlinear Motion"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:43:18.637041Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2234218a82f0","title":"Re:Form -","content":"arXiv:2507.16331v3 Announce Type: replace Abstract: Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.16331","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.235744","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":243,"author":"Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, Jie Fu","raw_content_length":1877,"priority":7,"update_frequency":1,"reading_time_minutes":1.215,"robust_parsing_used":true,"entities":{"organizations":["Reinforcement Learning","Dafny"],"persons":[],"locations":[],"monetary":[]},"char_count":1876,"language_detected":"en","key_concepts":{"key_phrases":["LLMs","arXiv250716331v3","Announce Type","Abstract","Existing informal language-based eg human language","Large Language Models","Reinforcement Learning","a significant challenge","their verification processes","which"],"filter_categories":{"ai_ml":["LLMs","Large Language Models","Reinforcement Learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"LLMs":2.0,"arXiv250716331v3":1.0,"Announce Type":1.0,"Abstract":1.0,"Existing informal language-based eg human language":1.0,"Large Language Models":1.0,"Reinforcement Learning":1.0,"a significant challenge":1.0,"their verification processes":1.0,"which":1.0}},"age_hours":2.7686548127777777,"is_recent":true,"quality_score":1.0,"sentiment_score":7.929500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5859,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.5365,"joy":0.0072,"surprise":0.0125,"sadness":0.0132,"fear":0.374,"anger":0.0388,"disgust":0.0178},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper explores using formal language-based reasoning to improve the reliability and scalability of LLMs, potentially leading to more reliable software verification. The concrete action is the development of DafnyComp, a benchmark for compositional formal programs, and the demonstration that even small models can generate verifiable code. The evidence is the performance of the models on the DafnyComp benchmark, surpassing proprietary models. The stage of deployment is applied research, with a focus on developing and testing the methodology.","key_impact_metrics":["0.5B model size","Surpassing proprietary models in Dafny code generation"],"technology_tags":["Large Language Models","Formal Verification","Reinforcement Learning","Dafny"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T12:43:27.004519Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_376bca66ff1b","title":"STAR: A Benchmark for Astronomical Star Fields Super","content":"arXiv:2507.16385v2 Announce Type: replace Abstract: Super-resolution (SR) advances astronomical imaging by enabling cost-effective high-resolution capture, crucial for detecting faraway celestial objects and precise structural analysis. However, existing datasets for astronomical SR (ASR) exhibit three critical limitations: flux inconsistency, object-crop setting, and insufficient data diversity, significantly impeding ASR development. We propose STAR, a large-scale astronomical SR dataset containing 54,738 flux-consistent star field image pairs covering wide celestial regions. These pairs combine Hubble Space Telescope high-resolution observations with physically faithful low-resolution counterparts generated through a flux-preserving data generation pipeline, enabling systematic development of field-level ASR models. To further empower the ASR community, STAR provides a novel Flux Error (FE) to evaluate SR models in physical view. Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR) model that could accurately infer the flux-consistent high-resolution images from input photometry, suppressing several SR state-of-the-art methods by 24.84% on a novel designed flux consistency metric, showing the priority of our method for astrophysics. Extensive experiments demonstrate the effectiveness of our proposed method and the value of our dataset. Code and models are available at https://github.com/GuoCheng12/STAR.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.16385","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.236132","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":180,"author":"Kuo-Cheng Wu, Guohang Zhuang, Jinyang Huang, Xiang Zhang, Wanli Ouyang, Yan Lu","raw_content_length":1456,"priority":7,"update_frequency":1,"reading_time_minutes":0.9,"robust_parsing_used":true,"entities":{"organizations":["STAR","Flux Error (FE","ASR","SR (ASR"],"persons":[],"locations":[],"monetary":[]},"char_count":1455,"language_detected":"en","key_concepts":{"key_phrases":["STAR","Astronomical Star Fields Super","Announce Type","Abstract","Super","resolution","astronomical imaging","cost-effective high-resolution capture","faraway celestial objects","precise structural analysis"],"filter_categories":{"business_innovation":["STAR"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"STAR":3.0,"Astronomical Star Fields Super":2.0,"Announce Type":1.0,"Abstract":1.0,"Super":1.0,"resolution":1.0,"astronomical imaging":1.0,"cost-effective high-resolution capture":1.0,"faraway celestial objects":1.0,"precise structural analysis":1.0}},"age_hours":2.768669876388889,"is_recent":true,"quality_score":1.0,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8404,"joy":0.0088,"surprise":0.0533,"sadness":0.0229,"fear":0.0264,"anger":0.0351,"disgust":0.0131},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving astronomical imaging through super-resolution techniques. While it doesn't directly address climate change, better astronomical data can potentially aid in understanding climate-related phenomena on other planets or in identifying exoplanets. The technical credibility is high due to the use of Hubble data and a flux-preserving data generation pipeline, validated by a 24.84% improvement on a flux consistency metric. However, it's still in the applied research stage with no immediate deployment.","key_impact_metrics":["24.84% improvement on flux consistency metric","54,738 star field image pairs"],"technology_tags":["Super-resolution imaging","Astronomical data analysis"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:43:30.168292Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e53e91073eda","title":"IONext: Unlocking the Next Era of Inertial Odometry","content":"arXiv:2507.17089v2 Announce Type: replace Abstract: Researchers have increasingly adopted Transformer-based models for inertial odometry. While Transformers excel at modeling long-range dependencies, their limited sensitivity to local, fine-grained motion variations and lack of inherent inductive biases often hinder localization accuracy and generalization. Recent studies have shown that incorporating large-kernel convolutions and Transformer-inspired architectural designs into CNN can effectively expand the receptive field, thereby improving global motion perception. Motivated by these insights, we propose a novel CNN-based module called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures both global motion patterns and local, fine-grained motion features from dynamic inputs. This module dynamically generates selective weights based on the input, enabling efficient multi-scale feature aggregation. To further improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU), which selectively extracts representative and task-relevant motion features in the temporal domain. This unit addresses the limitations of temporal modeling observed in existing CNN approaches. Built upon DADM and STGU, we present a new CNN-based inertial odometry backbone, named Next Era of Inertial Odometry (IONext). Extensive experiments on six public datasets demonstrate that IONext consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based methods. For instance, on the RNIN dataset, IONext reduces the average ATE by 10% and the average RTE by 12% compared to the representative model iMOT.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.17089","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.236539","language":"en","tags":["computer-science","preprints","cscv","research","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":212,"author":"Shanshan Zhang, Qi Zhang, Siyue Wang, Tianshui Wen, Liqin Wu, Ziheng Zhou, Xuemin Hong, Ao Peng, Lingxiang Zheng, Yu Yang","raw_content_length":1640,"priority":7,"update_frequency":1,"reading_time_minutes":1.06,"robust_parsing_used":true,"entities":{"organizations":["the Next Era of Inertial Odometry arXiv:2507.17089v2 Announce Type","DADM","Adaptive Dynamic Mixer","CNN","Transformer"],"persons":[],"locations":[],"monetary":[]},"char_count":1639,"language_detected":"en","key_concepts":{"key_phrases":["IONext","the Next Era","Inertial Odometry","arXiv250717089v2 Announce Type","Abstract","Researchers","Transformer-based models","inertial odometry","Transformers","modeling long-range dependencies"],"filter_categories":{"research_academic":["Researchers"],"ai_ml":["Transformers"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"IONext":2.0,"the Next Era":2.0,"Inertial Odometry":2.0,"arXiv250717089v2 Announce Type":1.0,"Abstract":1.0,"Researchers":1.0,"Transformer-based models":1.0,"inertial odometry":1.0,"Transformers":1.0,"modeling long-range dependencies":1.0}},"age_hours":2.768684277222222,"is_recent":true,"quality_score":1.0,"sentiment_score":4.742,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0516,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8979,"joy":0.0132,"surprise":0.0518,"sadness":0.012,"fear":0.0081,"anger":0.0103,"disgust":0.0068},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research presents a novel CNN-based inertial odometry backbone (IONext) that outperforms existing methods on public datasets. The concrete action is the development and testing of a new algorithm, showing a 10% reduction in average ATE and a 12% reduction in average RTE compared to iMOT on the RNIN dataset. However, it is still in the applied research stage with no evidence of real-world deployment or economic viability.","key_impact_metrics":["average ATE reduced by 10%","average RTE reduced by 12%"],"technology_tags":["inertial odometry","CNN","deep learning","robotics"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:43:33.111978Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_85c9118de6a6","title":"A Comprehensive Evaluation on Quantization Techniques for Large Language Models","content":"arXiv:2507.17417v2 Announce Type: replace Abstract: For large language models (LLMs), post-training quantization (PTQ) can significantly reduce memory footprint and computational overhead. Model quantization is rapidly evolving. Though many papers report breakthrough results, they are often evaluated under different settings because a method typically contains multiple components. Analyzing connections among existing methods is important for deeper understanding. To bridge these gaps, we conduct an extensive review of state-of-the-art methods and perform comprehensive evaluations under the same conditions for fair comparison. To our knowledge, such a fair and extensive investigation remains critically underexplored. To better understand connections, first, we decouple published quantization methods into two steps: pre-quantization transformation and quantization error mitigation. The former is a preprocessing step that reduces outlier impact by flattening the data distribution; the latter offsets quantization errors to improve performance. Second, we evaluate and analyze the impact of different settings, including granularity and symmetry. Third, we analyze and evaluate the latest MXFP4 and NVFP4 data formats and their performance. Our experiments first demonstrate that optimized rotation and scaling yield the best pre-quantization performance, and that combining low-rank compensation with GPTQ can occasionally outperform GPTQ alone for error mitigation. Second, finer granularity improves performance but increases storage overhead. Third, we find that scaling-factor format and precision greatly affect FP4 performance, and that rotation-based strategies effective for INT4 offer limited gains for MXFP4 and NVFP4, motivating further study.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.17417","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.236935","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":229,"author":"Yutong Liu, Cairong Zhao, Guosheng Hu","raw_content_length":1767,"priority":7,"update_frequency":1,"reading_time_minutes":1.145,"robust_parsing_used":true,"entities":{"organizations":["PTQ"],"persons":[],"locations":[],"monetary":[]},"char_count":1766,"language_detected":"en","key_concepts":{"key_phrases":["A Comprehensive Evaluation","Quantization Techniques","Large Language Models","arXiv250717417v2 Announce Type","Abstract","large language models","LLMs","post-training quantization","PTQ","memory footprint"],"filter_categories":{"ai_ml":["Large Language Models","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Comprehensive Evaluation":2.0,"Quantization Techniques":2.0,"Large Language Models":2.0,"arXiv250717417v2 Announce Type":1.0,"Abstract":1.0,"large language models":1.0,"LLMs":1.0,"post-training quantization":1.0,"PTQ":1.0,"memory footprint":1.0}},"age_hours":2.7686987,"is_recent":true,"quality_score":1.0,"sentiment_score":6.25,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.25,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8598,"joy":0.0219,"surprise":0.0792,"sadness":0.008,"fear":0.0084,"anger":0.0162,"disgust":0.0066},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":8,"economic_viability":5,"deployment_readiness":4,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper evaluates quantization techniques for large language models, which can reduce memory footprint and computational overhead. This can indirectly reduce energy consumption of running LLMs, a significant contributor to carbon emissions. The research provides concrete metrics on the performance of different quantization methods, but deployment is still in the pilot stage.","key_impact_metrics":["Memory footprint reduction","Computational overhead reduction"],"technology_tags":["Model Quantization","Large Language Models","MXFP4","NVFP4"],"sdg_alignment":[7,9,13],"analyzed_at":"2025-10-29T12:43:36.431023Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b7581b26c6e9","title":"TriangleMix: Accelerating Prefilling via Decoding","content":"arXiv:2507.21526v2 Announce Type: replace Abstract: Large Language Models (LLMs) incur quadratic attention complexity with input length, creating a major time bottleneck in the prefilling stage. Existing acceleration methods largely exploit attention score sparsity by estimating blocks with high attention scores and applying dynamic sparse attention. In this work, we identify another untapped form of sparsity in the prefilling stage, namely decoding-time contribution sparsity, where many attention blocks exhibit nontrivial attention scores during prefilling yet contribute negligibly to subsequent decoding, as indicated by gradient-based analysis. Building on this observation, we propose TriangleMix, a training-free static attention pattern that uses dense attention in a subset of layers and switches to Triangle attention in the others. Extensive experiments show that TriangleMix preserves nearly lossless performance relative to dense attention while substantially reducing attention overhead in Triangle layers. For 128K inputs, Triangle attention achieves a 15.3x speedup in attention computation, significantly exceeding the acceleration of typical dynamic sparse methods (1.9x to 3.4x). Furthermore, TriangleMix can be seamlessly combined with dynamic sparsity approaches, delivering an additional 6% to 19% reduction in TTFT over using dynamic sparsity alone.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.21526","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.237313","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":182,"author":"Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu","raw_content_length":1378,"priority":7,"update_frequency":1,"reading_time_minutes":0.91,"robust_parsing_used":true,"entities":{"organizations":["TriangleMix","Decoding arXiv:2507.21526v2"],"persons":[],"locations":[],"monetary":[]},"char_count":1377,"language_detected":"en","key_concepts":{"key_phrases":["TriangleMix","the prefilling stage","Announce Type","Abstract","Large Language Models","LLMs","quadratic attention complexity","input length","a major time bottleneck","Existing acceleration methods"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"TriangleMix":2.0,"the prefilling stage":2.0,"Announce Type":1.0,"Abstract":1.0,"Large Language Models":1.0,"LLMs":1.0,"quadratic attention complexity":1.0,"input length":1.0,"a major time bottleneck":1.0,"Existing acceleration methods":1.0}},"age_hours":2.7687133088888887,"is_recent":true,"quality_score":1.0,"sentiment_score":6.806,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3612,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9015,"joy":0.0042,"surprise":0.0452,"sadness":0.0095,"fear":0.009,"anger":0.0208,"disgust":0.0098},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel method (TriangleMix) to accelerate LLM prefilling, potentially reducing energy consumption in AI training and inference. The 15.3x speedup in attention computation for 128K inputs is a concrete metric. However, it's still in the research phase with no deployed units or independent verification, hence the vaporware flag and lower scores on deployment readiness and economic viability.","key_impact_metrics":["15.3x speedup in attention computation","6% to 19% reduction in TTFT"],"technology_tags":["Large Language Models","Attention Mechanisms","AI Efficiency"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-29T12:43:39.364162Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_fd0b3f7eea97","title":"Goal","content":"arXiv:2507.23042v2 Announce Type: replace Abstract: Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2507.23042","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.238236","language":"en","tags":["cslg","csai","preprints","cscv","research","csmm","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":197,"author":"Santosh Patapati, Trisanth Srinivasan","raw_content_length":1416,"priority":7,"update_frequency":1,"reading_time_minutes":0.985,"robust_parsing_used":true,"entities":{"organizations":["NovaDrive","SPL"],"persons":["Goal arXiv:2507.23042v2 Announce Type"],"locations":["+4%"],"monetary":[]},"char_count":1415,"language_detected":"en","key_concepts":{"key_phrases":["Goal","arXiv250723042v2","Announce Type","Abstract","Autonomous vehicles","milliseconds","road geometry and traffic intent","complex situations","NovaDrive","a single-branch vision-language architecture"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Goal":2.0,"arXiv250723042v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Autonomous vehicles":1.0,"milliseconds":1.0,"road geometry and traffic intent":1.0,"complex situations":1.0,"NovaDrive":1.0,"a single-branch vision-language architecture":1.0}},"age_hours":2.7687431575,"is_recent":true,"quality_score":0.7,"sentiment_score":2.798,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4404,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9288,"joy":0.0065,"surprise":0.0406,"sadness":0.0025,"fear":0.0066,"anger":0.0115,"disgust":0.0034},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel autonomous vehicle architecture (NovaDrive) that improves driving performance, specifically increasing success rate to 84% and reducing collision frequency to 1.2% on the nuScenes/Waymo subset of the MD-NEX Outdoor benchmark. The improved path efficiency, resulting from the smoothness loss, suggests potential for lower fuel or battery usage. However, it is still in the applied research stage and lacks real-world deployment data, limiting its deployment readiness and economic viability.","key_impact_metrics":["Success rate increase to 84%","Collision frequency reduction to 1.2%"],"technology_tags":["Autonomous vehicles","Vision-language models","HD mapping","LiDAR"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-29T12:43:43.042601Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ea9101b119b2","title":"Agentic large language models improve retrieval","content":"arXiv:2508.00743v3 Announce Type: replace Abstract: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose radiology Retrieval and Reasoning (RaR), a multi-step retrieval and reasoning framework designed to improve diagnostic accuracy, factual consistency, and clinical reliability of LLMs in radiology question answering. We evaluated 25 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. To assess generalizability, we additionally tested on an unseen internal dataset of 65 real-world radiology board examination questions. RaR significantly improved mean diagnostic accuracy over zero-shot prompting and conventional online RAG. The greatest gains occurred in small-scale models, while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models showed gains from RaR (e.g., MedGemma-27B), indicating that retrieval remains beneficial despite embedded domain knowledge. These results highlight the potential of RaR to enhance factuality and diagnostic accuracy in radiology QA, warranting future studies to validate their clinical utility. All datasets, code, and the full RaR framework are publicly available to support open research and clinical translation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.00743","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.240207","language":"en","tags":["computer-science","cslg","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":237,"author":"Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald K\\\"ostler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh","raw_content_length":1922,"priority":7,"update_frequency":1,"reading_time_minutes":1.185,"robust_parsing_used":true,"entities":{"organizations":["RSNA","0.5B"],"persons":[],"locations":[],"monetary":[]},"char_count":1921,"language_detected":"en","key_concepts":{"key_phrases":["Agentic large language models","retrieval","arXiv250800743v3 Announce Type","Abstract","Clinical decision-making","radiology","artificial intelligence","large language models","LLMs","traditional retrieval-augmented generation"],"filter_categories":{"ai_ml":["Agentic large language models","artificial intelligence","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Agentic large language models":2.0,"retrieval":2.0,"arXiv250800743v3 Announce Type":1.0,"Abstract":1.0,"Clinical decision-making":1.0,"radiology":1.0,"artificial intelligence":1.0,"large language models":1.0,"LLMs":1.0,"traditional retrieval-augmented generation":1.0}},"age_hours":2.7688132358333335,"is_recent":true,"quality_score":1.0,"sentiment_score":9.36,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.872,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8307,"joy":0.0119,"surprise":0.0543,"sadness":0.0128,"fear":0.0392,"anger":0.0344,"disgust":0.0168},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on improving LLMs for radiology QA. While it shows improved diagnostic accuracy and reduced hallucinations (mean 9.4%), it is still in the research phase with no deployed units or customer contracts. The impact on climate is indirect, potentially reducing resource consumption in healthcare, but not directly addressing GHG emissions.","key_impact_metrics":["Hallucination reduction 9.4%","Clinically relevant context retrieval 46%"],"technology_tags":["Large Language Models","Retrieval-Augmented Generation","Radiology Question Answering"],"sdg_alignment":[3,9],"analyzed_at":"2025-10-29T12:43:46.804065Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2c858436c8a3","title":"Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning","content":"arXiv:2508.01181v2 Announce Type: replace Abstract: Despite their strong performance in multimodal emotion reasoning, existing Multimodal Large Language Models (MLLMs) often overlook the scenarios involving emotion conflicts, where emotional cues from different modalities are inconsistent. To fill this gap, we first introduce CA-MER, a new benchmark designed to examine MLLMs under realistic emotion conflicts. It consists of three subsets: video-aligned, audio-aligned, and consistent, where only one or all modalities reflect the true emotion. However, evaluations on our CA-MER reveal that current state-of-the-art emotion MLLMs systematically over-rely on audio signal during emotion conflicts, neglecting critical cues from visual modality. To mitigate this bias, we propose MoSEAR, a parameter-efficient framework that promotes balanced modality integration. MoSEAR consists of two modules: (1)MoSE, modality-specific experts with a regularized gating mechanism that reduces modality bias in the fine-tuning heads; and (2)AR, an attention reallocation mechanism that rebalances modality contributions in frozen backbones during inference. Our framework offers two key advantages: it mitigates emotion conflicts and improves performance on consistent samples-without incurring a trade-off between audio and visual modalities. Experiments on multiple benchmarks-including MER2023, EMER, DFEW, and our CA-MER-demonstrate that MoSEAR achieves state-of-the-art performance, particularly under modality conflict conditions.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.01181","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.240630","language":"en","tags":["csai","eessas","preprints","cscv","research","cssd","csmm","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Zhiyuan Han, Beier Zhu, Yanlong Xu, Peipei Song, Xun Yang","raw_content_length":1526,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["Multimodal Large Language Models","MoSEAR"],"persons":[],"locations":[],"monetary":[]},"char_count":1525,"language_detected":"en","key_concepts":{"key_phrases":["Benchmarking and Bridging Emotion Conflicts","Multimodal Emotion Reasoning","MLLMs","arXiv250801181v2 Announce Type","Abstract","their strong performance","multimodal emotion reasoning","existing Multimodal Large Language Models","the scenarios","emotion conflicts"],"filter_categories":{"ai_ml":["MLLMs","existing Multimodal Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Benchmarking and Bridging Emotion Conflicts":2.0,"Multimodal Emotion Reasoning":2.0,"MLLMs":2.0,"arXiv250801181v2 Announce Type":1.0,"Abstract":1.0,"their strong performance":1.0,"multimodal emotion reasoning":1.0,"existing Multimodal Large Language Models":1.0,"the scenarios":1.0,"emotion conflicts":1.0}},"age_hours":2.7688284886111107,"is_recent":true,"quality_score":1.0,"sentiment_score":0.8195000000000002,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8361,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8945,"joy":0.0033,"surprise":0.0112,"sadness":0.0063,"fear":0.0262,"anger":0.0356,"disgust":0.0229},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new benchmark (CA-MER) and a framework (MoSEAR) for improving multimodal emotion reasoning in AI systems. While the research is technically sound and shows improved performance on existing benchmarks, it's still in the applied research stage with no clear path to deployment or direct environmental impact. The impact is limited to improving AI accuracy, which may indirectly contribute to sustainability in other applications.","key_impact_metrics":["Improved performance on MER2023","Improved performance on EMER"],"technology_tags":["Multimodal Large Language Models","Emotion Reasoning","Artificial Intelligence"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:43:50.772408Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_562e78917d8a","title":"Context Guided Transformer Entropy Modeling for Video Compression","content":"arXiv:2508.01852v2 Announce Type: replace Abstract: Conditional entropy models effectively leverage spatio-temporal contexts to reduce video redundancy. However, incorporating temporal context often introduces additional model complexity and increases computational cost. In parallel, many existing spatial context models lack explicit modeling the ordering of spatial dependencies, which may limit the availability of relevant context during decoding. To address these issues, we propose the Context Guided Transformer (CGT) entropy model, which estimates probability mass functions of the current frame conditioned on resampled temporal context and dependency-weighted spatial context. A temporal context resampler learns predefined latent queries to extract critical temporal information using transformer encoders, reducing downstream computational overhead. Meanwhile, a teacher-student network is designed as dependency-weighted spatial context assigner to explicitly model the dependency of spatial context order. The teacher generates an attention map to represent token importance and an entropy map to reflect prediction certainty from randomly masked inputs, guiding the student to select the weighted top-k tokens with the highest spatial dependency. During inference, only the student is used to predict undecoded tokens based on high-dependency context. Experimental results demonstrate that our CGT model reduces entropy modeling time by approximately 65% and achieves an 11% BD-Rate reduction compared to the previous state-of-the-art conditional entropy model.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.01852","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.241410","language":"en","tags":["computer-science","preprints","cscv","csmm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":204,"author":"Junlong Tong, Wei Zhang, Yaohui Jin, Xiaoyu Shen","raw_content_length":1578,"priority":7,"update_frequency":1,"reading_time_minutes":1.02,"robust_parsing_used":true,"entities":{"organizations":["Context Guided Transformer Entropy Modeling","the Context Guided Transformer","CGT"],"persons":[],"locations":[],"monetary":[]},"char_count":1577,"language_detected":"en","key_concepts":{"key_phrases":["Context Guided Transformer Entropy Modeling","Video Compression","arXiv250801852v2","Announce Type","Abstract","Conditional entropy models","video redundancy","temporal context","additional model complexity","computational cost"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Context Guided Transformer Entropy Modeling":2.0,"Video Compression":2.0,"arXiv250801852v2":1.0,"Announce Type":1.0,"Abstract":1.0,"Conditional entropy models":1.0,"video redundancy":1.0,"temporal context":1.0,"additional model complexity":1.0,"computational cost":1.0}},"age_hours":2.7688570680555555,"is_recent":true,"quality_score":1.0,"sentiment_score":5.7655,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8923,"joy":0.0055,"surprise":0.0368,"sadness":0.022,"fear":0.0116,"anger":0.0178,"disgust":0.0139},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving video compression, which can reduce energy consumption associated with video streaming and storage. The article presents experimental results showing a 65% reduction in entropy modeling time and an 11% BD-Rate reduction compared to previous methods. While promising, this is still in the research phase with no indication of real-world deployment.","key_impact_metrics":["65% reduction in entropy modeling time","11% BD-Rate reduction"],"technology_tags":["video compression","entropy modeling","transformer networks"],"sdg_alignment":[7,9,12],"analyzed_at":"2025-10-29T12:43:53.743407Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_15a8e8c390f7","title":"StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding","content":"arXiv:2508.01875v3 Announce Type: replace Abstract: Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.01875","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.241828","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":211,"author":"Haolin Yang, Feilong Tang, Lingxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak","raw_content_length":1695,"priority":7,"update_frequency":1,"reading_time_minutes":1.055,"robust_parsing_used":true,"entities":{"organizations":["StreamAgent"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1694,"language_detected":"en","key_concepts":{"key_phrases":["StreamAgent","Anticipatory Agents","Streaming Video Understanding","arXiv250801875v3 Announce Type","Real-time streaming video understanding","domains","autonomous driving","intelligent surveillance","challenges","conventional offline video processing"],"filter_categories":{"ai_ml":["domains"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"StreamAgent":2.0,"Anticipatory Agents":2.0,"Streaming Video Understanding":2.0,"arXiv250801875v3 Announce Type":1.0,"Real-time streaming video understanding":1.0,"domains":1.0,"autonomous driving":1.0,"intelligent surveillance":1.0,"challenges":1.0,"conventional offline video processing":1.0}},"age_hours":2.7688707280555556,"is_recent":true,"quality_score":1.0,"sentiment_score":9.3125,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8625,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.4627,"joy":0.0308,"surprise":0.041,"sadness":0.007,"fear":0.4237,"anger":0.0268,"disgust":0.0079},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article proposes a new method for real-time streaming video understanding, which could potentially improve the efficiency of systems like autonomous driving and intelligent surveillance. The method claims to outperform existing methods in response accuracy and real-time efficiency, but it is still in the early stages of development with no deployed units or customer contracts mentioned. The impact on climate is indirect, through potential improvements in energy efficiency of these systems, but not directly quantified.","key_impact_metrics":["response accuracy","real-time efficiency"],"technology_tags":["streaming video understanding","artificial intelligence","autonomous driving","intelligent surveillance"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:43:57.210114Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_68a19416ba61","title":"NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks","content":"arXiv:2508.02046v2 Announce Type: replace Abstract: Recent advances in Graphical User Interface (GUI) and embodied navigation have driven progress, yet these domains have largely evolved in isolation, with disparate datasets and training paradigms. In this paper, we observe that both tasks can be formulated as Markov Decision Processes (MDP), suggesting a foundational principle for their unification. Hence, we present NaviMaster, the first unified agent capable of unifying GUI navigation and embodied navigation within a single framework. Specifically, NaviMaster (i) proposes a visual-target trajectory collection pipeline that generates trajectories for both GUI and embodied tasks using a single formulation. (ii) employs a unified reinforcement learning framework on the mix data to improve generalization. (iii) designs a novel distance-aware reward to ensure efficient learning from the trajectories. Through extensive experiments on out-of-domain benchmarks, NaviMaster is shown to outperform state-of-the-art agents in GUI navigation, spatial affordance prediction, and embodied navigation. Ablation studies further demonstrate the efficacy of our unified training strategy, data mixing strategy, and reward design.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.02046","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.242207","language":"en","tags":["computer-science","cslg","preprints","research","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Zhihao Luo, Wentao Yan, Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, Xin Tan","raw_content_length":1229,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":["Graphical User Interface","NaviMaster","Markov Decision Processes (MDP","GUI","Embodied Navigation Tasks"],"persons":["NaviMaster"],"locations":[],"monetary":[]},"char_count":1228,"language_detected":"en","key_concepts":{"key_phrases":["GUI","a Unified Policy","Embodied Navigation Tasks","Announce Type","Abstract","Recent advances","Graphical User Interface","navigation","progress","these domains"],"filter_categories":{"ai_ml":["these domains"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"GUI":3.0,"a Unified Policy":2.0,"Embodied Navigation Tasks":2.0,"Announce Type":1.0,"Abstract":1.0,"Recent advances":1.0,"Graphical User Interface":1.0,"navigation":1.0,"progress":1.0,"these domains":1.0}},"age_hours":2.768885741944444,"is_recent":true,"quality_score":1.0,"sentiment_score":7.009499999999999,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4019,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.899,"joy":0.0196,"surprise":0.0385,"sadness":0.0053,"fear":0.0103,"anger":0.0193,"disgust":0.0079},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a unified agent, NaviMaster, for GUI and embodied navigation. While it demonstrates improved performance on benchmarks, it's still in the research phase with no deployed units or real-world impact data. The potential climate impact is indirect, relying on the downstream applications of improved navigation, and is therefore modest at this stage.","key_impact_metrics":["Outperforms state-of-the-art agents in GUI navigation","Outperforms state-of-the-art agents in spatial affordance prediction"],"technology_tags":["Reinforcement Learning","GUI Navigation","Embodied Navigation"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:44:00.446180Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_bfcaf07b3331","title":"CLIP","content":"arXiv:2508.02329v3 Announce Type: replace Abstract: Despite the success of Vision-Language Models (VLMs) like CLIP in aligning vision and language, their proficiency in detailed, fine-grained visual comprehension remains a key challenge. We present CLIP-IN, a novel framework that bolsters CLIP's fine-grained perception through two core innovations. Firstly, we leverage instruction-editing datasets, originally designed for image manipulation, as a unique source of hard negative image-text pairs. Coupled with a symmetric hard negative contrastive loss, this enables the model to effectively distinguish subtle visual-semantic differences. Secondly, CLIP-IN incorporates long descriptive captions, utilizing rotary positional encodings to capture rich semantic context often missed by standard CLIP. Our experiments demonstrate that CLIP-IN achieves substantial gains on the MMVP benchmark and various fine-grained visual recognition tasks, without compromising robust zero-shot performance on broader classification and retrieval tasks. Critically, integrating CLIP-IN's visual representations into Multimodal Large Language Models significantly reduces visual hallucinations and enhances reasoning abilities. This work underscores the considerable potential of synergizing targeted, instruction-based contrastive learning with comprehensive descriptive information to elevate the fine-grained understanding of VLMs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.02329","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.242611","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":172,"author":"Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma","raw_content_length":1421,"priority":7,"update_frequency":1,"reading_time_minutes":0.86,"robust_parsing_used":true,"entities":{"organizations":["MMVP","CLIP-IN","Vision-Language Models","CLIP"],"persons":[],"locations":[],"monetary":[]},"char_count":1420,"language_detected":"en","key_concepts":{"key_phrases":["CLIP","arXiv250802329v3 Announce Type","Abstract","the success","Vision-Language Models","VLMs","vision","language","their proficiency","detailed fine-grained visual comprehension"],"filter_categories":{"ai_ml":["vision","language"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CLIP":3.0,"arXiv250802329v3 Announce Type":1.0,"Abstract":1.0,"the success":1.0,"Vision-Language Models":1.0,"VLMs":1.0,"vision":1.0,"language":1.0,"their proficiency":1.0,"detailed fine-grained visual comprehension":1.0}},"age_hours":2.7689002925,"is_recent":true,"quality_score":0.7,"sentiment_score":2.532,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4936,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9247,"joy":0.0244,"surprise":0.0274,"sadness":0.0052,"fear":0.0053,"anger":0.0089,"disgust":0.004},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework, CLIP-IN, for improving the fine-grained perception of Vision-Language Models. The concrete action is the development and testing of this framework, with measurable outcomes demonstrated on the MMVP benchmark and various fine-grained visual recognition tasks. However, it is still in the applied research stage, with no deployment or commercialization mentioned, hence the low scores for economic viability and deployment readiness.","key_impact_metrics":["Substantial gains on the MMVP benchmark","Reduces visual hallucinations in Multimodal Large Language Models"],"technology_tags":["Vision-Language Models","Contrastive Learning","Image Recognition"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T12:44:04.611287Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a5a2f37f3a33","title":"HealthFlow: A Self","content":"arXiv:2508.02621v2 Announce Type: replace Abstract: The rapid proliferation of scientific knowledge presents a grand challenge: transforming this vast repository of information into an active engine for discovery, especially in high-stakes domains like healthcare. Current AI agents, however, are constrained by static, predefined strategies, limiting their ability to navigate the complex, evolving ecosystem of scientific research. This paper introduces HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its high-level problem-solving policies by distilling procedural successes and failures into a durable, structured knowledge base, enabling it to learn not just how to use tools, but how to strategize. To anchor our research and provide a community resource, we introduce EHRFlowBench, a new benchmark featuring complex health data analysis tasks systematically derived from peer-reviewed scientific literature. Our experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work offers a new paradigm for intelligent systems that can learn to operationalize the procedural knowledge embedded in scientific content, marking a critical step toward more autonomous and effective AI for healthcare scientific discovery.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.02621","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.243008","language":"en","tags":["cslg","csma","csai","preprints","research","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":180,"author":"Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Junjun He, Liantao Ma, Lequan Yu","raw_content_length":1393,"priority":7,"update_frequency":1,"reading_time_minutes":0.9,"robust_parsing_used":true,"entities":{"organizations":["HealthFlow"],"persons":[],"locations":[],"monetary":[]},"char_count":1392,"language_detected":"en","key_concepts":{"key_phrases":["HealthFlow","A Self","Announce Type","Abstract","The rapid proliferation","scientific knowledge","a grand challenge","this vast repository","information","an active engine"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"HealthFlow":3.0,"A Self":2.0,"Announce Type":1.0,"Abstract":1.0,"The rapid proliferation":1.0,"scientific knowledge":1.0,"a grand challenge":1.0,"this vast repository":1.0,"information":1.0,"an active engine":1.0}},"age_hours":2.768914496111111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.2775,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8555,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.803,"joy":0.0266,"surprise":0.1143,"sadness":0.0056,"fear":0.026,"anger":0.0183,"disgust":0.0062},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"HealthFlow is a self-evolving AI agent for healthcare scientific discovery. The article introduces EHRFlowBench, a new benchmark featuring complex health data analysis tasks systematically derived from peer-reviewed scientific literature, and demonstrates that HealthFlow outperforms state-of-the-art agent frameworks. It's currently in the applied research stage, with no deployment data available.","key_impact_metrics":["Significantly outperforms state-of-the-art agent frameworks"],"technology_tags":["Artificial Intelligence","Machine Learning","Healthcare","Data Analysis"],"sdg_alignment":[3,9],"analyzed_at":"2025-10-29T12:44:07.433215Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9043bbc362fb","title":"Modeling Annotator Disagreement with Demographic","content":"arXiv:2508.02853v2 Announce Type: replace Abstract: We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address sparse demographic coverage, we test whether LLM-generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.02853","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.243382","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":151,"author":"Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens","raw_content_length":1146,"priority":7,"update_frequency":1,"reading_time_minutes":0.755,"robust_parsing_used":true,"entities":{"organizations":["NLP","DEM-MoE","Demographic-Aware Mixture of Experts","Demographic arXiv:2508.02853v2"],"persons":[],"locations":[],"monetary":[]},"char_count":1145,"language_detected":"en","key_concepts":{"key_phrases":["Modeling Annotator Disagreement","Demographic","arXiv250802853v2","Announce Type","Abstract","an approach","annotator disagreement","subjective NLP tasks","both architectural and data-centric innovations","Our model"],"filter_categories":{"ai_ml":["subjective NLP tasks"],"business_innovation":["both architectural and data-centric innovations"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Modeling Annotator Disagreement":2.0,"Demographic":2.0,"arXiv250802853v2":1.0,"Announce Type":1.0,"Abstract":1.0,"an approach":1.0,"annotator disagreement":1.0,"subjective NLP tasks":1.0,"both architectural and data-centric innovations":1.0,"Our model":1.0}},"age_hours":2.768929048611111,"is_recent":true,"quality_score":1.0,"sentiment_score":3.634,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.2732,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8772,"joy":0.0154,"surprise":0.0126,"sadness":0.0049,"fear":0.0341,"anger":0.0353,"disgust":0.0205},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":5,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a model for improving the representation of diverse perspectives in subjective NLP tasks by addressing annotator disagreement. While the model shows promise in improving performance across demographic groups and utilizes synthetic data for data imputation, it remains in the research phase with no concrete deployment or measurable outcomes related to climate impact. The 'moderate' alignment of synthetic judgments with human annotations is a metric, but not a sustainability metric.","key_impact_metrics":["moderate alignment with human annotations"],"technology_tags":["NLP","Machine Learning","Demographic-Aware Modeling"],"sdg_alignment":[5,10],"analyzed_at":"2025-10-29T12:44:10.585085Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6379d6e10e36","title":"Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers","content":"arXiv:2508.03014v2 Announce Type: replace Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, and their integration with Extended Reality (XR) is poised to transform how users interact with immersive environments. This survey provides a comprehensive review of recent developments at the intersection of LLMs and XR, offering a structured organization of research along both technical and application dimensions. We propose a taxonomy of LLM-enhanced XR systems centered on key technical paradigms -- such as interactive agent control, XR development toolkits, and generative scene synthesis -- and discuss how these paradigms enable novel capabilities in XR. In parallel, we examine how LLM-driven techniques support practical XR applications across diverse domains, including immersive education, clinical healthcare, and industrial manufacturing. By connecting these technical paradigms with application frontiers, our survey highlights current trends, delineates design considerations, and identifies open challenges in building LLM-augmented XR systems. This work provides insights that can guide researchers and practitioners in advancing the state of the art in intelligent XR experiences.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.03014","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.243794","language":"en","tags":["preprints","research","computer-science","cshc","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":169,"author":"Jingyan Wang, Yang Zhao, Haotian Mao, Xubo Yang","raw_content_length":1276,"priority":7,"update_frequency":1,"reading_time_minutes":0.845,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1275,"language_detected":"en","key_concepts":{"key_phrases":["Large Language Models","Extended Reality","Survey","Technical Paradigms","Application","Frontiers","LLMs","arXiv250803014v2","Announce Type","remarkable capabilities"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Large Language Models":3.0,"Extended Reality":3.0,"Survey":2.0,"Technical Paradigms":2.0,"Application":2.0,"Frontiers":2.0,"LLMs":2.0,"arXiv250803014v2":1.0,"Announce Type":1.0,"remarkable capabilities":1.0}},"age_hours":2.7689431333333334,"is_recent":true,"quality_score":0.7,"sentiment_score":9.221,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8442,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.553,"joy":0.0691,"surprise":0.2864,"sadness":0.0053,"fear":0.057,"anger":0.0215,"disgust":0.0077},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article is a survey paper, reviewing the intersection of LLMs and XR. It identifies technical paradigms and application frontiers, but does not present any concrete deployments or measured outcomes related to sustainability. The vaporware flag is set because it discusses early-stage concepts without evidence of deployment.","key_impact_metrics":[],"technology_tags":["Large Language Models","Extended Reality"],"sdg_alignment":[],"analyzed_at":"2025-10-29T12:44:14.456361Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_57b1d5449dd8","title":"Training Long","content":"arXiv:2508.03501v2 Announce Type: replace Abstract: Research on applications of reinforcement learning (RL) to large language models has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn Markov decision processes (MDPs), this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Our methodology begins with rejection fine-tuning (RFT) using execution feedback to train a policy to follow instructions and formatting effectively, followed by a synchronous RL pipeline using DAPO for iterative improvement. Applying this pipeline to Qwen2.5-72B-Instruct, we increase its Pass@1 on the SWE-bench Verified benchmark from 11% to 39%, substantially improving upon the 20% RFT baseline. On the May and June splits of SWE-rebench, the resulting agent achieves Pass@1 of 35% and 31% respectively, competitive with even larger models such as DeepSeek-V3-0324 or Qwen3-235B-A22B, demonstrating that our methodology offers a practical approach for training capable agents for multi-turn interactive tasks using open-weight models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.03501","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:39.244611","language":"en","tags":["computer-science","cslg","preprints","cscl","csse","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, Boris Yangel","raw_content_length":1498,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["RFT"],"persons":["Markov"],"locations":[],"monetary":[]},"char_count":1497,"language_detected":"en","key_concepts":{"key_phrases":["Training Long","Announce Type","Abstract","Research","applications","reinforcement learning","large language models","single-turn problems","mathematical reasoning","single-shot code generation"],"filter_categories":{"ai_ml":["Training Long","reinforcement learning","large language models"],"healthcare_tech":["Research"],"research_academic":["Research"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Training Long":2.0,"Announce Type":1.0,"Abstract":1.0,"Research":1.0,"applications":1.0,"reinforcement learning":1.0,"large language models":1.0,"single-turn problems":1.0,"mathematical reasoning":1.0,"single-shot code generation":1.0}},"age_hours":2.768972874166667,"is_recent":true,"quality_score":1.0,"sentiment_score":1.9379999999999997,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6124,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8403,"joy":0.007,"surprise":0.0603,"sadness":0.0193,"fear":0.0073,"anger":0.0291,"disgust":0.0368},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes research improving the performance of large language models in software engineering tasks. While improved software engineering could indirectly lead to more efficient resource use, there is no direct, measurable climate impact at this stage. The research is in the applied research phase, with improved performance metrics but no real-world deployment.","key_impact_metrics":["Pass@1 on SWE-bench Verified benchmark increased from 11% to 39%","Pass@1 of 35% and 31% on May and June splits of SWE-rebench"],"technology_tags":["Reinforcement Learning","Large Language Models"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T12:44:19.455808Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
