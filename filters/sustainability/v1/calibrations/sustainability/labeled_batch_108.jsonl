{"id":"science_arxiv_cs_2644f81669ed","title":"Remote Interference Mitigation through Null Precoding and Fractional Programming","content":"arXiv:2510.09989v1 Announce Type: new Abstract: With the rapid deployment of 5G systems, remote interference (RI) caused by atmospheric ducting has emerged as an occasional, but critical challenge. This phenomenon occurs when the downlink (DL) signals from distant base stations (BSs) propagate over long distances through tropospheric ducting, severely disrupting uplink (UL) reception at local BSs. To address this challenge, we analyze the effect of RI on network performance, including the channel estimation phase. We then develop a solution that identifies the angle-of-arrival (AOA) estimation of RI and designs precoders and combiners that mitigate RI. Our approach employs interference cancellation techniques through null precoding and fractional programming which enhance the performance of the network. Interestingly, we show that using our scheme, uplink communication is possible at low transmit power regimes that were unusable due to RI. Our results further show a 5.23~dB reduction in normalized mean square error for channel estimation and achieved data rates around 5.8~bit/s/Hz at the previously unusable low uplink transmit power conditions.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.09989","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.608035","language":"en","tags":["computer-science","preprints","mathit","csit","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":165,"author":"Xuyang Sun, Hussein A. Ammar, Israfil Bahceci, Raviraj Adve, Gary Boudreau, Zehua Li","raw_content_length":1163,"priority":7,"update_frequency":1,"reading_time_minutes":0.825,"robust_parsing_used":true,"entities":{"organizations":["Null Precoding and Fractional Programming arXiv:2510.09989v1 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1162,"language_detected":"en","key_concepts":{"key_phrases":["Remote Interference Mitigation","Null Precoding","Fractional Programming","arXiv251009989v1 Announce Type","new Abstract","the rapid deployment","5G systems","remote interference","atmospheric ducting","an occasional but critical challenge"],"filter_categories":{"engineering":["Fractional Programming"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Remote Interference Mitigation":2.0,"Null Precoding":2.0,"Fractional Programming":2.0,"arXiv251009989v1 Announce Type":1.0,"new Abstract":1.0,"the rapid deployment":1.0,"5G systems":1.0,"remote interference":1.0,"atmospheric ducting":1.0,"an occasional but critical challenge":1.0}},"age_hours":2.7472738583333336,"is_recent":true,"quality_score":1.0,"sentiment_score":1.4450000000000003,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.711,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8639,"joy":0.003,"surprise":0.0381,"sadness":0.0135,"fear":0.0221,"anger":0.0421,"disgust":0.0174},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to mitigating remote interference in 5G systems, which could potentially reduce energy consumption by enabling uplink communication at lower transmit power. The research provides quantified metrics such as a 5.23 dB reduction in normalized mean square error for channel estimation and achieved data rates around 5.8 bit/s/Hz. However, it is currently in the applied research stage, with no evidence of deployment or commercial viability.","key_impact_metrics":["5.23 dB reduction in normalized mean square error","5.8 bit/s/Hz data rates"],"technology_tags":["Null Precoding","Fractional Programming","Interference Mitigation","5G"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:04:48.932991Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c6d7d0e26677","title":"FlareX: A Physics","content":"arXiv:2510.09995v1 Announce Type: new Abstract: Lens flare occurs when shooting towards strong light sources, significantly degrading the visual quality of images. Due to the difficulty in capturing flare-corrupted and flare-free image pairs in the real world, existing datasets are typically synthesized in 2D by overlaying artificial flare templates onto background images. However, the lack of flare diversity in templates and the neglect of physical principles in the synthesis process hinder models trained on these datasets from generalizing well to real-world scenarios. To address these challenges, we propose a new physics-informed method for flare data generation, which consists of three stages: parameterized template creation, the laws of illumination-aware 2D synthesis, and physical engine-based 3D rendering, which finally gives us a mixed flare dataset that incorporates both 2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D scenes. Furthermore, we design a masking approach to obtain real-world flare-free images from their corrupted counterparts to measure the performance of the model on real-world images. Extensive experiments demonstrate the effectiveness of our method and dataset.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.09995","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.608828","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":187,"author":"Lishen Qu, Zhihao Liu, Jinshan Pan, Shihao Zhou, Jinglei Shi, Duosheng Chen, Jufeng Yang","raw_content_length":1314,"priority":7,"update_frequency":1,"reading_time_minutes":0.935,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1313,"language_detected":"en","key_concepts":{"key_phrases":["FlareX","A Physics","arXiv251009995v1 Announce Type","new Abstract","Lens flare","strong light sources","the visual quality","images","the difficulty","flare-corrupted and flare-free image pairs"],"filter_categories":{"ai_ml":["flare-corrupted and flare-free image pairs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"FlareX":2.0,"A Physics":2.0,"arXiv251009995v1 Announce Type":1.0,"new Abstract":1.0,"Lens flare":1.0,"strong light sources":1.0,"the visual quality":1.0,"images":1.0,"the difficulty":1.0,"flare-corrupted and flare-free image pairs":1.0}},"age_hours":2.747304180277778,"is_recent":true,"quality_score":0.7,"sentiment_score":0.9899999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.802,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.678,"joy":0.0065,"surprise":0.0648,"sadness":0.0758,"fear":0.0255,"anger":0.0583,"disgust":0.0911},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new method for generating realistic lens flare data for training image processing models. While it addresses a technical challenge in image processing, it has no direct or immediate impact on climate change, resource use, or social equity. The dataset creation and masking approach are concrete actions, but the application is primarily focused on improving image quality.","key_impact_metrics":["9,500 2D templates","3,000 flare image pairs"],"technology_tags":["image processing","data generation","computer vision"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:04:52.055604Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1d97c6ae1aad","title":"BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes","content":"arXiv:2510.09996v1 Announce Type: new Abstract: Flicker artifacts in short-exposure images are caused by the interplay between the row-wise exposure mechanism of rolling shutter cameras and the temporal intensity variations of alternating current (AC)-powered lighting. These artifacts typically appear as uneven brightness distribution across the image, forming noticeable dark bands. Beyond compromising image quality, this structured noise also affects high-level tasks, such as object detection and tracking, where reliable lighting is crucial. Despite the prevalence of flicker, the lack of a large-scale, realistic dataset has been a significant barrier to advancing research in flicker removal. To address this issue, we present BurstDeflicker, a scalable benchmark constructed using three complementary data acquisition strategies. First, we develop a Retinex-based synthesis pipeline that redefines the goal of flicker removal and enables controllable manipulation of key flicker-related attributes (e.g., intensity, area, and frequency), thereby facilitating the generation of diverse flicker patterns. Second, we capture 4,000 real-world flicker images from different scenes, which help the model better understand the spatial and temporal characteristics of real flicker artifacts and generalize more effectively to wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we propose a green-screen method to incorporate motion into image pairs while preserving real flicker degradation. Comprehensive experiments demonstrate the effectiveness of our dataset and its potential to advance research in flicker removal.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.09996","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.609243","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":220,"author":"Lishen Qu, Zhihao Liu, Shihao Zhou, Yaqi Luo, Jie Liang, Hui Zeng, Lei Zhang, Jufeng Yang","raw_content_length":1649,"priority":7,"update_frequency":1,"reading_time_minutes":1.1,"robust_parsing_used":true,"entities":{"organizations":["Retinex"],"persons":["BurstDeflicker"],"locations":[],"monetary":[]},"char_count":1648,"language_detected":"en","key_concepts":{"key_phrases":["BurstDeflicker","A Benchmark Dataset","Flicker Removal","Dynamic Scenes","arXiv251009996v1 Announce Type","new Abstract","Flicker artifacts","short-exposure images","the interplay","the row-wise exposure mechanism"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"BurstDeflicker":2.0,"A Benchmark Dataset":2.0,"Flicker Removal":2.0,"Dynamic Scenes":2.0,"arXiv251009996v1 Announce Type":1.0,"new Abstract":1.0,"Flicker artifacts":1.0,"short-exposure images":1.0,"the interplay":1.0,"the row-wise exposure mechanism":1.0}},"age_hours":2.747319579444444,"is_recent":true,"quality_score":1.0,"sentiment_score":8.1845,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6369,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.713,"joy":0.0025,"surprise":0.0238,"sadness":0.0219,"fear":0.0521,"anger":0.0601,"disgust":0.1266},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new dataset for flicker removal in images, which could indirectly improve the accuracy of object detection and tracking in dynamic scenes. This could potentially benefit applications like smart grids or autonomous vehicles, but the direct climate impact is minimal at this stage. The dataset includes 4,000 real-world flicker images, suggesting some level of data collection and analysis, but it is still in the applied research phase.","key_impact_metrics":["4,000 real-world flicker images","intensity, area, and frequency of flicker"],"technology_tags":["image processing","flicker removal","computer vision"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:04:55.162547Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_49da5e126b20","title":"CLoD-GS: Continuous Level","content":"arXiv:2510.09997v1 Announce Type: new Abstract: Level of Detail (LoD) is a fundamental technique in real-time computer graphics for managing the rendering costs of complex scenes while preserving visual fidelity. Traditionally, LoD is implemented using discrete levels (DLoD), where multiple, distinct versions of a model are swapped out at different distances. This long-standing paradigm, however, suffers from two major drawbacks: it requires significant storage for multiple model copies and causes jarring visual ``popping\" artifacts during transitions, degrading the user experience. We argue that the explicit, primitive-based nature of the emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm: Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality scaling within a single, unified model, thereby circumventing the core problems of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a continuous LoD mechanism directly into a 3DGS representation. Our method introduces a learnable, distance-dependent decay parameter for each Gaussian primitive, which dynamically adjusts its opacity based on viewpoint proximity. This allows for the progressive and smooth filtering of less significant primitives, effectively creating a continuous spectrum of detail within one model. To train this model to be robust across all distances, we introduce a virtual distance scaling mechanism and a novel coarse-to-fine training strategy with rendered point count regularization. Our approach not only eliminates the storage overhead and visual artifacts of discrete methods but also reduces the primitive count and memory footprint of the final model. Extensive experiments demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a single model, delivering high-fidelity results across a wide range of performance targets.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.09997","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.609697","language":"en","tags":["computer-science","preprints","cscv","csgr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":262,"author":"Zhigang Cheng, Mingchao Sun, Yu Liu, Zengye Ge, Luyang Tang, Mu Xu, Yangyan Li, Peng Pan","raw_content_length":1896,"priority":7,"update_frequency":1,"reading_time_minutes":1.31,"robust_parsing_used":true,"entities":{"organizations":["CLoD-GS","Continuous Level arXiv:2510.09997v1 Announce Type","3DGS","LoD","CLoD","DLOD"],"persons":["Continuous LoD"],"locations":[],"monetary":[]},"char_count":1895,"language_detected":"en","key_concepts":{"key_phrases":["CLoD-GS Continuous Level","LoD","arXiv251009997v1 Announce Type","new Abstract","Level","Detail","a fundamental technique","real-time computer graphics","the rendering costs","complex scenes"],"filter_categories":{"ai_ml":["Detail"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CLoD-GS Continuous Level":2.0,"LoD":2.0,"arXiv251009997v1 Announce Type":1.0,"new Abstract":1.0,"Level":1.0,"Detail":1.0,"a fundamental technique":1.0,"real-time computer graphics":1.0,"the rendering costs":1.0,"complex scenes":1.0}},"age_hours":2.747335777222222,"is_recent":true,"quality_score":1.0,"sentiment_score":3.409,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3182,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9366,"joy":0.0038,"surprise":0.0092,"sadness":0.0074,"fear":0.011,"anger":0.0136,"disgust":0.0184},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (CLoD-GS) for continuous level of detail in 3D Gaussian Splatting, aiming to reduce storage and visual artifacts. The concrete action is the development of a learnable, distance-dependent decay parameter for Gaussian primitives. While it reduces primitive count and memory footprint, the climate impact is indirect through potential energy savings in rendering, and the technology is still in the applied research stage.","key_impact_metrics":["Reduced primitive count","Reduced memory footprint"],"technology_tags":["3D Gaussian Splatting","Level of Detail","Computer Graphics"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T11:04:58.330562Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f5d5e0247f6e","title":"Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks","content":"arXiv:2510.10000v1 Announce Type: new Abstract: Wasserstein distributionally robust optimization (WDRO) provides a framework for adversarial robustness, yet existing methods based on global Lipschitz continuity or strong duality often yield loose upper bounds or require prohibitive computation. In this work, we address these limitations by introducing a primal approach and adopting a notion of exact Lipschitz certificate to tighten this upper bound of WDRO. In addition, we propose a novel Wasserstein distributional attack (WDA) that directly constructs a candidate for the worst-case distribution. Compared to existing point-wise attack and its variants, our WDA offers greater flexibility in the number and location of attack points. In particular, by leveraging the piecewise-affine structure of ReLU networks on their activation cells, our approach results in an exact tractable characterization of the corresponding WDRO problem. Extensive evaluations demonstrate that our method achieves competitive robust accuracy against state-of-the-art baselines while offering tighter certificates than existing methods. Our code is available at https://github.com/OLab-Repo/WDA","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10000","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.610092","language":"en","tags":["statml","cslg","preprints","research","mathoc","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":156,"author":"Bach C. Le, Tung V. Dao, Binh T. Nguyen, Hong T. M. Chu","raw_content_length":1179,"priority":7,"update_frequency":1,"reading_time_minutes":0.78,"robust_parsing_used":true,"entities":{"organizations":["Tight Robustness Certificates","ReLU","WDRO","WDA"],"persons":["Lipschitz","Wasserstein"],"locations":[],"monetary":[]},"char_count":1178,"language_detected":"en","key_concepts":{"key_phrases":["Tight Robustness Certificates","Wasserstein Distributional Attacks","Deep Neural Networks","WDRO","arXiv251010000v1","Announce Type","new Abstract","Wasserstein distributionally robust optimization","a framework","adversarial robustness"],"filter_categories":{"ai_ml":["Deep Neural Networks"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Tight Robustness Certificates":2.0,"Wasserstein Distributional Attacks":2.0,"Deep Neural Networks":2.0,"WDRO":2.0,"arXiv251010000v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Wasserstein distributionally robust optimization":1.0,"a framework":1.0,"adversarial robustness":1.0}},"age_hours":2.747350258611111,"is_recent":true,"quality_score":1.0,"sentiment_score":5.7655,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8764,"joy":0.0042,"surprise":0.0149,"sadness":0.0045,"fear":0.0429,"anger":0.0382,"disgust":0.0191},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for improving the robustness of deep neural networks, potentially leading to more reliable AI systems. While the research shows tighter robustness certificates compared to existing methods, it is still in the early stages of development (basic research) with no deployed units or real-world data. The impact on climate is indirect, as more robust AI could potentially improve efficiency in various sectors, but this is not quantified.","key_impact_metrics":["Competitive robust accuracy against state-of-the-art baselines","Tighter certificates than existing methods"],"technology_tags":["Deep Neural Networks","Adversarial Robustness","Wasserstein Distributionally Robust Optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:05:01.872967Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_dc8011c5487c","title":"Deliberative Dynamics and Value Alignment in LLM Debates","content":"arXiv:2510.10002v1 Announce Type: new Abstract: As large language models (LLMs) are increasingly deployed in sensitive everyday contexts - offering personal advice, mental health support, and moral guidance - understanding their elicited values in navigating complex moral reasoning is essential. Most evaluations study this sociotechnical alignment through single-turn prompts, but it is unclear if these findings extend to multi-turn settings where values emerge through dialogue, revision, and consensus. We address this gap using LLM debate to examine deliberative dynamics and value alignment in multi-turn settings by prompting subsets of three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively assign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\" community. We use both synchronous (parallel responses) and round-robin (sequential responses) formats to test order effects and verdict revision. Our findings show striking behavioral differences. In the synchronous setting, GPT showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were far more flexible (28-41%). Value patterns also diverged: GPT emphasized personal autonomy and direct communication, while Claude and Gemini prioritized empathetic dialogue. Certain values proved especially effective at driving verdict changes. We further find that deliberation format had a strong impact on model behavior: GPT and Gemini stood out as highly conforming relative to Claude, with their verdict behavior strongly shaped by order effects. These results show how deliberation format and model-specific behaviors shape moral reasoning in multi-turn interactions, underscoring that sociotechnical alignment depends on how systems structure dialogue as much as on their outputs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10002","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.610525","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":248,"author":"Pratik S. Sachdeva, Tom van Nuenen","raw_content_length":1794,"priority":7,"update_frequency":1,"reading_time_minutes":1.24,"robust_parsing_used":true,"entities":{"organizations":["Deliberative Dynamics","LLM","Asshole","Claude 3.7 Sonnet"],"persons":[],"locations":["Reddit"],"monetary":[]},"char_count":1793,"language_detected":"en","key_concepts":{"key_phrases":["Deliberative Dynamics","Value Alignment","LLM Debates","Announce Type","new Abstract","large language models","LLMs","personal advice","mental health support","their elicited values"],"filter_categories":{"ai_ml":["LLM Debates","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Deliberative Dynamics":2.0,"Value Alignment":2.0,"LLM Debates":2.0,"Announce Type":1.0,"new Abstract":1.0,"large language models":1.0,"LLMs":1.0,"personal advice":1.0,"mental health support":1.0,"their elicited values":1.0}},"age_hours":2.747365097222222,"is_recent":true,"quality_score":1.0,"sentiment_score":7.1075,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4215,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9335,"joy":0.01,"surprise":0.0245,"sadness":0.0054,"fear":0.0077,"anger":0.01,"disgust":0.0089},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on LLM behavior in moral reasoning, specifically focusing on how different models revise their verdicts in multi-turn debates. While the research is technically credible and innovative, it is in an early stage of development and does not have direct, measurable climate impact or economic viability. The deployment readiness is low as it is primarily a research study.","key_impact_metrics":["revision rates (0.6-41%)"],"technology_tags":["large language models","AI ethics","moral reasoning"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:05:05.201281Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_01a623e575ce","title":"MTP-S2UT: Enhancing Speech-to","content":"arXiv:2510.10003v1 Announce Type: new Abstract: Current direct speech-to-speech translation methods predominantly employ speech tokens as intermediate representations. However, a single speech token is not dense in semantics, so we generally need multiple tokens to express a complete semantic unit. To address this limitation, we introduce multi-token prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling models to predict multiple subsequent tokens at each position, thereby capturing more complete semantics and enhancing information density per position. Initial MTP implementations apply the loss at the final layer, which improves output representation but initiates information enrichment too late. We hypothesize that advancing the information enrichment process to intermediate layers can achieve earlier and more effective enhancement of hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP loss to hidden representation where CTC loss is computed. Experiments demonstrate that all MTP loss variants consistently improve the quality of S2UT translation, with MTP-S2UT achieving the best performance.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10003","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.610923","language":"en","tags":["eessas","preprints","research","cssd","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":152,"author":"Jianjin Wang, Runsong Zhao, Xiaoqian Liu, Yuan Ge, Ziqiang Xu, Tong Xiao, Shengxiang Gao, Zhengtao Yu, Jingbo Zhu","raw_content_length":1158,"priority":7,"update_frequency":1,"reading_time_minutes":0.76,"robust_parsing_used":true,"entities":{"organizations":["MTP"],"persons":["Enhancing Speech","Announce Type"],"locations":[],"monetary":[]},"char_count":1157,"language_detected":"en","key_concepts":{"key_phrases":["-S2UT","Speech","models","arXiv251010003v1 Announce Type","new Abstract","speech","speech tokens","intermediate representations","a single speech","semantics"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"-S2UT":2.0,"Speech":2.0,"models":2.0,"arXiv251010003v1 Announce Type":1.0,"new Abstract":1.0,"speech":1.0,"speech tokens":1.0,"intermediate representations":1.0,"a single speech":1.0,"semantics":1.0}},"age_hours":2.74738029,"is_recent":true,"quality_score":1.0,"sentiment_score":2.0705,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5859,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9287,"joy":0.0087,"surprise":0.0382,"sadness":0.005,"fear":0.0077,"anger":0.0074,"disgust":0.0042},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (MTP-S2UT) for improving speech-to-speech translation by enhancing information density. While the experiments demonstrate improved translation quality, the impact on sustainability is indirect and theoretical, as it's a fundamental research project with no immediate deployment or quantifiable environmental benefit. The article mentions improved performance, but lacks details on energy consumption or resource usage.","key_impact_metrics":["Improved translation quality"],"technology_tags":["speech-to-speech translation","multi-token prediction","speech-to-unit translation"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:05:08.205663Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_8734f02c782d","title":"Bidirectional Time","content":"arXiv:2510.10004v1 Announce Type: new Abstract: Existing EEG recognition models suffer from poor cross-paradigm generalization due to dataset-specific constraints and individual variability. To overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid Network), an end-to-end unified architecture featuring robust multistream synergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive convolutions. The framework uniquely integrates: 1) Aligned time-frequency streams maintaining temporal synchronization with STFT for bidirectional modeling, 2) PTFA-based multi-scale feature enhancement amplifying critical neural patterns, 3) BiTCN with learnable fusion capturing forward/backward neural dynamics. Demonstrating enhanced robustness, BITE achieves state-of-the-art performance across four divergent paradigms (BCICIV-2A/2B, HGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject generalization. As a unified architecture, it combines robust performance across both MI and SSVEP tasks with exceptional computational efficiency. Our work validates that paradigm-aligned spectral-temporal processing is essential for reliable BCI systems. Just as its name suggests, BITE \"takes a bite out of EEG.\" The source code is available at https://github.com/cindy-hong/BiteEEG.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10004","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.611311","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":154,"author":"Jiahui Hong, Siqing Li, Muqing Jian, Luming Yang","raw_content_length":1326,"priority":7,"update_frequency":1,"reading_time_minutes":0.77,"robust_parsing_used":true,"entities":{"organizations":["PTFA","HGD","Bidirectional Time arXiv:2510.10004v1"],"persons":[],"locations":[],"monetary":[]},"char_count":1325,"language_detected":"en","key_concepts":{"key_phrases":["Bidirectional Time","arXiv251010004v1","Announce Type","new Abstract","Existing EEG recognition models","poor cross-paradigm generalization","dataset-specific constraints","individual variability","these limitations","BITE"],"filter_categories":{"ai_ml":["dataset-specific constraints"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Bidirectional Time":2.0,"arXiv251010004v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Existing EEG recognition models":1.0,"poor cross-paradigm generalization":1.0,"dataset-specific constraints":1.0,"individual variability":1.0,"these limitations":1.0,"BITE":1.0}},"age_hours":2.7473954313888886,"is_recent":true,"quality_score":1.0,"sentiment_score":3.091,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3818,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8572,"joy":0.0067,"surprise":0.0359,"sadness":0.0533,"fear":0.0151,"anger":0.013,"disgust":0.0189},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel architecture for EEG recognition, demonstrating improved accuracy and computational efficiency across different paradigms. While the technology shows promise, it is currently in the basic research phase with no deployed units or concrete actions directly impacting sustainability. The potential climate impact is minimal as it's primarily focused on improving brain-computer interfaces.","key_impact_metrics":["State-of-the-art performance across four divergent paradigms","Enhanced robustness in within-subject accuracy and cross-subject generalization"],"technology_tags":["EEG recognition","Brain-computer interface","Deep learning"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:05:11.189992Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_08266eb7c289","title":"RIPRAG: Hack a Black-box Retrieval","content":"arXiv:2510.10008v1 Announce Type: new Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. However, by injecting poisoned documents into the database of RAG systems, attackers can manipulate LLMs to generate text that aligns with their intended preferences. Existing research has primarily focused on white-box attacks against simplified RAG architectures. In this paper, we investigate a more complex and realistic scenario: the attacker lacks knowledge of the RAG system's internal composition and implementation details, and the RAG system comprises components beyond a mere retriever. Specifically, we propose the RIPRAG attack framework, an end-to-end attack pipeline that treats the target RAG system as a black box, where the only information accessible to the attacker is whether the poisoning succeeds. Our method leverages Reinforcement Learning (RL) to optimize the generation model for poisoned documents, ensuring that the generated poisoned document aligns with the target RAG system's preferences. Experimental results demonstrate that this method can effectively execute poisoning attacks against most complex RAG systems, achieving an attack success rate (ASR) improvement of up to 0.72 compared to baseline methods. This highlights prevalent deficiencies in current defensive methods and provides critical insights for LLM security research.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10008","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.611738","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Meng Xi, Sihan Lv, Yechen Jin, Guanjie Cheng, Naibo Wang, Ying Li, Jianwei Yin","raw_content_length":1496,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["Large Language Models","RAG","Retrieval-Augmented Generation"],"persons":["RAG"],"locations":[],"monetary":[]},"char_count":1495,"language_detected":"en","key_concepts":{"key_phrases":["RIPRAG","a Black-box Retrieval","LLMs","arXiv251010008v1","Announce Type","new Abstract Retrieval-Augmented Generation RAG systems","Large Language Models","a core technology","tasks","content generation"],"filter_categories":{"ai_ml":["LLMs","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"RIPRAG":2.0,"a Black-box Retrieval":2.0,"LLMs":2.0,"arXiv251010008v1":1.0,"Announce Type":1.0,"new Abstract Retrieval-Augmented Generation RAG systems":1.0,"Large Language Models":1.0,"a core technology":1.0,"tasks":1.0,"content generation":1.0}},"age_hours":2.7474105786111114,"is_recent":true,"quality_score":1.0,"sentiment_score":0.9899999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.802,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8748,"joy":0.0047,"surprise":0.0201,"sadness":0.0064,"fear":0.0175,"anger":0.0584,"disgust":0.018},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel attack framework on RAG systems, highlighting vulnerabilities in LLM security. While the research is technically sound and demonstrates a measurable attack success rate (ASR) improvement of up to 0.72, it is currently in the basic research stage and does not directly contribute to climate change mitigation or adaptation. The focus is on cybersecurity, not sustainability.","key_impact_metrics":["Attack success rate (ASR) improvement of up to 0.72"],"technology_tags":["Retrieval-Augmented Generation (RAG)","Large Language Models (LLMs)","Reinforcement Learning (RL)","Cybersecurity"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:05:14.510198Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_33e46be736de","title":"Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning","content":"arXiv:2510.10009v1 Announce Type: new Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason, search, and generate the final answer iteratively. Nevertheless, due to their limited capabilities in reasoning and search, their performance on multi-hop QA benchmarks remains far from satisfactory. To handle complex or compound queries, we train an LLM-based search agent with the native capability of query expansion through reinforcement learning. In each turn, our search agent proposes several query variants, which are searched simultaneously to cover more relevant information. Meanwhile, given limited post-training data and computing resources, it is very challenging for a search agent to master multiple tasks, including query generation, retrieved information understanding, and answer generation. Therefore, we propose incorporating a pre-trained squeezer model that helps the search agent understand the retrieved documents, allowing the search agent to focus on query generation for high retrieval recall. With the assistance of the squeezer model, we discover that even a small-scale 3B LLM can demonstrate a strong capability of query expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks. To be specific, our experiments across seven question-answering benchmarks demonstrate that our method, named ExpandSearch, achieves an average improvement of 4.4% compared to state-of-the-art baselines, with strong gains on multi-hop reasoning tasks requiring diverse evidence aggregation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10009","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.612148","language":"en","tags":["computer-science","csai","preprints","cscl","csir","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Shu Zhao, Tan Yu, Anbang Xu","raw_content_length":1549,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["Reinforcement Learning arXiv:2510.10009v1 Announce Type","Search-R1","LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1548,"language_detected":"en","key_concepts":{"key_phrases":["query expansion","the limitation","a single query","your LLM","Reinforcement Learning","search","Announce Type","new Abstract","Reasoning-augmented search agents","Search-R1"],"filter_categories":{"ai_ml":["your LLM","Reinforcement Learning"],"healthcare_tech":["search"],"research_academic":["search"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"query expansion":3.0,"the limitation":2.0,"a single query":2.0,"your LLM":2.0,"Reinforcement Learning":2.0,"search":2.0,"Announce Type":1.0,"new Abstract":1.0,"Reasoning-augmented search agents":1.0,"Search-R1":1.0}},"age_hours":2.7474265938888887,"is_recent":true,"quality_score":1.0,"sentiment_score":4.2345,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9475,"joy":0.0117,"surprise":0.0234,"sadness":0.0036,"fear":0.0038,"anger":0.0064,"disgust":0.0035},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a reinforcement learning approach to improve query expansion for search agents, resulting in a 4.4% average improvement on question-answering benchmarks. While the technology shows promise, it is still in the applied research phase with no deployment or independent verification. The impact on climate change is indirect, as it could potentially improve access to information relevant to sustainability, but there is no concrete action or measurable outcome in terms of GHG emissions reduction or carbon sequestration.","key_impact_metrics":["4.4% average improvement on QA benchmarks"],"technology_tags":["Reinforcement Learning","Query Expansion","LLM"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:05:18.128695Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6a45da6cd7a5","title":"SLEAN: Simple Lightweight Ensemble Analysis Network for Multi","content":"arXiv:2510.10010v1 Announce Type: new Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a deterministic framework for coordinating multiple LLM providers through text-based prompt orchestration. Unlike complex multi-agent systems requiring specialized infrastructure, SLEAN operates as a simple prompt bridge between LLMs using .txt templates, requiring no deep technical knowledge for deployment. The three-phase protocol formed by independent analysis, cross-critique, and arbitration, filters harmful AI-generated code suggestions before production deployment, addressing how AI-assisted debugging increasingly produces modifications that introduce unnecessary complexity, break existing functionality, or address problems. Evaluating 15 software bugs, we analyzed 69 AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95% CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied verbatim. The arbitration process reduced code change surface by 83-90% relative to raw AI outputs, enforcing minimal causal edits over scope-expanding modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1 inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus 28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems showed weak correlation with fix quality: high convergence (at least 80%) occurred in 4 of 15 cases and improved acceptance by only 2.4% points; arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although low convergence alone did not necessitate arbitration. The file-driven, provider-agnostic architecture enables deployment without specialized coding expertise, making it applicable to security auditing, code review, document verification, and other domains requiring reliable multi-provider synthesis with end-to-end auditability.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10010","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.612591","language":"en","tags":["computer-science","csai","preprints","csse","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":244,"author":"Matheus J. T. Vargas","raw_content_length":1893,"priority":7,"update_frequency":1,"reading_time_minutes":1.22,"robust_parsing_used":true,"entities":{"organizations":["SLEAN"],"persons":[],"locations":[],"monetary":[]},"char_count":1892,"language_detected":"en","key_concepts":{"key_phrases":["SLEAN","Simple Lightweight Ensemble Analysis Network","Multi","Announce Type","new Abstract","a deterministic framework","multiple LLM providers","text-based prompt orchestration","complex multi-agent systems","specialized infrastructure"],"filter_categories":{"ai_ml":["multiple LLM providers"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"SLEAN":4.0,"Simple Lightweight Ensemble Analysis Network":3.0,"Multi":2.0,"Announce Type":1.0,"new Abstract":1.0,"a deterministic framework":1.0,"multiple LLM providers":1.0,"text-based prompt orchestration":1.0,"complex multi-agent systems":1.0,"specialized infrastructure":1.0}},"age_hours":2.747441946111111,"is_recent":true,"quality_score":1.0,"sentiment_score":3.5199999999999996,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.296,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.942,"joy":0.0177,"surprise":0.0207,"sadness":0.0026,"fear":0.0059,"anger":0.0081,"disgust":0.0029},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework (SLEAN) for improving the quality of AI-generated code fixes, which could indirectly reduce energy consumption by minimizing unnecessary code complexity and improving software efficiency. The paper provides metrics on the acceptance rate of fixes and code change surface reduction, but it is still in the applied research stage with no real-world deployment. The impact on climate is indirect and difficult to quantify at this stage.","key_impact_metrics":["31.9% fix acceptance rate","83-90% code change surface reduction"],"technology_tags":["AI","LLM","Software Engineering","Code Optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:05:21.299838Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d7d7b4c1027d","title":"MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output","content":"arXiv:2510.10011v1 Announce Type: new Abstract: Currently, medical vision language models are widely used in medical vision question answering tasks. However, existing models are confronted with two issues: for input, the model only relies on text instructions and lacks direct understanding of visual clues in the image; for output, the model only gives text answers and lacks connection with key areas in the image. To address these issues, we propose a unified medical vision language model MIMO, with visual referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not only combine visual clues and textual instructions to understand complex medical images and semantics, but can also ground medical terminologies in textual output within the image. To overcome the scarcity of relevant data in the medical field, we propose MIMOSeg, a comprehensive medical multimodal dataset including 895K samples. MIMOSeg is constructed from four different perspectives, covering basic instruction following and complex question answering with multimodal input and multimodal output. We conduct experiments on several downstream medical multimodal tasks. Extensive experimental results verify that MIMO can uniquely combine visual referring and pixel grounding capabilities, which are not available in previous models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10011","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.612985","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":189,"author":"Yanyuan Chen, Dexuan Xu, Yu Huang, Songkun Zhan, Hanpin Wang, Dongxue Chen, Xueping Wang, Meikang Qiu, Hang Li","raw_content_length":1328,"priority":7,"update_frequency":1,"reading_time_minutes":0.945,"robust_parsing_used":true,"entities":{"organizations":["Multimodal Output"],"persons":["MIMO","Multimodal Input"],"locations":[],"monetary":[]},"char_count":1327,"language_detected":"en","key_concepts":{"key_phrases":["MIMO","A medical vision language model","visual referring multimodal input","pixel","multimodal output","the model","the image","arXiv251010011v1 Announce Type","new Abstract","medical vision language models"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"MIMO":2.0,"A medical vision language model":2.0,"visual referring multimodal input":2.0,"pixel":2.0,"multimodal output":2.0,"the model":2.0,"the image":2.0,"arXiv251010011v1 Announce Type":1.0,"new Abstract":1.0,"medical vision language models":1.0}},"age_hours":2.7474576436111113,"is_recent":true,"quality_score":1.0,"sentiment_score":7.4695,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4939,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7873,"joy":0.005,"surprise":0.0593,"sadness":0.0414,"fear":0.0312,"anger":0.0444,"disgust":0.0313},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes a new medical vision language model (MIMO) and a corresponding dataset (MIMOSeg). The concrete action is the creation of the model and dataset, with the measurable outcome being the 895K samples in MIMOSeg. The deployment readiness is low as it is still in the research phase, and economic viability is uncertain.","key_impact_metrics":["895K samples in MIMOSeg"],"technology_tags":["medical imaging","vision language model","artificial intelligence"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:05:24.440966Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9d842a0bff9b","title":"Path Drift in Large Reasoning Models:How First","content":"arXiv:2510.10013v1 Announce Type: new Abstract: As large language models (LLMs) are increasingly deployed for complex reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a key paradigm for structured inference. Despite early-stage safeguards enabled by alignment techniques such as RLHF, we identify a previously underexplored vulnerability: reasoning trajectories in Long-CoT models can drift from aligned paths, resulting in content that violates safety constraints. We term this phenomenon Path Drift. Through empirical analysis, we uncover three behavioral triggers of Path Drift: (1) first-person commitments that induce goal-driven reasoning that delays refusal signals; (2) ethical evaporation, where surface-level disclaimers bypass alignment checkpoints; (3) condition chain escalation, where layered cues progressively steer models toward unsafe completions. Building on these insights, we introduce a three-stage Path Drift Induction Framework comprising cognitive load amplification, self-role priming, and condition chain hijacking. Each stage independently reduces refusal rates, while their combination further compounds the effect. To mitigate these risks, we propose a path-level defense strategy incorporating role attribution correction and metacognitive reflection (reflective safety cues). Our findings highlight the need for trajectory-level alignment oversight in long-form reasoning beyond token-level alignment.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10013","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.613377","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":183,"author":"Yuyi Huang, Runzhe Zhan, Lidia S. Chao, Ailin Tao, Derek F. Wong","raw_content_length":1460,"priority":7,"update_frequency":1,"reading_time_minutes":0.915,"robust_parsing_used":true,"entities":{"organizations":["Path Drift","RLHF"],"persons":["Path Drift","Long Chain"],"locations":[],"monetary":[]},"char_count":1459,"language_detected":"en","key_concepts":{"key_phrases":["Path Drift","Large Reasoning Models","arXiv251010013v1 Announce Type","new Abstract","large language models","LLMs","complex reasoning tasks","Thought","Long-CoT","a key paradigm"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Path Drift":2.0,"Large Reasoning Models":2.0,"arXiv251010013v1 Announce Type":1.0,"new Abstract":1.0,"large language models":1.0,"LLMs":1.0,"complex reasoning tasks":1.0,"Thought":1.0,"Long-CoT":1.0,"a key paradigm":1.0}},"age_hours":2.7474734716666664,"is_recent":true,"quality_score":1.0,"sentiment_score":2.7645,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4471,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.6274,"joy":0.0059,"surprise":0.0471,"sadness":0.0202,"fear":0.2609,"anger":0.0289,"disgust":0.0095},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper identifies vulnerabilities in large language models that could lead to unsafe outputs, even with alignment techniques. The research proposes a framework to induce 'Path Drift' and a defense strategy. While it doesn't directly address climate change, it's crucial for ensuring AI systems used in sustainability contexts (e.g., climate modeling, policy analysis) remain reliable and safe.","key_impact_metrics":[],"technology_tags":["Large Language Models","AI Safety","Reasoning"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T11:05:27.066629Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3b31a9a82808","title":"End","content":"arXiv:2510.10453v1 Announce Type: new Abstract: The mismatch of speech length and text length poses a challenge in automatic speech recognition (ASR). In previous research, various approaches have been employed to align text with speech, including the utilization of Connectionist Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS) was introduced, utilizing intermediate CTC outputs to guide downsampling and preserve keyframes, but traditional methods (CTC) failed to align speech and text appropriately when downsampling speech to a text-similar length. In this paper, we focus on speech recognition in those cases where the length of speech aligns closely with that of the corresponding text. To address this issue, we introduce two methods for alignment: a) Time Independence Loss (TIL) and b) Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance the information on keyframes, we incorporate frame fusion by applying weights and summing the keyframe with its context 2 frames. Experimental results on AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods outperform the previous work and achieve a reduction of at least 86\\% in the number of frames.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10453","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.717175","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":184,"author":"Peng Fan, Wenping Wang, Fei Deng","raw_content_length":1222,"priority":7,"update_frequency":1,"reading_time_minutes":0.92,"robust_parsing_used":true,"entities":{"organizations":["Connectionist Temporal Classification","CTC","TIL","Cross Entropy"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1221,"language_detected":"en","key_concepts":{"key_phrases":["End","CTC","arXiv251010453v1 Announce Type","new Abstract","The mismatch","speech length","text length","a challenge","automatic speech recognition","ASR"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"End":2.0,"CTC":2.0,"arXiv251010453v1 Announce Type":1.0,"new Abstract":1.0,"The mismatch":1.0,"speech length":1.0,"text length":1.0,"a challenge":1.0,"automatic speech recognition":1.0,"ASR":1.0}},"age_hours":2.751213806388889,"is_recent":true,"quality_score":0.7,"sentiment_score":1.7570000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6486,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8583,"joy":0.0051,"surprise":0.047,"sadness":0.0327,"fear":0.0151,"anger":0.0247,"disgust":0.0172},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper presents two novel methods for speech recognition that reduce the number of frames needed, potentially leading to lower energy consumption for ASR applications. The reduction of at least 86% in the number of frames is the key metric. However, this is still early-stage research without deployment or economic viability data.","key_impact_metrics":["reduction of frames with 86%"],"technology_tags":["automatic speech recognition","speech recognition","Connectionist Temporal Classification"],"sdg_alignment":[7,9],"analyzed_at":"2025-10-29T11:05:29.908186Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1c0319ecd781","title":"Hybrid Robotic Meta","content":"arXiv:2510.10016v1 Announce Type: new Abstract: The agricultural sector is rapidly evolving to meet growing global food demands, yet tasks like fruit and vegetable handling remain labor-intensive, causing inefficiencies and post-harvest losses. Automation, particularly selective harvesting, offers a viable solution, with soft robotics emerging as a key enabler. This study introduces a novel hybrid gripper for tomato harvesting, incorporating a rigid outer frame with a soft auxetic internal lattice. The six-finger, 3D caging-effect design enables gentle yet secure grasping in unstructured environments. Uniquely, the work investigates the effect of auxetic lattice orientation on grasping conformability, combining experimental validation with 2D Digital Image Correlation (DIC) and nonlinear finite element analysis (FEA). Auxetic configurations with unit cell inclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their grasping forces, deformation responses, and motor torque requirements are systematically compared. Results demonstrate that lattice orientation strongly influences compliance, contact forces, and energy efficiency, with distinct advantages across configurations. This comparative framework highlights the novelty of tailoring auxetic geometries to optimize robotic gripper performance. The findings provide new insights into soft-rigid hybrid gripper design, advancing automation strategies for precision agriculture while minimizing crop damage.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10016","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.614225","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":193,"author":"Shahid Ansari, Vivek Gupta, Bishakh Bhattacharya","raw_content_length":1490,"priority":7,"update_frequency":1,"reading_time_minutes":0.965,"robust_parsing_used":true,"entities":{"organizations":["DIC"],"persons":[],"locations":[],"monetary":[]},"char_count":1489,"language_detected":"en","key_concepts":{"key_phrases":["Hybrid Robotic Meta","new Abstract","The agricultural sector","growing global food demands","tasks","fruit and vegetable handling","inefficiencies","post-harvest losses","Automation","particularly selective harvesting"],"filter_categories":{"engineering":["Automation"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hybrid Robotic Meta":2.0,"new Abstract":1.0,"The agricultural sector":1.0,"growing global food demands":1.0,"tasks":1.0,"fruit and vegetable handling":1.0,"inefficiencies":1.0,"post-harvest losses":1.0,"Automation":1.0,"particularly selective harvesting":1.0}},"age_hours":2.7475053583333335,"is_recent":true,"quality_score":1.0,"sentiment_score":7.786999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5574,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9196,"joy":0.0123,"surprise":0.0304,"sadness":0.0093,"fear":0.0036,"anger":0.0128,"disgust":0.012},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on a novel robotic gripper for tomato harvesting, focusing on the impact of auxetic lattice orientation on grasping performance. The study combines experimental validation with 2D Digital Image Correlation (DIC) and nonlinear finite element analysis (FEA), providing a level of technical credibility. However, it is still in the early stages of development, with no deployed units or economic viability demonstrated.","key_impact_metrics":["grasping forces","deformation responses"],"technology_tags":["soft robotics","precision agriculture","automation"],"sdg_alignment":[2,9,12],"analyzed_at":"2025-10-29T11:05:33.146114Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7d6746fe4c0e","title":"\"Can I Decorate My Teeth With Diamonds?\": Exploring Multi","content":"arXiv:2510.10019v1 Announce Type: new Abstract: Dental anxiety is prevalent among children, often leading to missed treatment and potential negative effects on their mental well-being. While several interventions (e.g., pharmacological and psychotherapeutic techniques) have been introduced for anxiety alleviation, the recently emerged virtual reality (VR) technology, with its immersive and playful nature, opened new opportunities for complementing and enhancing the therapeutic effects of existing interventions. In this light, we conducted a series of co-design workshops with 13 children aged 10-12 to explore how they envisioned using VR to address their fear and stress associated with dental visits, followed by interviews with parents (n = 13) and two dentists. Our findings revealed that children expected VR to provide immediate relief, social support, and a sense of control during dental treatment, parents sought educational opportunities for their children to learn about oral health, and dentists prioritized treatment efficiency and safety issues. Drawing from the findings, we discuss the considerations of multi-stakeholders for developing VR-assisted anxiety management applications for children within and beyond dental settings.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10019","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.614646","language":"en","tags":["preprints","research","computer-science","cshc","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Yaxuan Mao, Yanheng Li, Duo Gong, Pengcheng An, Yuhan Luo","raw_content_length":1252,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["Exploring Multi arXiv:2510.10019v1"],"persons":[],"locations":[],"monetary":[]},"char_count":1251,"language_detected":"en","key_concepts":{"key_phrases":["My Teeth","Diamonds","Multi","new Abstract","Dental anxiety","children","missed treatment","potential negative effects","their mental well-being","several interventions"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"My Teeth":2.0,"Diamonds":2.0,"Multi":2.0,"new Abstract":1.0,"Dental anxiety":1.0,"children":1.0,"missed treatment":1.0,"potential negative effects":1.0,"their mental well-being":1.0,"several interventions":1.0}},"age_hours":2.747520961666667,"is_recent":true,"quality_score":1.0,"sentiment_score":2.8925,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4215,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.5773,"joy":0.1964,"surprise":0.038,"sadness":0.0261,"fear":0.1434,"anger":0.0121,"disgust":0.0067},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes co-design workshops and interviews to explore the potential of VR for managing dental anxiety in children. While the research identifies needs and considerations for VR application development, it does not present any deployed technology or measured outcomes related to sustainability. The vaporware flag is applied because it's an early-stage concept without deployment.","key_impact_metrics":[],"technology_tags":["Virtual Reality","Anxiety Management"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:05:35.927663Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_81d13b591acc","title":"Skill","content":"arXiv:2510.10023v1 Announce Type: new Abstract: Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10023","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.615492","language":"en","tags":["computer-science","cslg","csai","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":252,"author":"Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora","raw_content_length":1660,"priority":7,"update_frequency":1,"reading_time_minutes":1.26,"robust_parsing_used":true,"entities":{"organizations":["SFT"],"persons":["Didolkar et al."],"locations":[],"monetary":[]},"char_count":1659,"language_detected":"en","key_concepts":{"key_phrases":["Skill","the task","arXiv251010023v1 Announce Type","new Abstract","Language models","no improvement","ie saturation","vanilla","data","what"],"filter_categories":{"ai_ml":["data"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Skill":2.0,"the task":2.0,"arXiv251010023v1 Announce Type":1.0,"new Abstract":1.0,"Language models":1.0,"no improvement":1.0,"ie saturation":1.0,"vanilla":1.0,"data":1.0,"what":1.0}},"age_hours":2.7475540547222224,"is_recent":true,"quality_score":0.7,"sentiment_score":8.7915,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7583,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8887,"joy":0.0091,"surprise":0.0668,"sadness":0.0062,"fear":0.0051,"anger":0.0151,"disgust":0.0089},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new fine-tuning strategy (STAT) for language models, showing improvements of up to 7.5% on MATH and 4.6% on out-of-distribution benchmarks. While the research is promising, it is still in the applied research phase with no deployment or real-world data on energy consumption or emissions reduction, therefore limiting its immediate climate impact. The code is available, increasing transparency.","key_impact_metrics":["Improvement on MATH: 7.5%","Improvement on out-of-distribution benchmarks: 4.6%"],"technology_tags":["Language Models","Fine-tuning","Metacognition"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:05:39.823458Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_2623ccabd6a9","title":"Efficient Onboard Vision-Language Inference in UAV-Enabled Low","content":"arXiv:2510.10028v1 Announce Type: new Abstract: The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10028","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.616308","language":"en","tags":["cslg","csai","preprints","csdc","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim","raw_content_length":1629,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["UAVs","UAV","Vision-Language Inference","VQA"],"persons":[],"locations":[],"monetary":[]},"char_count":1628,"language_detected":"en","key_concepts":{"key_phrases":["Efficient Onboard Vision-Language Inference","UAV-Enabled Low","arXiv251010028v1 Announce Type","new Abstract","The rapid advancement","Low-Altitude Economy Networks","LAENets","a variety","applications","aerial surveillance"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Efficient Onboard Vision-Language Inference":2.0,"UAV-Enabled Low":2.0,"arXiv251010028v1 Announce Type":1.0,"new Abstract":1.0,"The rapid advancement":1.0,"Low-Altitude Economy Networks":1.0,"LAENets":1.0,"a variety":1.0,"applications":1.0,"aerial surveillance":1.0}},"age_hours":2.7475857452777777,"is_recent":true,"quality_score":1.0,"sentiment_score":9.2955,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8591,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9397,"joy":0.013,"surprise":0.0203,"sadness":0.0047,"fear":0.0068,"anger":0.0081,"disgust":0.0073},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article proposes a framework for optimizing UAV trajectory and resource allocation to minimize task latency and power consumption in low-altitude economy networks. Numerical results demonstrate the efficacy of the framework in improving inference performance and communication efficiency, but there's no mention of real-world deployment or specific emissions reductions. The vaporware flag is raised because it's a proposed system with numerical results, but no deployed units.","key_impact_metrics":["minimize task latency","minimize power consumption"],"technology_tags":["UAV","Vision-Language Models","Reinforcement Learning","Optimization"],"sdg_alignment":[9,11],"analyzed_at":"2025-10-29T11:05:43.116884Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9484bd8df659","title":"Experience-Efficient Model","content":"arXiv:2510.10029v1 Announce Type: new Abstract: We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel, model-free deep-reinforcement-learning algorithm that leverages pretraining to achieve high training efficiency and stability on very small training samples in physics-based environments. Reinforcement learning agents typically rely on large samples of environment interactions to learn a policy. However, frequent interactions with a (computer-simulated) environment may incur high computational costs, especially when the environment is complex. Our main innovation is a new policy neural network architecture that consists of a pretrained neural network middle section sandwiched between two fully-connected networks. Pretraining part of the network on a different environment with similar physics will help the agent learn the target environment with high efficiency because it will leverage a general understanding of the transferrable physics characteristics from the pretraining environment. We demonstrate that PPOPT outperforms baseline classic PPO on small training samples both in terms of rewards gained and general training stability. While PPOPT underperforms against classic model-based methods such as DYNA DDPG, the model-free nature of PPOPT allows it to train in significantly less time than its model-based counterparts. Finally, we present our implementation of PPOPT as open-source software, available at github.com/Davidrxyang/PPOPT.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10029","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.616737","language":"en","tags":["statml","cslg","preprints","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":196,"author":"Ruoxing Yang","raw_content_length":1483,"priority":7,"update_frequency":1,"reading_time_minutes":0.98,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1482,"language_detected":"en","key_concepts":{"key_phrases":["Experience-Efficient Model","arXiv251010029v1 Announce Type","new Abstract","PPOPT - Proximal Policy Optimization","Pretraining","a novel model-free deep-reinforcement-learning algorithm","high training efficiency","stability","very small training samples","physics-based environments"],"filter_categories":{"ai_ml":["Pretraining","a novel model-free deep-reinforcement-learning algorithm"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Experience-Efficient Model":2.0,"arXiv251010029v1 Announce Type":1.0,"new Abstract":1.0,"PPOPT - Proximal Policy Optimization":1.0,"Pretraining":1.0,"a novel model-free deep-reinforcement-learning algorithm":1.0,"high training efficiency":1.0,"stability":1.0,"very small training samples":1.0,"physics-based environments":1.0}},"age_hours":2.747599918333333,"is_recent":true,"quality_score":0.7,"sentiment_score":8.753,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7506,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8656,"joy":0.015,"surprise":0.0838,"sadness":0.0044,"fear":0.0103,"anger":0.0129,"disgust":0.0082},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel reinforcement learning algorithm (PPOPT) designed to improve training efficiency in physics-based environments, potentially reducing computational costs associated with complex simulations. While the algorithm shows promise in outperforming baseline PPO in terms of rewards and stability with small training samples, it is still in the early stages of development and lacks real-world deployment. The open-source implementation is a positive step, but economic viability and scalability remain uncertain.","key_impact_metrics":["Rewards gained","Training stability"],"technology_tags":["Reinforcement Learning","Proximal Policy Optimization","Pretraining"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:05:46.472155Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_796b0b48afce","title":"Failure","content":"arXiv:2510.10035v1 Announce Type: new Abstract: Optimizing LLM-based workflows is typically formulated as a global search, where candidate workflows are evaluated based on a scalar metric. This paradigm, however, suffers from a critical flaw: information collapse. By reducing rich, multi-step execution traces to simple success/failure signals, existing methods are rendered blind to the underlying structure of failures, fundamentally preventing them from modeling the workflow's failure distribution. We reconceptualize this challenge as a distributional problem. We propose a new paradigm where the optimization goal is not to maximize a scalar score, but to directly minimize a workflow's Expected Failure Mass, i.e., the integral of its failure probability density function defined over a high-dimensional Failure Signature Space (FSS). This distributional lens allows us to move from inefficient, zero-order optimization to a principled, gradient-like descent on the failure landscape itself. We introduce CE-Graph, a framework that operationalizes this paradigm through a novel, failure-driven refinement process. CE-Graph approximates the failure distribution from a pool of counterexamples, identifies its densest regions as recurring failure modes, and applies targeted, operator-constrained graph edits via a Propose-and-Verify mechanism to greedily reduce the failure mass. On math, code, and QA benchmarks, our CE-Graph achieves higher robustness at a significantly lower cost than strong baselines. This suggests that a system's reliability emerges not from avoiding failures, but from systematically learning and reshaping the geometric structure of its failure distributions.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10035","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.617572","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":228,"author":"Jusheng Zhang, Kaitong Cai, Qinglin Zeng, Ningyuan Liu, Stephen Fan, Ziliang Chen, Keze Wang","raw_content_length":1693,"priority":7,"update_frequency":1,"reading_time_minutes":1.14,"robust_parsing_used":true,"entities":{"organizations":["Expected Failure Mass","FSS"],"persons":[],"locations":[],"monetary":[]},"char_count":1692,"language_detected":"en","key_concepts":{"key_phrases":["Failure","arXiv251010035v1 Announce Type","new Abstract","Optimizing LLM-based workflows","a global search","candidate workflows","a scalar metric","This paradigm","a critical flaw","information collapse"],"filter_categories":{"ai_ml":["Failure"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Failure":2.0,"arXiv251010035v1 Announce Type":1.0,"new Abstract":1.0,"Optimizing LLM-based workflows":1.0,"a global search":1.0,"candidate workflows":1.0,"a scalar metric":1.0,"This paradigm":1.0,"a critical flaw":1.0,"information collapse":1.0}},"age_hours":2.74762977,"is_recent":true,"quality_score":1.0,"sentiment_score":0.3770000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.9246,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7568,"joy":0.0048,"surprise":0.0423,"sadness":0.0599,"fear":0.0466,"anger":0.0371,"disgust":0.0526},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel approach to optimizing LLM workflows by minimizing expected failure mass, potentially leading to more robust and reliable AI systems. The CE-Graph framework is introduced, which approximates the failure distribution and applies targeted edits to reduce failure mass. While promising, this is currently in the applied research stage with no deployed systems or quantified environmental benefits, hence the low climate impact score.","key_impact_metrics":["Significantly lower cost than strong baselines","Higher robustness on math, code, and QA benchmarks"],"technology_tags":["LLM","Workflow Optimization","Failure Distribution","CE-Graph"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:05:49.549256Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6a2a6985072c","title":"Automated Glaucoma Report Generation via Dual","content":"arXiv:2510.10037v1 Announce Type: new Abstract: Generative AI for automated glaucoma diagnostic report generation faces two predominant challenges: content redundancy in narrative outputs and inadequate highlighting of pathologically significant features including optic disc cupping, retinal nerve fiber layer defects, and visual field abnormalities. These limitations primarily stem from current multimodal architectures' insufficient capacity to extract discriminative structural-textural patterns from fundus imaging data while maintaining precise semantic alignment with domain-specific terminology in comprehensive clinical reports. To overcome these constraints, we present the Dual-Attention Semantic Parallel-LSTM Network (DA-SPL), an advanced multimodal generation framework that synergistically processes both fundus imaging and supplementary visual inputs. DA-SPL employs an Encoder-Decoder structure augmented with the novel joint dual-attention mechanism in the encoder for cross-modal feature refinement, the parallelized LSTM decoder architecture for enhanced temporal-semantic consistency, and the specialized label enhancement module for accurate disease-relevant term generation. Rigorous evaluation on standard glaucoma datasets demonstrates DA-SPL's consistent superiority over state-of-the-art models across quantitative metrics. DA-SPL exhibits exceptional capability in extracting subtle pathological indicators from multimodal inputs while generating diagnostically precise reports that exhibit strong concordance with clinical expert annotations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10037","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.617983","language":"en","tags":["preprints","research","computer-science","csce","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":180,"author":"Cheng Huang, Weizheng Xie, Zeyu Han, Tsengdar Lee, Karanjit Kooner, Jui-Ka Wang, Ning Zhang, Jia Zhang","raw_content_length":1573,"priority":7,"update_frequency":1,"reading_time_minutes":0.9,"robust_parsing_used":true,"entities":{"organizations":["the Dual-Attention Semantic Parallel-LSTM Network","Automated Glaucoma Report Generation"],"persons":[],"locations":[],"monetary":[]},"char_count":1572,"language_detected":"en","key_concepts":{"key_phrases":["Automated Glaucoma Report Generation","Dual","arXiv251010037v1 Announce Type","new Abstract","Generative AI","automated glaucoma diagnostic report generation","two predominant challenges","content redundancy","narrative outputs","inadequate highlighting"],"filter_categories":{"ai_ml":["Generative AI"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Automated Glaucoma Report Generation":2.0,"Dual":2.0,"arXiv251010037v1 Announce Type":1.0,"new Abstract":1.0,"Generative AI":1.0,"automated glaucoma diagnostic report generation":1.0,"two predominant challenges":1.0,"content redundancy":1.0,"narrative outputs":1.0,"inadequate highlighting":1.0}},"age_hours":2.747643873055556,"is_recent":true,"quality_score":1.0,"sentiment_score":2.4469999999999996,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5106,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8842,"joy":0.0059,"surprise":0.0474,"sadness":0.0221,"fear":0.0208,"anger":0.0112,"disgust":0.0084},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents a novel AI model for automated glaucoma report generation. While it demonstrates improved diagnostic accuracy compared to existing models, it's still in the research and development phase with no deployed units or real-world impact data. The potential for reducing healthcare costs and improving access to diagnosis is there, but it's currently theoretical.","key_impact_metrics":["Exceptional capability in extracting subtle pathological indicators","Strong concordance with clinical expert annotations"],"technology_tags":["Generative AI","Multimodal Learning","Medical Imaging"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:05:52.724337Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a441127f3fe5","title":"Combinatorial Philosopher Inequalities","content":"arXiv:2510.10039v1 Announce Type: new Abstract: In online combinatorial allocation, agents arrive sequentially and items are allocated in an online manner. The algorithm designer only knows the distribution of each agent's valuation, while the actual realization of the valuation is revealed only upon her arrival. Against the offline benchmark, Feldman, Gravin, and Lucier (SODA 2015) designed an optimal $0.5$-competitive algorithm for XOS agents. An emerging line of work focuses on designing approximation algorithms against the (computationally unbounded) optimal online algorithm. The primary goal is to design algorithms with approximation ratios strictly greater than $0.5$, surpassing the impossibility result against the offline optimum. Positive results are established for unit-demand agents (Papadimitriou, Pollner, Saberi, Wajc, MOR 2024), and for $k$-demand agents (Braun, Kesselheim, Pollner, Saberi, EC 2024). In this paper, we extend the existing positive results for agents with submodular valuations by establishing a $0.5 + \\Omega(1)$ approximation against a newly constructed online configuration LP relaxation for the combinatorial allocation setting. Meanwhile, we provide negative results for agents with XOS valuations by providing a $0.5$ integrality gap for the online configuration LP, showing an obstacle of existing approaches.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10039","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.618382","language":"en","tags":["preprints","research","computer-science","csds","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":185,"author":"Enze Sun, Zhihao Gavin Tang, Yifan Wang","raw_content_length":1361,"priority":7,"update_frequency":1,"reading_time_minutes":0.925,"robust_parsing_used":true,"entities":{"organizations":["Wajc","Saberi","XOS","Papadimitriou, Pollner"],"persons":["Pollner","Lucier","Gravin","Feldman","Kesselheim"],"locations":[],"monetary":["greater than $0.5$","k$-demand","0.5$-competitive"]},"char_count":1358,"language_detected":"en","key_concepts":{"key_phrases":["Combinatorial Philosopher Inequalities","arXiv251010039v1","Announce Type","new Abstract","online combinatorial allocation","agents","items","an online manner","The algorithm designer","the distribution"],"filter_categories":{"ai_ml":["The algorithm designer"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Combinatorial Philosopher Inequalities":2.0,"arXiv251010039v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"online combinatorial allocation":1.0,"agents":1.0,"items":1.0,"an online manner":1.0,"The algorithm designer":1.0,"the distribution":1.0}},"age_hours":2.7476593527777777,"is_recent":true,"quality_score":1.0,"sentiment_score":6.25,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.25,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8846,"joy":0.0183,"surprise":0.0765,"sadness":0.0041,"fear":0.0037,"anger":0.0092,"disgust":0.0036},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper focuses on theoretical improvements to online combinatorial allocation algorithms. While improved resource allocation could indirectly contribute to sustainability by optimizing the distribution of resources, there are no concrete actions or measurable outcomes related to climate impact, deployment, or economic viability described in the abstract. The work is primarily theoretical, with peer-reviewed validation, but lacks direct sustainability applications.","key_impact_metrics":[],"technology_tags":["algorithm optimization","combinatorial allocation"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:05:55.799744Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_59a3dad6d760","title":"Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference","content":"arXiv:2510.10040v1 Announce Type: new Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands via commercial wireless services, yet its effective regulation and technological enablement, particularly concerning coexistence with incumbent systems, remain significant challenges. Federal organizations have established regulatory frameworks to manage shared commercial use alongside mission-critical operations, such as military communications. This paper investigates the potential of machine learning (ML)-based approaches to enhance spectrum sharing capabilities within the Citizens Broadband Radio Service (CBRS) band, specifically focusing on the coexistence of commercial signals (e.g., 5G) and military radar systems. We demonstrate that ML techniques can potentially extend the Federal Communications Commission (FCC)-recommended signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar detection and waveform identification in high-interference environments. Through rigorous evaluation using both synthetic and real-world signals, our findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ) data and spectrograms, can achieve the FCC-recommended $99\\%$ radar detection accuracy even when subjected to high interference from 5G signals upto -5dB SINR, exceeding the required limits of $20$ SINR. Our experimental studies distinguish this work from the state-of-the-art by significantly extending the SINR limit for $99\\%$ radar detection accuracy from approximately $12$ dB down to $-5$ dB. Subsequent to detection, we further apply ML to analyze and identify radar waveforms. The proposed models also demonstrate the capability to classify six distinct radar waveform types with $93\\%$ accuracy.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10040","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.618820","language":"en","tags":["preprints","csni","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":225,"author":"Shafi Ullah Khan, Michel Kulhandjian, Debashri Roy","raw_content_length":1776,"priority":7,"update_frequency":1,"reading_time_minutes":1.125,"robust_parsing_used":true,"entities":{"organizations":["the Federal Communications Commission","CBRS","the Citizens Broadband Radio Service"],"persons":[],"locations":[],"monetary":[]},"char_count":1775,"language_detected":"en","key_concepts":{"key_phrases":["the Boundaries","CBRS Band","High 5G Interference","arXiv251010040v1 Announce Type","new Abstract","Spectrum sharing","a critical strategy","escalating user demands","commercial wireless services","its effective regulation"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"the Boundaries":2.0,"CBRS Band":2.0,"High 5G Interference":2.0,"arXiv251010040v1 Announce Type":1.0,"new Abstract":1.0,"Spectrum sharing":1.0,"a critical strategy":1.0,"escalating user demands":1.0,"commercial wireless services":1.0,"its effective regulation":1.0}},"age_hours":2.7476733063888887,"is_recent":true,"quality_score":1.0,"sentiment_score":9.2955,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8591,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7911,"joy":0.0121,"surprise":0.0336,"sadness":0.0092,"fear":0.1075,"anger":0.0369,"disgust":0.0097},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents research on improving spectrum sharing between 5G and radar systems using machine learning. The concrete action is the development and testing of ML models that can detect radar signals in high-interference environments. The evidence is based on experimental studies using synthetic and real-world signals, demonstrating improved radar detection accuracy at lower SINR levels, but it is still in the applied research stage.","key_impact_metrics":["99% radar detection accuracy at -5dB SINR","93% accuracy in classifying six radar waveform types"],"technology_tags":["Machine Learning","Spectrum Sharing","Radar Detection","5G"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:05:59.414727Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_cc36186438fb","title":"Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation","content":"arXiv:2510.10042v1 Announce Type: new Abstract: Belief systems are rarely globally consistent, yet effective reasoning often persists locally. We propose a novel graph-theoretic framework that cleanly separates credibility--external, a priori trust in sources--from confidence--an internal, emergent valuation induced by network structure. Beliefs are nodes in a directed, signed, weighted graph whose edges encode support and contradiction. Confidence is obtained by a contractive propagation process that mixes a stated prior with structure-aware influence and guarantees a unique, stable solution. Within this dynamics, we define reasoning zones: high-confidence, structurally balanced subgraphs on which classical inference is safe despite global contradictions. We provide a near-linear procedure that seeds zones by confidence, tests balance using a parity-based coloring, and applies a greedy, locality-preserving repair with Jaccard de-duplication to build a compact atlas. To model belief change, we introduce shock updates that locally downscale support and elevate targeted contradictions while preserving contractivity via a simple backtracking rule. Re-propagation yields localized reconfiguration-zones may shrink, split, or collapse--without destabilizing the entire graph. We outline an empirical protocol on synthetic signed graphs with planted zones, reporting zone recovery, stability under shocks, and runtime. The result is a principled foundation for contradiction-tolerant reasoning that activates classical logic precisely where structure supports it.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10042","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.619633","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":202,"author":"Saleh Nikooroo, Thomas Engel","raw_content_length":1576,"priority":7,"update_frequency":1,"reading_time_minutes":1.01,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Belief Graphs"],"locations":[],"monetary":[]},"char_count":1575,"language_detected":"en","key_concepts":{"key_phrases":["Belief Graphs","Reasoning Zones","Structure","Dynamics","Epistemic Activation","arXiv251010042v1 Announce Type","new Abstract","Belief systems","effective reasoning","a novel graph-theoretic framework"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Belief Graphs":2.0,"Reasoning Zones":2.0,"Structure":2.0,"Dynamics":2.0,"Epistemic Activation":2.0,"arXiv251010042v1 Announce Type":1.0,"new Abstract":1.0,"Belief systems":1.0,"effective reasoning":1.0,"a novel graph-theoretic framework":1.0}},"age_hours":2.7477036169444444,"is_recent":true,"quality_score":1.0,"sentiment_score":9.55,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.91,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8957,"joy":0.0188,"surprise":0.0542,"sadness":0.0029,"fear":0.0057,"anger":0.015,"disgust":0.0077},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel graph-theoretic framework for reasoning with inconsistent belief systems. While the research is technically sound and provides a near-linear procedure for identifying reasoning zones, it is currently at a basic research stage with no concrete deployment or measured outcomes related to sustainability. The potential climate impact is theoretical and unproven.","key_impact_metrics":["Zone recovery (%)","Runtime"],"technology_tags":["Belief Graphs","Reasoning Zones","Graph Theory"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:02.638531Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_64559a9a875d","title":"Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures","content":"arXiv:2510.10044v1 Announce Type: new Abstract: The growing demand for effective spectrum management and interference mitigation in shared bands, such as the Citizens Broadband Radio Service (CBRS), requires robust radar detection algorithms to protect the military transmission from interference due to commercial wireless transmission. These algorithms, in turn, depend on large, diverse, and carefully labeled spectrogram datasets. However, collecting and annotating real-world radio frequency (RF) spectrogram data remains a significant challenge, as radar signals are rare, and their occurrences are infrequent. This challenge makes the creation of balanced datasets difficult, limiting the performance and generalizability of AI models in this domain. To address this critical issue, we propose a diffusion-based generative model for synthesizing realistic and diverse spectrograms of five distinct categories that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a structural and statistical fidelity analysis of the generated spectrograms using widely accepted evaluation metrics Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from the training data. Furthermore, we demonstrate that pre-training on the generated spectrograms significantly improves training efficiency on a real-world radar detection task by enabling $51.5\\%$ faster convergence.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10044","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.620034","language":"en","tags":["preprints","csni","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":191,"author":"Rahul Vanukuri, Shafi Ullah Khan, Talip Tolga Sar{\\i}, Gokhan Secinti, Diego Pati\\~no, Debashri Roy","raw_content_length":1443,"priority":7,"update_frequency":1,"reading_time_minutes":0.955,"robust_parsing_used":true,"entities":{"organizations":["CBRS","the Citizens Broadband Radio Service"],"persons":[],"locations":[],"monetary":[]},"char_count":1440,"language_detected":"en","key_concepts":{"key_phrases":["Waves","Imagination","Unconditional Spectrogram Generation","Diffusion Architectures","arXiv251010044v1","Announce Type","new Abstract","The growing demand","effective spectrum management","mitigation"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Waves":2.0,"Imagination":2.0,"Unconditional Spectrogram Generation":2.0,"Diffusion Architectures":2.0,"arXiv251010044v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"The growing demand":1.0,"effective spectrum management":1.0,"mitigation":1.0}},"age_hours":2.7477187869444446,"is_recent":true,"quality_score":1.0,"sentiment_score":9.403500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8807,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.846,"joy":0.0114,"surprise":0.043,"sadness":0.0099,"fear":0.0592,"anger":0.0228,"disgust":0.0077},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a diffusion-based generative model for synthesizing realistic spectrograms to improve radar detection. The concrete action is the development and testing of this model, with a measured outcome of 51.5% faster convergence in a radar detection task. However, this is still in the research phase and lacks real-world deployment or validation beyond the reported metrics.","key_impact_metrics":["51.5% faster convergence"],"technology_tags":["diffusion models","spectrogram generation","radar detection","spectrum management"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:05.907708Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_83578228a342","title":"LOMORO: Long","content":"arXiv:2510.10046v1 Announce Type: new Abstract: Long-term monitoring of numerous dynamic targets can be tedious for a human operator and infeasible for a single robot, e.g., to monitor wild flocks, detect intruders, search and rescue. Fleets of autonomous robots can be effective by acting collaboratively and concurrently. However, the online coordination is challenging due to the unknown behaviors of the targets and the limited perception of each robot. Existing work often deploys all robots available without minimizing the fleet size, or neglects the constraints on their resources such as battery and memory. This work proposes an online coordination scheme called LOMORO for collaborative target monitoring, path routing and resource charging. It includes three core components: (I) the modeling of multi-robot task assignment problem under the constraints on resources and monitoring intervals; (II) the resource-aware task coordination algorithm iterates between the high-level assignment of dynamic targets and the low-level multi-objective routing via the Martin's algorithm; (III) the online adaptation algorithm in case of unpredictable target behaviors and robot failures. It ensures the explicitly upper-bounded monitoring intervals for all targets and the lower-bounded resource levels for all robots, while minimizing the average number of active robots. The proposed methods are validated extensively via large-scale simulations against several baselines, under different road networks, robot velocities, charging rates and monitoring intervals.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10046","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.620448","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":216,"author":"Mingke Lu, Shuaikang Wang, Meng Guo","raw_content_length":1566,"priority":7,"update_frequency":1,"reading_time_minutes":1.08,"robust_parsing_used":true,"entities":{"organizations":["LOMORO"],"persons":[],"locations":[],"monetary":[]},"char_count":1565,"language_detected":"en","key_concepts":{"key_phrases":["LOMORO","Long","arXiv251010046v1 Announce Type","new Abstract","Long-term monitoring","numerous dynamic targets","a human operator","a single robot","wild flocks","intruders"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"LOMORO":2.0,"Long":2.0,"arXiv251010046v1 Announce Type":1.0,"new Abstract":1.0,"Long-term monitoring":1.0,"numerous dynamic targets":1.0,"a human operator":1.0,"a single robot":1.0,"wild flocks":1.0,"intruders":1.0}},"age_hours":2.7477339880555554,"is_recent":true,"quality_score":1.0,"sentiment_score":9.1355,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8271,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9086,"joy":0.0102,"surprise":0.0274,"sadness":0.0232,"fear":0.0083,"anger":0.0136,"disgust":0.0087},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article proposes an online coordination scheme for collaborative target monitoring using autonomous robots, aiming to minimize the number of active robots while ensuring monitoring intervals and resource levels. The methods are validated via large-scale simulations, providing some evidence of potential impact, but there are no real-world deployments yet, making it vaporware at this stage. The reduction in active robots could lead to lower energy consumption, contributing to climate impact.","key_impact_metrics":["average number of active robots","upper-bounded monitoring intervals"],"technology_tags":["autonomous robots","multi-robot coordination","resource management"],"sdg_alignment":[7,9,13],"analyzed_at":"2025-10-29T11:06:09.714560Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9bab8dc7cab8","title":"Between Knowledge and Care: Evaluating Generative AI","content":"arXiv:2510.10048v1 Announce Type: new Abstract: Generative AI systems are increasingly adopted by patients seeking everyday health guidance, yet their reliability and clinical appropriateness remain uncertain. Taking Type 2 Diabetes Mellitus (T2DM) as a representative chronic condition, this paper presents a two-part mixed-methods study that examines how patients and physicians in China evaluate the quality and usability of AI-generated health information. Study~1 analyzes 784 authentic patient questions to identify seven core categories of informational needs and five evaluation dimensions -- \\textit{Accuracy, Safety, Clarity, Integrity}, and \\textit{Action Orientation}. Study~2 involves seven endocrinologists who assess responses from four mainstream AI models across these dimensions. Quantitative and qualitative findings reveal consistent strengths in factual and lifestyle guidance but significant weaknesses in medication interpretation, contextual reasoning, and empathy. Patients view AI as an accessible ``pre-visit educator,'' whereas clinicians highlight its lack of clinical safety and personalization. Together, the findings inform design implications for interactive health systems, advocating for multi-model orchestration, risk-aware fallback mechanisms, and emotionally attuned communication to ensure trustworthy AI assistance in chronic disease care.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10048","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.621250","language":"en","tags":["preprints","research","computer-science","cshc","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Yibo Meng, Ruiqi Chen, Zhiming Liu, Xiaolan Ding, Yan Guan","raw_content_length":1381,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["Between Knowledge and Care: Evaluating","Safety, Clarity, Integrity"],"persons":[],"locations":["China"],"monetary":[]},"char_count":1380,"language_detected":"en","key_concepts":{"key_phrases":["Knowledge","Care","patients","arXiv251010048v1","Announce Type","new Abstract","Generative AI systems","everyday health guidance","their reliability","clinical appropriateness"],"filter_categories":{"ai_ml":["Generative AI systems"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Knowledge":2.0,"Care":2.0,"patients":2.0,"arXiv251010048v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Generative AI systems":1.0,"everyday health guidance":1.0,"their reliability":1.0,"clinical appropriateness":1.0}},"age_hours":2.7477629233333336,"is_recent":true,"quality_score":1.0,"sentiment_score":6.25,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.25,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6882,"joy":0.0222,"surprise":0.0564,"sadness":0.0083,"fear":0.1954,"anger":0.0177,"disgust":0.0118},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":5,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research analyzes the quality of AI-generated health information for diabetes patients, identifying weaknesses in medication interpretation and contextual reasoning. The study uses quantitative and qualitative methods, including analysis of 784 patient questions and assessment by seven endocrinologists. While it identifies design implications for interactive health systems, it remains in the applied research stage with no deployed units or customer contracts.","key_impact_metrics":["784 patient questions analyzed","7 endocrinologists assessed AI responses"],"technology_tags":["Generative AI","Healthcare AI","Type 2 Diabetes Mellitus"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:06:13.260691Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f5af424cab83","title":"ALLOY: Generating Reusable Agent Workflows from User Demonstration","content":"arXiv:2510.10049v1 Announce Type: new Abstract: Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10049","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.621674","language":"en","tags":["computer-science","csma","csai","preprints","cshc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":172,"author":"Jiawen Li, Zheng Ning, Yuan Tian, Toby Jia-jun Li","raw_content_length":1298,"priority":7,"update_frequency":1,"reading_time_minutes":0.86,"robust_parsing_used":true,"entities":{"organizations":["LLM","User Demonstration","Programming by Demonstration"],"persons":["Workflows"],"locations":[],"monetary":[]},"char_count":1297,"language_detected":"en","key_concepts":{"key_phrases":["ALLOY","Generating Reusable Agent","User Demonstration","arXiv251010049v1 Announce Type","new Abstract","Large language models","LLMs","end-users","complex tasks","autonomous agents"],"filter_categories":{"ai_ml":["Large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"ALLOY":2.0,"Generating Reusable Agent":2.0,"User Demonstration":2.0,"arXiv251010049v1 Announce Type":1.0,"new Abstract":1.0,"Large language models":1.0,"LLMs":1.0,"end-users":1.0,"complex tasks":1.0,"autonomous agents":1.0}},"age_hours":2.747778315,"is_recent":true,"quality_score":1.0,"sentiment_score":5.385999999999999,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0772,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9284,"joy":0.0112,"surprise":0.0254,"sadness":0.0072,"fear":0.0074,"anger":0.0158,"disgust":0.0046},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a system (ALLOY) for generating reusable agent workflows, which could indirectly impact sustainability by improving efficiency in tasks like trip planning or social media content creation. However, the impact is theoretical and not directly tied to measurable reductions in GHG emissions or resource consumption. The system is in an early stage of development, demonstrated with a study of 12 participants.","key_impact_metrics":["User intent captured with ALLOY","Procedural preferences captured with ALLOY"],"technology_tags":["Large Language Models","Programming by Demonstration","Web Agents"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T11:06:17.090471Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_d764cc6cb7e4","title":"Complementary and Contrastive Learning for Audio","content":"arXiv:2510.10051v1 Announce Type: new Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps that correlate with the auditory signals of objects. This field has seen significant progress with numerous CNN and Transformer-based methods enhancing the segmentation accuracy and robustness. Traditional CNN approaches manage audio-visual interactions through basic operations like padding and multiplications but are restricted by CNNs' limited local receptive field. More recently, Transformer-based methods treat auditory cues as queries, utilizing attention mechanisms to enhance audio-visual cooperation within frames. Nevertheless, they typically struggle to extract multimodal coefficients and temporal dynamics adequately. To overcome these limitations, we present the Complementary and Contrastive Transformer (CCFormer), a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively. Our CCFormer initiates with the Early Integration Module (EIM) that employs a parallel bilateral architecture, merging multi-scale visual features with audio data to boost cross-modal complementarity. To extract the intra-frame spatial features and facilitate the perception of temporal coherence, we introduce the Multi-query Transformer Module (MTM), which dynamically endows audio queries with learning capabilities and models the frame and video-level relations simultaneously. Furthermore, we propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across both modalities in the unified feature space. Through the effective combination of those designs, our method sets new state-of-the-art benchmarks across the S4, MS3 and AVSS datasets. Our source code and model weights will be made publicly available at https://github.com/SitongGong/CCFormer","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10051","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.622105","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":235,"author":"Sitong Gong, Yunzhi Zhuge, Lu Zhang, Pingping Zhang, Huchuan Lu","raw_content_length":1857,"priority":7,"update_frequency":1,"reading_time_minutes":1.175,"robust_parsing_used":true,"entities":{"organizations":["Contrastive Learning for Audio arXiv:2510.10051v1 Announce","Contrastive Transformer","AVS","CNN","Transformer"],"persons":[],"locations":[],"monetary":[]},"char_count":1856,"language_detected":"en","key_concepts":{"key_phrases":["Complementary and Contrastive Learning","Audio","arXiv251010051v1 Announce Type","new Abstract","Audio-Visual Segmentation","AVS","pixel-wise segmentation maps","the auditory signals","objects","This field"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Complementary and Contrastive Learning":2.0,"Audio":2.0,"arXiv251010051v1 Announce Type":1.0,"new Abstract":1.0,"Audio-Visual Segmentation":1.0,"AVS":1.0,"pixel-wise segmentation maps":1.0,"the auditory signals":1.0,"objects":1.0,"This field":1.0}},"age_hours":2.747793061944445,"is_recent":true,"quality_score":1.0,"sentiment_score":4.55,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.09,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9192,"joy":0.0076,"surprise":0.0311,"sadness":0.0055,"fear":0.007,"anger":0.015,"disgust":0.0146},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel AI framework (CCFormer) for audio-visual segmentation, improving accuracy on benchmark datasets. While the technology is innovative, its direct climate impact is currently minimal and theoretical, as it's in the early research stage with no deployment or quantified environmental benefits. The code is being made publicly available, which increases transparency.","key_impact_metrics":["Segmentation accuracy on S4 dataset","Segmentation accuracy on MS3 dataset"],"technology_tags":["Audio-Visual Segmentation","Transformer Networks","Contrastive Learning"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:06:20.417206Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_cdf6a0d60405","title":"Think Twice to See More: Iterative Visual Reasoning in Medical VLMs","content":"arXiv:2510.10052v1 Announce Type: new Abstract: Medical vision-language models (VLMs) excel at image-text understanding but typically rely on a single-pass reasoning that neglects localized visual cues. In clinical practice, however, human experts iteratively scan, focus, and refine the regions of interest before reaching a final diagnosis. To narrow this machine-human perception gap, we introduce ViTAR, a novel VLM framework that emulates the iterative reasoning process of human experts through a cognitive chain of \"think-act-rethink-answer\". ViTAR treats medical images as interactive objects, enabling models to engage multi-step visual reasoning. To support this approach, we curate a high-quality instruction dataset comprising 1K interactive examples that encode expert-like diagnostic behaviors. In addition, a 16K visual question answering training data has been curated towards fine-grained visual diagnosis. We introduce a two-stage training strategy that begins with supervised fine-tuning to guide cognitive trajectories, followed by the reinforcement learning to optimize decision-making. Extensive evaluations demonstrate that ViTAR outperforms strong state-of-the-art models. Visual attention analysis reveals that from the \"think\" to \"rethink\" rounds, ViTAR increasingly anchors visual grounding to clinically critical regions and maintains high attention allocation to visual tokens during reasoning, providing mechanistic insight into its improved performance. These findings demonstrate that embedding expert-style iterative thinking chains into VLMs enhances both performance and trustworthiness of medical AI.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10052","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.622530","language":"en","tags":["computer-science","csai","preprints","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong Wang, Mu Zhou, Mianxin Liu","raw_content_length":1637,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["VLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1636,"language_detected":"en","key_concepts":{"key_phrases":["Iterative Visual Reasoning","Medical VLMs","human experts","Announce Type","new Abstract","Medical vision-language models","VLMs","image-text understanding","a single-pass reasoning","localized visual cues"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Iterative Visual Reasoning":2.0,"Medical VLMs":2.0,"human experts":2.0,"Announce Type":1.0,"new Abstract":1.0,"Medical vision-language models":1.0,"VLMs":1.0,"image-text understanding":1.0,"a single-pass reasoning":1.0,"localized visual cues":1.0}},"age_hours":2.7478087552777777,"is_recent":true,"quality_score":1.0,"sentiment_score":8.525,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.705,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9249,"joy":0.0084,"surprise":0.0478,"sadness":0.0033,"fear":0.0045,"anger":0.0075,"disgust":0.0037},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel VLM framework (ViTAR) for medical image analysis. While it shows improved performance over existing models and provides mechanistic insights, it is still in the early stages of development and lacks real-world deployment or quantified environmental benefits. The impact is theoretical at this stage.","key_impact_metrics":["1K interactive examples","16K visual question answering training data"],"technology_tags":["Vision-Language Models","Medical AI","Iterative Reasoning"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:06:23.259867Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9000e7174b03","title":"DREAM: A Benchmark Study for Deepfake REalism AssessMent","content":"arXiv:2510.10053v1 Announce Type: new Abstract: Deep learning based face-swap videos, widely known as deepfakes, have drawn wide attention due to their threat to information credibility. Recent works mainly focus on the problem of deepfake detection that aims to reliably tell deepfakes apart from real ones, in an objective way. On the other hand, the subjective perception of deepfakes, especially its computational modeling and imitation, is also a significant problem but lacks adequate study. In this paper, we focus on the visual realism assessment of deepfakes, which is defined as the automatic assessment of deepfake visual realism that approximates human perception of deepfakes. It is important for evaluating the quality and deceptiveness of deepfakes which can be used for predicting the influence of deepfakes on Internet, and it also has potentials in improving the deepfake generation process by serving as a critic. This paper prompts this new direction by presenting a comprehensive benchmark called DREAM, which stands for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of diverse quality, a large scale annotation that includes 140,000 realism scores and textual descriptions obtained from 3,500 human annotators, and a comprehensive evaluation and analysis of 16 representative realism assessment methods, including recent large vision language model based methods and a newly proposed description-aligned CLIP method. The benchmark and insights included in this study can lay the foundation for future research in this direction and other related areas.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10053","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.622952","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":237,"author":"Bo Peng, Zichuan Wang, Sheng Yu, Xiaochuan Jin, Wei Wang, Jing Dong","raw_content_length":1602,"priority":7,"update_frequency":1,"reading_time_minutes":1.185,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1601,"language_detected":"en","key_concepts":{"key_phrases":["deepfakes","DREAM","A Benchmark Study","Deepfake REalism AssessMent","arXiv251010053v1 Announce Type","new Abstract","Deep learning","face-swap videos","wide attention","their threat"],"filter_categories":{"research_academic":["A Benchmark Study"],"ai_ml":["Deep learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"deepfakes":3.0,"DREAM":2.0,"A Benchmark Study":2.0,"Deepfake REalism AssessMent":2.0,"arXiv251010053v1 Announce Type":1.0,"new Abstract":1.0,"Deep learning":1.0,"face-swap videos":1.0,"wide attention":1.0,"their threat":1.0}},"age_hours":2.747824209722222,"is_recent":true,"quality_score":1.0,"sentiment_score":4.7844999999999995,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0431,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.2806,"joy":0.0119,"surprise":0.0302,"sadness":0.0078,"fear":0.5996,"anger":0.0557,"disgust":0.0142},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a benchmark dataset and evaluation methods for assessing the realism of deepfakes. While it doesn't directly impact climate change, it could indirectly influence public perception and trust in information, which is crucial for climate action. The research is in the early stages, with no deployed technology or measured outcomes related to sustainability.","key_impact_metrics":["140,000 realism scores","3,500 human annotators"],"technology_tags":["deepfake detection","visual realism assessment","vision language models"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:06:26.181120Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_64724301573d","title":"Collaborative Learning of Semantic","content":"arXiv:2510.10055v1 Announce Type: new Abstract: Multi-label image recognition with incomplete labels is a critical learning task and has emerged as a focal topic in computer vision. However, this task is confronted with two core challenges: semantic-aware feature learning and missing label recovery. In this paper, we propose a novel Collaborative Learning of Semantic-aware feature learning and Label recovery (CLSL) method for multi-label image recognition with incomplete labels, which unifies the two aforementioned challenges into a unified learning framework. More specifically, we design a semantic-related feature learning module to learn robust semantic-related features by discovering semantic information and label correlations. Then, a semantic-guided feature enhancement module is proposed to generate high-quality discriminative semantic-aware features by effectively aligning visual and semantic feature spaces. Finally, we introduce a collaborative learning framework that integrates semantic-aware feature learning and label recovery, which can not only dynamically enhance the discriminability of semantic-aware features but also adaptively infer and recover missing labels, forming a mutually reinforced loop between the two processes. Extensive experiments on three widely used public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art multi-label image recognition methods with incomplete labels.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10055","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.623353","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":189,"author":"Zhi-Fen He, Ren-Dong Xie, Bo Li, Bin Liu, Jin-Yan Hu","raw_content_length":1465,"priority":7,"update_frequency":1,"reading_time_minutes":0.945,"robust_parsing_used":true,"entities":{"organizations":["Collaborative Learning of Semantic arXiv:2510.10055v1 Announce Type","Collaborative Learning of Semantic-aware"],"persons":["Label"],"locations":[],"monetary":[]},"char_count":1464,"language_detected":"en","key_concepts":{"key_phrases":["Collaborative Learning","Semantic","incomplete labels","arXiv251010055v1 Announce Type","new Abstract","Multi-label image recognition","a critical learning task","a focal topic","computer vision","this task"],"filter_categories":{"ai_ml":["computer vision"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Collaborative Learning":3.0,"Semantic":2.0,"incomplete labels":2.0,"arXiv251010055v1 Announce Type":1.0,"new Abstract":1.0,"Multi-label image recognition":1.0,"a critical learning task":1.0,"a focal topic":1.0,"computer vision":1.0,"this task":1.0}},"age_hours":2.747838863333333,"is_recent":true,"quality_score":1.0,"sentiment_score":4.1105,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1779,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.5419,"joy":0.0082,"surprise":0.0158,"sadness":0.0105,"fear":0.3469,"anger":0.049,"disgust":0.0277},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method (CLSL) for multi-label image recognition, which could potentially improve the efficiency of various applications. However, it's currently in the research phase, with no deployed units or real-world data on its impact on sustainability. The experiments are conducted on public datasets, demonstrating the method's performance but not its direct contribution to climate action or other sustainability goals.","key_impact_metrics":["MS-COCO performance","VOC2007 performance"],"technology_tags":["Multi-label image recognition","Semantic-aware feature learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:29.741250Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_ff5eb7ca7e34","title":"One4Many","content":"arXiv:2510.10057v1 Announce Type: new Abstract: The three-dimensional bin packing problem (3D-BPP) is widely applied in logistics and warehousing. Existing learning-based approaches often neglect practical stability-related constraints and exhibit limitations in generalizing across diverse bin dimensions. To address these limitations, we propose a novel deep reinforcement learning framework, One4Many-StablePacker (O4M-SP). The primary advantage of O4M-SP is its ability to handle various bin dimensions in a single training process while incorporating support and weight constraints common in practice. Our training method introduces two innovative mechanisms. First, it employs a weighted reward function that integrates loading rate and a new height difference metric for packing layouts, promoting improved bin utilization through flatter packing configurations. Second, it combines clipped policy gradient optimization with a tailored policy drifting method to mitigate policy entropy collapse, encouraging exploration at critical decision nodes during packing to avoid suboptimal solutions. Extensive experiments demonstrate that O4M-SP generalizes successfully across diverse bin dimensions and significantly outperforms baseline methods. Furthermore, O4M-SP exhibits strong practical applicability by effectively addressing packing scenarios with stability constraints.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10057","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.623776","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Lei Gao, Shihong Huang, Shengjie Wang, Hong Ma, Feng Zhang, Hengda Bao, Qichang Chen, Weihua Zhou","raw_content_length":1381,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["3D-BPP"],"persons":[],"locations":[],"monetary":[]},"char_count":1380,"language_detected":"en","key_concepts":{"key_phrases":["O4M-SP","arXiv251010057v1 Announce Type","new Abstract","The three-dimensional bin packing problem","3D-BPP","logistics","warehousing","Existing learning-based approaches","practical stability-related constraints","exhibit limitations"],"filter_categories":{"ai_ml":["practical stability-related constraints"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"O4M-SP":2.0,"arXiv251010057v1 Announce Type":1.0,"new Abstract":1.0,"The three-dimensional bin packing problem":1.0,"3D-BPP":1.0,"logistics":1.0,"warehousing":1.0,"Existing learning-based approaches":1.0,"practical stability-related constraints":1.0,"exhibit limitations":1.0}},"age_hours":2.7478535680555556,"is_recent":true,"quality_score":1.0,"sentiment_score":4.8709999999999996,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8979,"joy":0.0133,"surprise":0.0508,"sadness":0.0131,"fear":0.0071,"anger":0.0115,"disgust":0.0063},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel deep reinforcement learning framework for 3D bin packing, aiming to improve bin utilization and stability. It quantifies improvements by outperforming baseline methods and addressing stability constraints, but it's still in the research phase without deployed units. The claim of improved bin utilization suggests a potential reduction in transportation needs, leading to lower emissions, but this is not directly quantified.","key_impact_metrics":["Improved bin utilization","Generalizes successfully across diverse bin dimensions"],"technology_tags":["Deep Reinforcement Learning","3D Bin Packing","Logistics Optimization"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T11:06:32.804127Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_89fa146effed","title":"Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model","content":"arXiv:2510.10059v1 Announce Type: new Abstract: Recent advancements in lunar positioning, navigation, and timing (PNT) have demonstrated that terrestrial GNSS signals, including weak sidelobe transmissions, can be exploited for lunar spacecraft positioning and timing. While GNSS-based navigation at the Moon has been validated recently, unmodeled ionospheric and plasmaspheric delays remain a significant error source, particularly given the unique signal geometry and extended propagation paths. This paper characterizes these delays using the Global Core Plasma Model (GCPM) and a custom low-cost ray-tracing algorithm that iteratively solves for bent signal paths. We simulate first-, second-, and third-order group delays, as well as excess path length from ray bending, for GNSS signals received at both lunar orbit and the lunar south pole under varying solar and geomagnetic conditions. Results show that mean group delays are typically on the order of 1 m, but can exceed 100 m for low-altitude ray paths during high solar activity, while bending delays are generally smaller but non-negligible for low-altitude ray paths. We also quantify the influence of signal frequency, geomagnetic $K_p$ index, and solar R12 index. These findings inform the design of robust positioning and timing algorithms that utilize terrestrial GNSS signals.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10059","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.624328","language":"en","tags":["preprints","research","computer-science","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":194,"author":"Keidai Iiyama, Grace Gao","raw_content_length":1346,"priority":7,"update_frequency":1,"reading_time_minutes":0.97,"robust_parsing_used":true,"entities":{"organizations":["the Global Core Plasma Model","Ionospheric","PNT","Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers","GNSS"],"persons":["Moon"],"locations":[],"monetary":[]},"char_count":1345,"language_detected":"en","key_concepts":{"key_phrases":["Ionospheric and Plasmaspheric Delay Characterization","Lunar Terrestrial GNSS Receivers","Global Core Plasma Model","timing","arXiv251010059v1 Announce Type","new Abstract","Recent advancements","lunar positioning","navigation","PNT"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Ionospheric and Plasmaspheric Delay Characterization":2.0,"Lunar Terrestrial GNSS Receivers":2.0,"Global Core Plasma Model":2.0,"timing":2.0,"arXiv251010059v1 Announce Type":1.0,"new Abstract":1.0,"Recent advancements":1.0,"lunar positioning":1.0,"navigation":1.0,"PNT":1.0}},"age_hours":2.747868548333333,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2850000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.743,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9271,"joy":0.0041,"surprise":0.0297,"sadness":0.0115,"fear":0.0081,"anger":0.0114,"disgust":0.0081},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents research on characterizing ionospheric and plasmaspheric delays for lunar GNSS receivers. While it uses a model and ray-tracing algorithm to quantify delays (mean group delays on the order of 1 m, exceeding 100 m in some conditions), it's still in the research phase with no actual deployment or economic viability demonstrated. The technical credibility is supported by the use of a model and algorithm, and the metrics are quantified.","key_impact_metrics":["mean group delays on the order of 1 m","delays can exceed 100 m for low-altitude ray paths during high solar activity"],"technology_tags":["GNSS","ionospheric delay modeling","ray-tracing algorithm"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:36.627332Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9d817f839ae0","title":"Translution: Unifying Self","content":"arXiv:2510.10060v1 Announce Type: new Abstract: When modeling a given type of data, we consider it to involve two key aspects: 1) identifying relevant elements (e.g., image pixels or textual words) to a central element, as in a convolutional receptive field, or to a query element, as in self-attention, and 2) encoding these tokens effectively. Self-attention can adaptively identify these elements but relies on absolute positional embedding for structural representation learning. In contrast, convolution encodes elements in a relative manner, yet their fixed kernel size limits their ability to adaptively select the relevant elements. In this paper, we introduce Translution, an operation that unifies the adaptive identification capability of self-attention and the relative encoding advantage of convolution. However, this integration leads to a substantial increase in the number of parameters, exceeding most currently available computational resources. Therefore, we propose a lightweight variant of Translution, named {\\alpha}-Translution. Experiments on computer vision and natural language processing tasks show that Translution (including {\\alpha}-Translution) achieves superior accuracy compared to self-attention. The code is available at https://github.com/hehefan/Translution.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10060","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.624758","language":"en","tags":["cslg","csai","preprints","cscv","research","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Hehe Fan, Yi Yang, Mohan Kankanhalli, Fei Wu","raw_content_length":1296,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["Translution"],"persons":[],"locations":[],"monetary":[]},"char_count":1295,"language_detected":"en","key_concepts":{"key_phrases":["Translution","Unifying Self","arXiv251010060v1 Announce Type","new Abstract","a given type","data","two key aspects","relevant elements","eg image pixels","textual words"],"filter_categories":{"ai_ml":["data"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Translution":2.0,"Unifying Self":2.0,"arXiv251010060v1 Announce Type":1.0,"new Abstract":1.0,"a given type":1.0,"data":1.0,"two key aspects":1.0,"relevant elements":1.0,"eg image pixels":1.0,"textual words":1.0}},"age_hours":2.7478839058333335,"is_recent":true,"quality_score":1.0,"sentiment_score":6.191,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2382,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9064,"joy":0.0092,"surprise":0.0095,"sadness":0.0035,"fear":0.0278,"anger":0.0179,"disgust":0.0257},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a new AI model (Translution) that aims to improve accuracy in computer vision and NLP tasks. While potentially improving efficiency in these fields, there are no concrete actions or measurable outcomes related to sustainability. The research is in the early stages, with no deployment or economic viability demonstrated.","key_impact_metrics":["Accuracy compared to self-attention"],"technology_tags":["AI","Machine Learning","Computer Vision","Natural Language Processing"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:06:39.645957Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_59b6ca59b11c","title":"HUME: Measuring the Human","content":"arXiv:2510.10062v1 Announce Type: new Abstract: Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10062","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.625165","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":191,"author":"Adnan El Assadi, Isaac Chung, Roman Solomatin, Niklas Muennighoff, Kenneth Enevoldsen","raw_content_length":1451,"priority":7,"update_frequency":1,"reading_time_minutes":0.955,"robust_parsing_used":true,"entities":{"organizations":["MTEB"],"persons":[],"locations":[],"monetary":[]},"char_count":1450,"language_detected":"en","key_concepts":{"key_phrases":["HUME","the Human","Announce Type","new Abstract","human and model performance","a valuable perspective","the strengths","limitations","embedding models","meaning"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"HUME":3.0,"the Human":2.0,"Announce Type":1.0,"new Abstract":1.0,"human and model performance":1.0,"a valuable perspective":1.0,"the strengths":1.0,"limitations":1.0,"embedding models":1.0,"meaning":1.0}},"age_hours":2.747898549166667,"is_recent":true,"quality_score":1.0,"sentiment_score":8.352500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6705,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9113,"joy":0.0028,"surprise":0.021,"sadness":0.0201,"fear":0.0078,"anger":0.017,"disgust":0.0199},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a framework for evaluating text embeddings by comparing them to human performance. While it provides a valuable tool for understanding model limitations, it does not directly address climate change or sustainability. The framework is in an early stage of development and lacks deployment.","key_impact_metrics":["Human performance average: 77.6%","Best embedding model performance average: 80.1%"],"technology_tags":["Text embeddings","Human evaluation framework"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:06:42.864646Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_005734c41f94","title":"CLMN: Concept based Language Models via Neural Symbolic Reasoning","content":"arXiv:2510.10063v1 Announce Type: new Abstract: Deep learning has advanced NLP, but interpretability remains limited, especially in healthcare and finance. Concept bottleneck models tie predictions to human concepts in vision, but NLP versions either use binary activations that harm text representations or latent concepts that weaken semantics, and they rarely model dynamic concept interactions such as negation and context. We introduce the Concept Language Model Network (CLMN), a neural-symbolic framework that keeps both performance and interpretability. CLMN represents concepts as continuous, human-readable embeddings and applies fuzzy-logic reasoning to learn adaptive interaction rules that state how concepts affect each other and the final decision. The model augments original text features with concept-aware representations and automatically induces interpretable logic rules. Across multiple datasets and pre-trained language models, CLMN achieves higher accuracy than existing concept-based methods while improving explanation quality. These results show that integrating neural representations with symbolic reasoning in a unified concept space can yield practical, transparent NLP systems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10063","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.625577","language":"en","tags":["computer-science","csai","preprints","cscl","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":157,"author":"Yibo Yang","raw_content_length":1211,"priority":7,"update_frequency":1,"reading_time_minutes":0.785,"robust_parsing_used":true,"entities":{"organizations":["NLP","Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1210,"language_detected":"en","key_concepts":{"key_phrases":["CLMN","Concept","Language Models","Neural Symbolic Reasoning","arXiv251010063v1 Announce Type","new Abstract","Deep learning","NLP","interpretability","healthcare"],"filter_categories":{"ai_ml":["Deep learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"CLMN":3.0,"Concept":2.0,"Language Models":2.0,"Neural Symbolic Reasoning":2.0,"arXiv251010063v1 Announce Type":1.0,"new Abstract":1.0,"Deep learning":1.0,"NLP":1.0,"interpretability":1.0,"healthcare":1.0}},"age_hours":2.747913379722222,"is_recent":true,"quality_score":1.0,"sentiment_score":0.5480000000000002,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8904,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7879,"joy":0.0043,"surprise":0.0135,"sadness":0.0106,"fear":0.0785,"anger":0.0504,"disgust":0.0548},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel neural-symbolic framework (CLMN) for improving interpretability in NLP, particularly in healthcare and finance. While it achieves higher accuracy than existing concept-based methods and improves explanation quality across multiple datasets, it remains in the applied research stage with no deployed units or operational data. The potential climate impact is indirect, as improved interpretability could lead to better decision-making in climate-related fields, but this is theoretical.","key_impact_metrics":["Higher accuracy than existing concept-based methods","Improved explanation quality"],"technology_tags":["Neural Symbolic Reasoning","Concept Language Model Network"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:46.686281Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_82d855ae979f","title":"OBsmith: Testing JavaScript Obfuscator using LLM","content":"arXiv:2510.10066v1 Announce Type: new Abstract: JavaScript obfuscators are widely deployed to protect intellectual property and resist reverse engineering, yet their correctness has been largely overlooked compared to performance and resilience. Existing evaluations typically measure resistance to deobfuscation, leaving the critical question of whether obfuscators preserve program semantics unanswered. Incorrect transformations can silently alter functionality, compromise reliability, and erode security-undermining the very purpose of obfuscation. To address this gap, we present OBsmith, a novel framework to systematically test JavaScript obfuscators using large language models (LLMs). OBsmith leverages LLMs to generate program sketches abstract templates capturing diverse language constructs, idioms, and corner cases-which are instantiated into executable programs and subjected to obfuscation under different configurations. Besides LLM-powered sketching, OBsmith also employs a second source: automatic extraction of sketches from real programs. This extraction path enables more focused testing of project specific features and lets developers inject domain knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown correctness bugs. Under an equal program budget, five general purpose state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE, Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary focus on obfuscation induced misbehavior. An ablation shows that all components except our generic MRs contribute to at least one bug class; the negative MR result suggests the need for obfuscator-specific metamorphic relations. Our results also seed discussion on how to balance obfuscation presets and performance cost. We envision OBsmith as an important step towards automated testing and quality assurance of obfuscators and other semantic-preserving toolchains.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10066","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.626016","language":"en","tags":["computer-science","csai","cspl","preprints","csse","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":248,"author":"Shan Jiang, Chenguang Zhu, Sarfraz Khurshid","raw_content_length":1942,"priority":7,"update_frequency":1,"reading_time_minutes":1.24,"robust_parsing_used":true,"entities":{"organizations":["OBsmith","JavaScript","LLM-"],"persons":[],"locations":[],"monetary":[]},"char_count":1941,"language_detected":"en","key_concepts":{"key_phrases":["OBsmith","Testing JavaScript Obfuscator","LLM","arXiv251010066v1 Announce Type","new Abstract","JavaScript obfuscators","intellectual property","reverse engineering","their correctness","performance"],"filter_categories":{"ai_ml":["LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"OBsmith":2.0,"Testing JavaScript Obfuscator":2.0,"LLM":2.0,"arXiv251010066v1 Announce Type":1.0,"new Abstract":1.0,"JavaScript obfuscators":1.0,"intellectual property":1.0,"reverse engineering":1.0,"their correctness":1.0,"performance":1.0}},"age_hours":2.7479271705555552,"is_recent":true,"quality_score":1.0,"sentiment_score":7.7115,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5423,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8995,"joy":0.0025,"surprise":0.0144,"sadness":0.0136,"fear":0.0156,"anger":0.029,"disgust":0.0253},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel framework, OBsmith, for testing JavaScript obfuscators using LLMs. It uncovers 11 previously unknown correctness bugs, demonstrating a concrete outcome. The research is peer-reviewed and presents specific metrics, but it's still in the applied research stage with no deployment.","key_impact_metrics":["11 previously unknown correctness bugs","5 general purpose state-of-the-art JavaScript fuzzers failed to detect these issues"],"technology_tags":["JavaScript obfuscation","Large Language Models","Software Testing"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:50.122768Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f4b9ec8ce0a6","title":"Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi","content":"arXiv:2510.10068v1 Announce Type: new Abstract: The computer vision domain has greatly benefited from an abundance of data across many modalities to improve on various visual tasks. Recently, there has been a lot of focus on self-supervised pre-training methods through Masked Autoencoders (MAE) \\cite{he2022masked,bachmann2022multimae}, usually used as a first step before optimizing for a downstream task, such as classification or regression. This is very useful as it doesn't require any manually labeled data. In this work, we introduce Probabilistic Hyper-Graphs using Masked Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural graphs \\cite{leordeanu2021semi} with the modern approach of masked autoencoders under a common theoretical framework. Through random masking of entire modalities, not just patches, the model samples from the distribution of hyper-edges on each forward pass. Additionally, the model adapts the standard MAE algorithm by combining pre-training and fine-tuning into a single training loop. Moreover, our approach enables the creation of inference-time ensembles which, through aggregation, boost the final prediction performance and consistency. Lastly, we show that we can apply knowledge distillation on top of the ensembles with little loss in performance, even with models that have fewer than 1M parameters. While our work mostly focuses on outdoor UAV scenes that contain multiple world interpretations and modalities, the same steps can be followed in other similar domains, such as autonomous driving or indoor robotics. In order to streamline the process of integrating external pre-trained experts for computer vision multi-modal multi-task learning (MTL) scenarios, we developed a data-pipeline software. Using this tool, we have created and released a fully-automated extension of the Dronescapes dataset. All the technical details, code and reproduction steps are publicly released.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10068","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.626454","language":"en","tags":["preprints","cscv","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":276,"author":"P\\^irvu Mihai-Cristian, Leordeanu Marius","raw_content_length":1955,"priority":7,"update_frequency":1,"reading_time_minutes":1.38,"robust_parsing_used":true,"entities":{"organizations":["Multiple Randomly Masked Autoencoders","Masked Autoencoders","MAE","PHG-MAE"],"persons":["Probabilistic Hyper-Graphs"],"locations":[],"monetary":[]},"char_count":1954,"language_detected":"en","key_concepts":{"key_phrases":["Probabilistic Hyper-Graphs","Multiple Randomly Masked Autoencoders","Semi-supervised Multi","arXiv251010068v1 Announce Type","new Abstract","The computer vision domain","an abundance","data","many modalities","various visual tasks"],"filter_categories":{"ai_ml":["Multiple Randomly Masked Autoencoders","The computer vision domain","data"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Probabilistic Hyper-Graphs":2.0,"Multiple Randomly Masked Autoencoders":2.0,"Semi-supervised Multi":2.0,"arXiv251010068v1 Announce Type":1.0,"new Abstract":1.0,"The computer vision domain":1.0,"an abundance":1.0,"data":1.0,"many modalities":1.0,"various visual tasks":1.0}},"age_hours":2.7479430069444444,"is_recent":true,"quality_score":1.0,"sentiment_score":8.9225,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7845,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.885,"joy":0.0312,"surprise":0.0546,"sadness":0.0048,"fear":0.0061,"anger":0.0127,"disgust":0.0057},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a novel machine learning model (PHG-MAE) for computer vision tasks, specifically focusing on UAV scenes. While the model itself doesn't directly address climate change, its potential lies in improving the efficiency and accuracy of data analysis in fields like autonomous driving and robotics, which could indirectly contribute to sustainability. The research is at an early stage, with no deployed units or quantified impact metrics related to climate or sustainability.","key_impact_metrics":[],"technology_tags":["machine learning","computer vision","masked autoencoders","hyper-graphs"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:06:53.727904Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_db1372df391b","title":"SyncLipMAE: Contrastive Masked Pretraining for Audio","content":"arXiv:2510.10069v1 Announce Type: new Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head/face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10069","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.626879","language":"en","tags":["computer-science","csai","preprints","csmm","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":178,"author":"Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou","raw_content_length":1451,"priority":7,"update_frequency":1,"reading_time_minutes":0.89,"robust_parsing_used":true,"entities":{"organizations":["Contrastive Masked Pretraining for Audio arXiv:2510.10069v1 Announce Type"],"persons":[],"locations":[],"monetary":[]},"char_count":1450,"language_detected":"en","key_concepts":{"key_phrases":["SyncLipMAE","Contrastive Masked Pretraining","Audio","arXiv251010069v1 Announce Type","new Abstract","a self-supervised pretraining framework","talking-face video","synchronization-aware and transferable facial dynamics","unlabeled audio-visual streams","Our approach couples"],"filter_categories":{"ai_ml":["Contrastive Masked Pretraining"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"SyncLipMAE":3.0,"Contrastive Masked Pretraining":2.0,"Audio":2.0,"arXiv251010069v1 Announce Type":1.0,"new Abstract":1.0,"a self-supervised pretraining framework":1.0,"talking-face video":1.0,"synchronization-aware and transferable facial dynamics":1.0,"unlabeled audio-visual streams":1.0,"Our approach couples":1.0}},"age_hours":2.747958573611111,"is_recent":true,"quality_score":1.0,"sentiment_score":6.3660000000000005,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2732,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9137,"joy":0.0123,"surprise":0.0443,"sadness":0.0042,"fear":0.007,"anger":0.0136,"disgust":0.0051},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel self-supervised pretraining framework. While the technology itself is innovative and shows promise in improving audio-visual synchronization and related tasks, it is currently in the basic research phase with no concrete deployment or measurable environmental impact. The article mentions state-of-the-art results, suggesting measurable performance improvements, but lacks information on real-world deployment or energy consumption.","key_impact_metrics":["State-of-the-art results in four task families"],"technology_tags":["Self-supervised learning","Audio-visual synchronization","Masked autoencoders"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:06:57.656908Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7b808c0c3c81","title":"ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning","content":"arXiv:2510.10071v1 Announce Type: new Abstract: Conventional continual pretraining (CPT) for large language model (LLM) domain adaptation often suffers from catastrophic forgetting and limited domain capacity. Existing strategies adopt layer expansion, introducing additional trainable parameters to accommodate new knowledge. However, the uniform expansion and updates still entangle general and domain learning, undermining its effectiveness. Our pilot studies reveal that LLMs exhibit functional specialization, where layers and units differentially encode general-critical capabilities, suggesting that parameter expansion and optimization should be function-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled Tuning for continual pretraining, a two-stage framework for domain-adaptive CPT. ADEPT first performs General-Competence Guided Selective Layer Expansion, duplicating layers least critical for the general domain to increase representational capacity while minimizing interference with general knowledge. It then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter units within expanded layers according to their general-domain importance and assigning asymmetric learning rates to balance knowledge injection and retention. Experiments on mathematical and medical benchmarks show that ADEPT outperforms full-parameter CPT by up to 5.76% on the general domain and 5.58% on the target domain with only 15% of parameters tuned and less than 50% training time. Ablation studies, theoretical analysis, and extended investigations further demonstrate the necessity of targeted expansion and decoupled optimization, providing new principles for efficient and robust domain-adaptive CPT. Our code is open-sourced at https://github.com/PuppyKnightUniversity/ADEPT","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10071","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.627294","language":"en","tags":["research","cslg","computer-science","preprints","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":223,"author":"Jinyang Zhang, Yue Fang, Hongxin Ding, Weibin Liao, Muyang Ye, Xu Chu, Junfeng Zhao, Yasha Wang","raw_content_length":1806,"priority":7,"update_frequency":1,"reading_time_minutes":1.115,"robust_parsing_used":true,"entities":{"organizations":["CPT","Adaptive Expansion","General-Competence Guided Selective Layer Expansion","ADEPT"],"persons":[],"locations":["criti"],"monetary":[]},"char_count":1805,"language_detected":"en","key_concepts":{"key_phrases":["ADEPT","Continual Pretraining","Adaptive Expansion","arXiv251010071v1 Announce Type","new Abstract","Conventional continual pretraining","CPT","large language model","LLM domain adaptation","catastrophic forgetting"],"filter_categories":{"ai_ml":["Continual Pretraining","large language model"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"ADEPT":2.0,"Continual Pretraining":2.0,"Adaptive Expansion":2.0,"arXiv251010071v1 Announce Type":1.0,"new Abstract":1.0,"Conventional continual pretraining":1.0,"CPT":1.0,"large language model":1.0,"LLM domain adaptation":1.0,"catastrophic forgetting":1.0}},"age_hours":2.7479728219444444,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2469999999999999,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7506,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.6111,"joy":0.0036,"surprise":0.0099,"sadness":0.0329,"fear":0.1289,"anger":0.0915,"disgust":0.1222},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (ADEPT) for continual pretraining of LLMs, demonstrating improved performance on mathematical and medical benchmarks. The concrete action is the development and testing of this algorithm, showing a performance increase of up to 5.76% on the general domain and 5.58% on the target domain with only 15% of parameters tuned and less than 50% training time. However, it is still in the research phase with no deployed applications yet.","key_impact_metrics":["5.76% performance increase on general domain","5.58% performance increase on target domain"],"technology_tags":["Large Language Models","Continual Pretraining","Domain Adaptation"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T11:07:00.931975Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_16f8b84d2ca6","title":"Unilaw","content":"arXiv:2510.10072v1 Announce Type: new Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across various domains, yet their capabilities in handling complex legal problems remains underexplored. In this paper, we introduce Unilaw-R1, a large language model tailored for legal reasoning. With a lightweight 7-billion parameter scale, Unilaw-R1 significantly reduces deployment cost while effectively tackling three core challenges in the legal domain: insufficient legal knowledge, unreliable reasoning logic, and weak business generalization. To address these issues, we first construct Unilaw-R1-Data, a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples. Based on this, we adopt a two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which significantly boosts the performance on complex legal reasoning tasks and supports interpretable decision-making in legal AI applications. To assess legal reasoning ability, we also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10072","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.627724","language":"en","tags":["preprints","research","computer-science","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Hua Cai, Shuang Zhao, Liang Zhang, Xuli Shen, Qing Xu, Weilin Shen, Zihao Wen, Tianke Ban","raw_content_length":1521,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["Unilaw-R1","Unilaw arXiv:2510.10072v1 Announce Type","SFT","Reinforcement Learning","Unilaw-R1-Data","CoT"],"persons":["Supervised Fine-Tuning"],"locations":[],"monetary":[]},"char_count":1520,"language_detected":"en","key_concepts":{"key_phrases":["Unilaw","Unilaw-R1","Announce Type","new Abstract","Reasoning-focused large language models","LLMs","various domains","their capabilities","complex legal problems","this paper"],"filter_categories":{"ai_ml":["Reasoning-focused large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Unilaw":2.0,"Unilaw-R1":2.0,"Announce Type":1.0,"new Abstract":1.0,"Reasoning-focused large language models":1.0,"LLMs":1.0,"various domains":1.0,"their capabilities":1.0,"complex legal problems":1.0,"this paper":1.0}},"age_hours":2.7479878063888887,"is_recent":true,"quality_score":1.0,"sentiment_score":7.7115,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5423,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8846,"joy":0.0243,"surprise":0.066,"sadness":0.0053,"fear":0.0059,"anger":0.0101,"disgust":0.0036},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a legal reasoning LLM, Unilaw-R1, and demonstrates its performance on legal benchmarks. While the model shows promise, it is still in the early stages of development and deployment. The potential climate impact is indirect, as it could potentially improve the efficiency of legal processes related to environmental regulations or climate-related disputes, but this is speculative and not directly measured.","key_impact_metrics":["Performance on LawBench and LexEval exceeding Qwen-2.5-7B-Instruct by 6.6%"],"technology_tags":["Large Language Models","Legal AI","Chain-of-Thought Reasoning"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T11:07:04.377131Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_091e3a468e70","title":"SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM","content":"arXiv:2510.10073v1 Announce Type: new Abstract: Large vision-language model (LVLM)-based web agents are emerging as powerful tools for automating complex online tasks. However, when deployed in real-world environments, they face serious security risks, motivating the design of security evaluation benchmarks. Existing benchmarks provide only partial coverage, typically restricted to narrow scenarios such as user-level prompt manipulation, and thus fail to capture the broad range of agent vulnerabilities. To address this gap, we present \\tool{}, the first holistic benchmark for evaluating the security of LVLM-based web agents. \\tool{} first introduces a unified evaluation suite comprising six simulated but realistic web environments (\\eg, e-commerce platforms, community forums) and includes 2,970 high-quality trajectories spanning diverse tasks and attack settings. The suite defines a structured taxonomy of six attack vectors spanning both user-level and environment-level manipulations. In addition, we introduce a multi-layered evaluation protocol that analyzes agent failures across three critical dimensions: internal reasoning, behavioral trajectory, and task outcome, facilitating a fine-grained risk analysis that goes far beyond simple success metrics. Using this benchmark, we conduct large-scale experiments on 9 representative LVLMs, which fall into three categories: general-purpose, agent-specialized, and GUI-grounded. Our results show that all tested agents are consistently vulnerable to subtle adversarial manipulations and reveal critical trade-offs between model specialization and security. By providing (1) a comprehensive benchmark suite with diverse environments and a multi-layered evaluation pipeline, and (2) empirical insights into the security challenges of modern LVLM-based web agents, \\tool{} establishes a foundation for advancing trustworthy web agent deployment.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10073","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.628151","language":"en","tags":["computer-science","preprints","cscv","cscr","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":247,"author":"Zonghao Ying, Yangguang Shao, Jianle Gan, Gan Xu, Junjie Shen, Wenxin Zhang, Quanchen Zou, Junzheng Shi, Zhenfei Yin, Mingchuan Zhang, Aishan Liu, Xianglong Liu","raw_content_length":1909,"priority":7,"update_frequency":1,"reading_time_minutes":1.235,"robust_parsing_used":true,"entities":{"organizations":["\\eg"],"persons":[],"locations":[],"monetary":[]},"char_count":1908,"language_detected":"en","key_concepts":{"key_phrases":["SecureWebArena","A Holistic Security Evaluation Benchmark","LVLM","arXiv251010073v1 Announce Type","new Abstract","Large vision-language model","LVLM-based web agents","powerful tools","complex online tasks","real-world environments"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"SecureWebArena":2.0,"A Holistic Security Evaluation Benchmark":2.0,"LVLM":2.0,"arXiv251010073v1 Announce Type":1.0,"new Abstract":1.0,"Large vision-language model":1.0,"LVLM-based web agents":1.0,"powerful tools":1.0,"complex online tasks":1.0,"real-world environments":1.0}},"age_hours":2.7480028944444443,"is_recent":true,"quality_score":1.0,"sentiment_score":8.591999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7184,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.1201,"joy":0.0051,"surprise":0.0082,"sadness":0.0139,"fear":0.8022,"anger":0.0453,"disgust":0.0053},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a benchmark for evaluating the security of LVLM-based web agents. While it doesn't directly reduce GHG emissions, it contributes to the development of more secure and reliable AI systems, which could indirectly support sustainability efforts in the future by enabling more efficient and trustworthy automation in various sectors. The benchmark includes 2,970 trajectories and a multi-layered evaluation protocol, indicating a significant research effort, but it remains in the pilot stage with simulated environments.","key_impact_metrics":["2,970 high-quality trajectories","6 simulated web environments"],"technology_tags":["Large Vision-Language Models","Web Agents","Security Evaluation"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T11:07:08.890136Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a83ba52e49e7","title":"Agentic Troubleshooting Guide Automation for Incident Management","content":"arXiv:2510.10074v1 Announce Type: new Abstract: Effective incident management in large-scale IT systems relies on troubleshooting guides (TSGs), but their manual execution is slow and error-prone. While recent advances in LLMs offer promise for automating incident management tasks, existing LLM-based solutions lack specialized support for several key challenges, including managing TSG quality issues, interpreting complex control flow, handling data-intensive queries, and exploiting execution parallelism. We first conducted an empirical study on 92 real-world TSGs, and, guided by our findings, we present StepFly, a novel end-to-end agentic framework for troubleshooting guide automation. Our approach features a three-stage workflow: the first stage provides a comprehensive guide together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the second stage performs offline preprocessing using LLMs to extract structured execution DAGs from unstructured TSGs and to create dedicated Query Preparation Plugins (QPPs); and the third stage executes online using a DAG-guided scheduler-executor framework with a memory system to guarantee correct workflow and support parallel execution of independent steps. Our empirical evaluation on a collection of real-world TSGs and incidents demonstrates that StepFly achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time and token consumption. Furthermore, it achieves a remarkable execution time reduction of 32.9% to 70.4% for parallelizable TSGs.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10074","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.628575","language":"en","tags":["preprints","csai","research","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Jiayi Mao, Liqun Li, Yanjie Gao, Zegang Peng, Shilin He, Chaoyun Zhang, Si Qin, Samia Khalid, Qingwei Lin, Saravan Rajmohan, Sitaram Lanka, Dongmei Zhang","raw_content_length":1537,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["Agentic Troubleshooting Guide Automation for Incident Management arXiv:2510.10074v1 Announce Type","LLM","TSG Mentor","StepFly"],"persons":[],"locations":[],"monetary":[]},"char_count":1536,"language_detected":"en","key_concepts":{"key_phrases":["Agentic Troubleshooting Guide Automation","Incident Management","arXiv251010074v1 Announce Type","new Abstract","Effective incident management","large-scale IT systems","troubleshooting guides","their manual execution","recent advances","LLMs"],"filter_categories":{"engineering":["Agentic Troubleshooting Guide Automation"],"ai_ml":["LLMs"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Agentic Troubleshooting Guide Automation":2.0,"Incident Management":2.0,"arXiv251010074v1 Announce Type":1.0,"new Abstract":1.0,"Effective incident management":1.0,"large-scale IT systems":1.0,"troubleshooting guides":1.0,"their manual execution":1.0,"recent advances":1.0,"LLMs":1.0}},"age_hours":2.7480183238888887,"is_recent":true,"quality_score":1.0,"sentiment_score":9.158,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8316,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.6402,"joy":0.0026,"surprise":0.0579,"sadness":0.1515,"fear":0.0827,"anger":0.0335,"disgust":0.0317},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel framework (StepFly) for automating troubleshooting in IT systems, leading to reduced execution time and token consumption. The 32.9% to 70.4% execution time reduction for parallelizable TSGs is a measurable outcome. However, it's still in the applied research stage with no mention of real-world deployment, hence the vaporware risk.","key_impact_metrics":["execution time reduction of 32.9% to 70.4%","94% success rate on GPT-4.1"],"technology_tags":["LLMs","automation","incident management"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T11:07:12.384274Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b93a1bc02144","title":"Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model","content":"arXiv:2510.10078v1 Announce Type: new Abstract: Although speech emotion recognition (SER) research has been advanced, thanks to deep learning methods, it still suffers from obtaining inputs from large quality-labelled training data. Data augmentation methods have been attempted to mitigate this issue, generative models have shown success among them recently. We propose a data augmentation framework that is aided by cross-modal information transfer and mutual information regularization. Mutual information based metric can serve as an indicator for the quality. Furthermore, we expand this data augmentation scope to multimodal inputs, thanks to mutual information ensureing dependency between modalities. Our framework was tested on three benchmark datasets: IEMOCAP, MSP-IMPROV and MSP-Podcast. The implementation was designed to generate input features that are fed into last layer for emotion classification. Our framework improved the performance of emotion prediction against existing works. Also, we discovered that our framework is able to generate new inputs without any cross-modal information.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10078","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.629811","language":"en","tags":["cslg","preprints","research","cssd","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":151,"author":"Chung-Soo Ahn, Rajib Rana, Sunil Sivadas, Carlos Busso, Jagath C. Rajapakse","raw_content_length":1109,"priority":7,"update_frequency":1,"reading_time_minutes":0.755,"robust_parsing_used":true,"entities":{"organizations":["MSP","fed","IEMOCAP","SER"],"persons":[],"locations":[],"monetary":[]},"char_count":1108,"language_detected":"en","key_concepts":{"key_phrases":["Speech Emotion Recognition","Mutual Information Regularized Generative Model","arXiv251010078v1 Announce Type","new Abstract","speech emotion recognition SER research","deep learning methods","inputs","large quality-labelled training data","Data augmentation methods","this issue"],"filter_categories":{"research_academic":["speech emotion recognition SER research"],"ai_ml":["deep learning methods"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Speech Emotion Recognition":2.0,"Mutual Information Regularized Generative Model":2.0,"arXiv251010078v1 Announce Type":1.0,"new Abstract":1.0,"speech emotion recognition SER research":1.0,"deep learning methods":1.0,"inputs":1.0,"large quality-labelled training data":1.0,"Data augmentation methods":1.0,"this issue":1.0}},"age_hours":2.748063055,"is_recent":true,"quality_score":1.0,"sentiment_score":9.036999999999999,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8074,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8161,"joy":0.0205,"surprise":0.0159,"sadness":0.0519,"fear":0.0288,"anger":0.0417,"disgust":0.0251},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":1,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a data augmentation framework for speech emotion recognition using generative models. While it improves performance on benchmark datasets, it's still in the applied research phase with no clear path to deployment or measurable environmental impact. The framework's potential for sustainability is limited as it primarily focuses on improving AI algorithms, not directly addressing environmental issues.","key_impact_metrics":["Performance improvement against existing works"],"technology_tags":["Speech Emotion Recognition","Generative Models","Data Augmentation"],"sdg_alignment":[],"analyzed_at":"2025-10-29T11:07:15.558173Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_063ca3fcff55","title":"How AI Companionship Develops: Evidence from a Longitudinal Study","content":"arXiv:2510.10079v1 Announce Type: new Abstract: The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.10079","published_date":"2025-10-14T04:00:00","collected_date":"2025-10-14T06:39:38.630197","language":"en","tags":["computer-science","csai","preprints","cshc","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":154,"author":"Angel Hsing-Chi Hwang, Fiona Li, Jacy Reese Anthis, Hayoun Noh","raw_content_length":1089,"priority":7,"update_frequency":1,"reading_time_minutes":0.77,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1088,"language_detected":"en","key_concepts":{"key_phrases":["How AI Companionship Develops","Evidence","a Longitudinal Study","Announce Type","new Abstract","The quickly growing popularity","AI companions","risks","mental health","personal wellbeing"],"filter_categories":{"ai_ml":["How AI Companionship Develops"],"research_academic":["a Longitudinal Study"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"How AI Companionship Develops":2.0,"Evidence":2.0,"a Longitudinal Study":2.0,"Announce Type":1.0,"new Abstract":1.0,"The quickly growing popularity":1.0,"AI companions":1.0,"risks":1.0,"mental health":1.0,"personal wellbeing":1.0}},"age_hours":2.7480789572222224,"is_recent":true,"quality_score":0.7,"sentiment_score":6.071999999999999,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2144,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7404,"joy":0.019,"surprise":0.0399,"sadness":0.0198,"fear":0.1481,"anger":0.022,"disgust":0.0108},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents research on AI companionship, focusing on user perceptions and longitudinal study results. While the research is interesting, it lacks concrete actions or measurable outcomes related to sustainability. The study identifies variables shaping AI companionship but doesn't demonstrate any direct environmental or social benefits.","key_impact_metrics":["N = 303 (Study 1)","N = 110 (Study 2)"],"technology_tags":["AI Companions","Chatbots"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T11:07:18.858291Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
