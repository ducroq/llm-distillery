# Investment-Risk Filter - Training Results Summary

**Date:** 2025-11-16
**Objective:** Compare Knowledge Distillation vs Instruction Tuning for investment-risk scoring
**Result:** âœ… Knowledge Distillation wins decisively

---

## Quick Navigation

### â­ Production Model (RECOMMENDED)
ğŸ“ `filters/investment-risk/v2_distillation/`
- **Val MAE:** 0.6711 (52.6% better than instruction tuning)
- **Status:** Production Ready
- **See:** `v2_distillation/README.md`

### ğŸ“Š Comparison Analysis
ğŸ“ `filters/investment-risk/v2_distillation/training_reports/`
- **Main Report:** `comparison_report.md`
- **Visualizations:** `mode_comparison_*.png`

### âš ï¸ Experimental Model (NOT RECOMMENDED)
ğŸ“ `filters/investment-risk/v2_instruction/`
- **Val MAE:** 1.4157 (underperformed significantly)
- **Status:** Archived for comparison only
- **See:** `v2_instruction/README.md`

---

## Results Summary

### Overall Performance

| Metric | Knowledge Distillation | Instruction Tuning | Winner |
|--------|----------------------|-------------------|--------|
| **Val MAE** | **0.6711** | 1.4157 | âœ… Distillation (52.6% better) |
| **Val RMSE** | **0.9303** | 1.7693 | âœ… Distillation (47.4% better) |
| **Max Tokens** | **512** | 1024 | âœ… Distillation (2x more efficient) |
| **Train/Val Gap** | +0.0463 | -0.0377 | âœ… Instruction (better generalization) |
| **Dimensions Won** | **8/8** | 0/8 | âœ… Distillation (clean sweep) |

### Per-Dimension Results

| Dimension | Distillation MAE | Instruction MAE | Improvement |
|-----------|-----------------|----------------|-------------|
| Macro Risk Severity | **0.6955** | 1.6431 | +57.7% |
| Credit Market Stress | **0.5597** | 1.0053 | +44.3% |
| Market Sentiment Extremes | **0.5906** | 1.1219 | +47.4% |
| Valuation Risk | **0.6384** | 1.2658 | +49.6% |
| Policy Regulatory Risk | **0.7363** | 1.6673 | +55.8% |
| Systemic Risk | **0.6366** | 1.3044 | +51.2% |
| Evidence Quality | **0.8622** | 1.7990 | +52.1% |
| Actionability | **0.6493** | 1.5191 | +57.3% |

---

## Directory Structure

```
filters/investment-risk/
â”œâ”€â”€ v2/                          # Filter specification (oracle-based)
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ prefilter.py
â”‚   â”œâ”€â”€ prompt-compressed.md
â”‚   â”œâ”€â”€ ground_truth_quality_report.md
â”‚   â”œâ”€â”€ package_validation.md
â”‚   â””â”€â”€ release_report.md
â”‚
â”œâ”€â”€ v2_distillation/             # â­ PRODUCTION MODEL
â”‚   â”œâ”€â”€ README.md                # Start here
â”‚   â”œâ”€â”€ model/                   # LoRA adapter weights
â”‚   â”œâ”€â”€ training_history.json
â”‚   â”œâ”€â”€ training_metadata.json
â”‚   â””â”€â”€ training_reports/
â”‚       â”œâ”€â”€ investment-risk_v2_distillation_training_report.docx
â”‚       â”œâ”€â”€ comparison_report.md              # Key comparison analysis
â”‚       â”œâ”€â”€ overall_metrics.png
â”‚       â”œâ”€â”€ per_dimension_mae.png
â”‚       â”œâ”€â”€ loss_curves.png
â”‚       â”œâ”€â”€ mode_comparison_mae.png           # Side-by-side comparison
â”‚       â”œâ”€â”€ mode_comparison_per_dimension.png
â”‚       â”œâ”€â”€ mode_comparison_improvement.png
â”‚       â””â”€â”€ training_summary.txt
â”‚
â””â”€â”€ v2_instruction/              # âš ï¸ EXPERIMENTAL (not recommended)
    â”œâ”€â”€ README.md                # Why this didn't work
    â”œâ”€â”€ model/
    â”œâ”€â”€ training_history.json
    â”œâ”€â”€ training_metadata.json
    â””â”€â”€ training_reports/
        â”œâ”€â”€ investment-risk_v2_instruction_training_report.docx
        â”œâ”€â”€ overall_metrics.png
        â”œâ”€â”€ per_dimension_mae.png
        â”œâ”€â”€ loss_curves.png
        â””â”€â”€ training_summary.txt
```

---

## Key Takeaways

### âœ… What Worked

1. **Knowledge Distillation is Superior for Regression**
   - Direct score learning beats reasoning-based learning
   - 52.6% better validation MAE
   - Won all 8 dimensions decisively

2. **Simpler is Better**
   - 512 tokens sufficient (vs 1024)
   - No prompt overhead needed
   - Faster inference, lower cost

3. **Training Infrastructure Works**
   - Both models trained successfully
   - Clean comparison methodology
   - Reproducible results

### âŒ What Didn't Work

1. **Instruction Tuning for Regression**
   - Adding reasoning didn't help score prediction
   - More complex doesn't mean better
   - 1024 token context was overkill

2. **Dual-Task Learning**
   - Learning reasoning + scores simultaneously hurt both
   - Split focus reduced score accuracy
   - Not worth the interpretability gain

### ğŸ“š Lessons Learned

1. **Default to Distillation** - For future filters with regression scoring
2. **Test Both Modes** - Comparison validates the choice
3. **Document Failures** - Negative results are valuable
4. **Package Everything** - All artifacts stay with the filter

---

## Recommendation

### For Production: Use v2_distillation

**Deploy:** `filters/investment-risk/v2_distillation/model/`

**Reasons:**
- âœ… Meets accuracy target (0.67 MAE vs <1.0)
- âœ… Most efficient (512 tokens)
- âœ… Fastest inference
- âœ… Lowest cost
- âœ… Best performance across all dimensions

**Next Steps:**
1. Deploy model to production pipeline
2. Monitor live performance
3. Collect edge cases for v3
4. Consider larger model (3B/7B) if more accuracy needed

### For Research: Keep v2_instruction

**Archive:** `filters/investment-risk/v2_instruction/`

**Value:**
- Documents what doesn't work
- Validates training strategy
- Reference for future experiments
- Comparison baseline

---

## Tools Used

All training analysis tools are now updated to output to filter packages by default:

1. **`training/plot_learning_curves.py`** - Generate training visualizations
2. **`training/generate_training_report.py`** - Create Word reports
3. **`training/compare_training_modes.py`** - Compare distillation vs instruction tuning

These tools automatically detect filter directories and output results to `training_reports/` subdirectories.

---

## Related Reports

- **Oracle Calibration:** `v2/ground_truth_quality_report.md` (5,150 articles scored)
- **Package Validation:** `v2/package_validation.md` (90 article validation)
- **Release Report:** `v2/release_report.md` (production readiness)
- **Training Comparison:** `v2_distillation/training_reports/comparison_report.md` â­

---

**For Questions:** Start with `v2_distillation/README.md` or the comparison report.
