# Decision: Prefilter Approach for sustainability_technology v1

**Date**: 2025-11-24
**Decision Maker**: Project Team
**Status**: âœ… APPROVED

---

## Decision

**Use keyword-based prefilter with negative keyword blocking for sustainability_technology v1 training data generation.**

Do NOT use semantic prefiltering at this stage.

---

## Context

We evaluated three prefiltering approaches to reduce false positives before oracle scoring:

1. **6-category semantic prefilter** - Failed (15% recall)
2. **2-category semantic prefilter** - Works (97.9% recall) but marginal benefit
3. **Keyword prefilter with negative blocking** - Simple, 100% recall, acceptable FP rate

---

## Options Considered

### Option A: Keyword Prefilter âœ… SELECTED

**Description**:
- Positive keywords for sustainability content (wide net, substring matching)
- Negative keywords for obvious off-topic content (2+ occurrence threshold)
- No semantic/ML component

**Pros**:
- âœ… 100% recall - catches all good articles
- âœ… Simple, fast, maintainable
- âœ… Transparent and debuggable
- âœ… No GPU/embedding dependencies
- âœ… Easy to adjust (add/remove keywords)

**Cons**:
- âš ï¸ 23.2% false positive rate
- âš ï¸ More oracle API calls

**Metrics**:
- Recall: 100%
- FP Rate: 23.2%
- Precision: 64.4%
- Speed: Instant
- Complexity: Low

---

### Option B: 2-Category Semantic Prefilter âŒ REJECTED

**Description**:
- Binary classification using sentence embeddings
- Categories: sustainability-related vs other topics
- Threshold 0.30 for best recall

**Pros**:
- âœ… Slightly better FP rate (22.3% vs 23.2%)
- âœ… Slightly better precision (65.9% vs 64.4%)
- âœ… More sophisticated approach

**Cons**:
- âŒ Loses 2.1% of good articles (97.9% recall)
- âŒ Adds 16.6s processing per 1K articles
- âŒ Requires embedding model + GPU
- âŒ More complex to maintain and debug
- âŒ Harder to explain why articles were blocked

**Metrics**:
- Recall: 97.9%
- FP Rate: 22.3%
- Precision: 65.9%
- Speed: 16.6s per 1K articles
- Complexity: High

**Cost-Benefit**: 4% FP reduction (saves ~$3 per 10K articles) doesn't justify:
- Missing 2.1% of good training examples
- Added complexity and maintenance burden
- GPU deployment requirement
- Slower processing

---

### Option C: 6-Category Semantic Prefilter âŒ REJECTED

**Description**:
- Multi-class classification with 6 categories
- Block "general news" and other off-topic categories

**Pros**:
- âœ… Very low FP rate (2.1%)

**Cons**:
- âŒ **Critical failure**: Only 15% recall
- âŒ Blocks 85% of good articles
- âŒ Too many categories create ambiguity
- âŒ "General news" catches legitimate sustainability content

**Metrics**:
- Recall: ~15% âš ï¸ UNACCEPTABLE
- FP Rate: 2.1%
- Complexity: Very High

**Conclusion**: Fundamentally flawed approach for this use case.

---

## Decision Criteria

### Priority 1: Recall (Weight: 50%)
- **Requirement**: â‰¥95% recall for training data quality
- **Winner**: Keyword (100%) > Semantic-2cat (97.9%) >> Semantic-6cat (15%)

### Priority 2: Simplicity (Weight: 25%)
- **Requirement**: Easy to maintain, debug, and adjust
- **Winner**: Keyword (simple) >> Semantic (complex)

### Priority 3: Cost-Effectiveness (Weight: 15%)
- **Requirement**: Balance API costs vs complexity
- **Winner**: Keyword (acceptable cost, no added complexity)

### Priority 4: FP Reduction (Weight: 10%)
- **Requirement**: Nice to have, but oracle handles scoring
- **Winner**: Semantic-2cat (22.3%) â‰ˆ Keyword (23.2%)

---

## Rationale

### 1. Recall is Non-Negotiable

For training data generation, missing good articles (false negatives) is worse than including bad articles (false positives):

- **False Negatives**: Permanently lost training examples, hurt model performance
- **False Positives**: Oracle scores them low (1.0-2.0), model learns to reject them

The 2.1% recall loss in semantic prefiltering means missing ~300 good articles in a 10K dataset. This is unacceptable.

### 2. Marginal Improvement Doesn't Justify Complexity

**FP reduction**: 23.2% â†’ 22.3% = 0.9 percentage points = 4% improvement

**Cost savings**: ~$3 per 10K articles (assuming $0.0075/article Ã— 400 fewer FPs)

**Added complexity**:
- Embedding model deployment
- GPU infrastructure requirement
- Harder debugging ("why was this blocked?")
- Maintenance burden
- Dependency on embedding quality

**Verdict**: Not worth it for $3 per 10K articles.

### 3. Oracle is the Real Filter

The prefilter's job is to cast a wide net and reduce obvious garbage. The oracle's job is to score relevance accurately.

**Division of labor**:
- **Prefilter**: Remove obviously irrelevant (sports, celebrities, weddings)
- **Oracle**: Score sustainability relevance on 1-10 scale
- **Student model**: Learn from oracle's scores

Trying to do semantic filtering at prefilter stage duplicates oracle's work with worse accuracy.

### 4. Simplicity Enables Iteration

With keyword prefiltering:
- Add/remove keywords in 5 minutes
- Debug blocked articles by checking keyword matches
- No model retraining needed
- No GPU infrastructure required

With semantic prefiltering:
- Need to retrain/adjust embeddings
- Harder to understand why classifications fail
- GPU required for deployment
- More moving parts = more failure modes

### 5. Training Data Needs Full Spectrum

The student model benefits from seeing the full range of relevance:
- **10/10**: Perfect sustainability technology articles
- **7-9/10**: Strong sustainability relevance
- **4-6/10**: Moderate relevance or tangential
- **1-3/10**: Weak or irrelevant (learns to reject)

Aggressive prefiltering removes the 1-3 range, making the model less robust at rejecting false positives in production.

---

## Implementation

### Approved Configuration

**File**: `filters/sustainability_technology/v1/prefilter.py`

**Positive Keywords** (~50 terms):
- Core: sustainability, sustainable, renewable, solar, wind, climate, carbon, emissions, biodiversity, conservation
- Tech: electric vehicle, energy storage, green hydrogen, carbon capture
- Policy: net zero, paris agreement, eu taxonomy
- Uses substring matching

**Negative Keywords** (~53 terms, 3 categories):
- **Sports**: soccer, football match, touchdown, nfl, nba, premier league, etc.
- **Entertainment**: kardashian, baldwin, reality show, red carpet, grammy, etc.
- **Lifestyle**: wedding dress, makeup tutorial, horoscope, lottery, etc.

**Blocking Logic**:
1. Check positive keywords â†’ If none found, BLOCK
2. Check negative keywords â†’ If 2+ occurrences, BLOCK
3. Otherwise, PASS to oracle

**Expected Performance**:
- Recall: 100%
- FP Rate: ~23.2%
- Pass Rate: ~70-75%

ðŸ“„ See: `PREFILTER_STRATEGY.md` for detailed implementation

---

## Success Metrics

### Training Data Quality (Post-Oracle)
- âœ… All high-relevance articles included (no false negatives from prefilter)
- âœ… Oracle scores distributed across full 1-10 range
- âœ… ~65% of passed articles score >3.0 (precision acceptable)

### Cost Efficiency
- âœ… Prefilter reduces corpus by ~25-30% (blocks obvious junk)
- âœ… Oracle API costs: ~$60-75 per 10K articles (acceptable budget)
- âœ… No additional GPU costs for prefiltering

### Maintainability
- âœ… Prefilter adjustments take <5 minutes
- âœ… Clear audit trail (keyword matches visible)
- âœ… No model retraining needed

---

## Next Steps

1. âœ… **Decision approved** - Use keyword prefilter
2. â³ **Generate 10K training dataset** - Run distillation with approved prefilter
3. â³ **Train student model** - Single-stage model on full relevance spectrum
4. â³ **Evaluate student** - Compare to oracle on held-out test set
5. â³ **Deploy to production** - If student achieves â‰¥0.90 oracle correlation

---

## Review & Reconsideration

### When to Reconsider Semantic Prefiltering

This decision should be revisited if:

1. **API costs become prohibitive** - If oracle costs exceed $200 per 10K articles
2. **Much better embeddings** - If new models achieve >99.5% recall with <10% FP rate
3. **Production deployment requirements** - If real-time filtering needs change
4. **Different use case** - If purpose shifts from training data to user-facing filtering

For now, keyword prefiltering is the right choice for training data generation.

### Review Schedule

- **After 10K dataset generation**: Verify FP rate is within expected range (20-25%)
- **After student training**: Assess if FP rate affected model quality
- **6 months from now**: Review if semantic embeddings have improved significantly

---

## Approval

**Decision**: Use keyword prefilter with negative keyword blocking

**Approved By**: Project Team
**Date**: 2025-11-24
**Status**: âœ… FINAL

---

## References

- **Evaluation summary**: `SEMANTIC_PREFILTER_EVALUATION_SUMMARY.md`
- **6-category results**: `SEMANTIC_EVALUATION_REPORT.md`
- **2-category results**: `SEMANTIC_IMPROVED_EVALUATION.md`
- **Prefilter strategy**: `PREFILTER_STRATEGY.md`
- **Implementation**: `prefilter.py`

---

## Appendix: Evaluation Data

### Comparison Table

| Metric | Keyword | Semantic-2cat | Semantic-6cat |
|--------|---------|---------------|---------------|
| Recall | 100% âœ… | 97.9% | ~15% âŒ |
| FP Rate | 23.2% | 22.3% | 2.1% |
| Precision | 64.4% | 65.9% | N/A |
| Speed | 0.0s âœ… | 16.6s | N/A |
| Complexity | Low âœ… | High | Very High |
| GPU Required | No âœ… | Yes | Yes |
| Maintainability | Excellent âœ… | Poor | Poor |

### Cost Analysis (per 10K articles)

**Scenario**: 10,000 raw articles â†’ prefilter â†’ oracle scoring

| Approach | Pass Rate | Oracle Calls | FPs | Oracle Cost | Infra Cost | Total |
|----------|-----------|--------------|-----|-------------|------------|-------|
| Keyword | 75% | 7,500 | ~1,740 | $56.25 | $0 | **$56.25** âœ… |
| Semantic | 72% | 7,200 | ~1,606 | $54.00 | ~$5 GPU | **$59.00** |
| Savings | -3% | -300 | -134 | **-$2.25** | | |

**Conclusion**: Semantic saves ~$2.25 per 10K but loses 2.1% recall and adds GPU costs. Not worth it.

---

## Document History

- **2025-11-24**: Initial decision - keyword prefilter approved
- **2025-11-23**: Semantic evaluation Phase 2 completed (2-category)
- **2025-11-22**: Semantic evaluation Phase 1 completed (6-category, failed)
- **2025-11-20**: Prefilter strategy with negative keywords approved
