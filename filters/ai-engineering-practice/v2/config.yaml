# AI Engineering Practice Filter - Version 1.0
# Surfaces articles providing evidence on AI-augmented engineering practice
# For research on digital engineer competencies (HAN University of Applied Sciences)
#
# v1.0 Redesign: Orthogonal dimensions to fix high correlations in initial calibration
# - Separated "what is described" from "how rigorous/who says it"
# - workflow_detail and practitioner_voice now independent of methodological_rigor

filter:
  name: ai-engineering-practice
  version: "1.0"
  created: "2025-12-18"
  description: "Surface articles providing evidence/insights on AI-augmented engineering practice for research on digital engineer competencies"
  philosophy: "Document the undocumented - prioritize authentic practice evidence over hype"

  scope:
    engineering_domains: "ALL (software, mechanical, electrical, embedded, etc.)"
    geography: "GLOBAL"
    temporal: "Fresh data - no historical filtering needed"
    education: "ALL (formal education + corporate training)"

  changelog_v1: |
    Redesigned dimensions for orthogonality after initial calibration showed:
    - practice_authenticity ↔ evidence_quality correlation: 0.93
    - educational_signal ↔ competency_clarity correlation: 0.90
    New dimensions separate WHAT is described from HOW rigorous/WHO says it.

prefilter:
  enabled: true
  expected_pass_rate: ~0.15
  description: "Blocks marketing fluff, listicles, consumer AI content, pure product announcements"

oracle:
  recommended: gemini-flash
  candidates:
    - gemini-flash
    - gemini-pro
    - claude-sonnet

scoring:
  dimensions:
    # === CONTENT DIMENSIONS (WHAT is described) ===

    workflow_detail:
      weight: 0.25
      category: content
      description: "Specific AI tool usage patterns and processes described"
      scale: |
        0-2: No workflows - marketing claims, announcements, or pure statistics
        3-4: Vague mentions ("we use AI tools") without process detail
        5-6: General workflow described but lacks specificity
        7-8: Detailed workflow with tool names, steps, decision points
        9-10: Ethnographic detail - specific commands, prompts, iteration patterns, time spent
      evidence_focus: "Tool names, step-by-step processes, decision criteria, concrete examples"
      independence_note: "Can score HIGH even in opinion pieces IF they describe concrete workflows"

    validation_coverage:
      weight: 0.20
      category: content
      description: "Methods for verifying/validating AI outputs"
      scale: |
        0-2: Not mentioned at all
        3-4: Passing mention ("we check the outputs")
        5-6: General discussion of validation challenges
        7-8: Specific validation methods described (testing, review, verification)
        9-10: Central topic with concrete techniques, metrics, examples
      evidence_focus: "Testing methods, review processes, quality gates, error detection, hallucination handling"
      independence_note: "Independent of rigor - anecdotal validation tips still score high here"

    # === QUALITY DIMENSIONS (HOW rigorous, WHO says it) ===

    methodological_rigor:
      weight: 0.20
      category: quality
      gatekeeper: true
      gatekeeper_threshold: 3
      gatekeeper_max_score: 3.0
      description: "Systematic data collection vs. opinion/anecdote"
      scale: |
        0-2: Pure opinion, speculation, marketing claims
        3-4: Anecdotal evidence, single examples, unsupported claims
        5-6: Some data cited (small survey, case study, metrics mentioned)
        7-8: Systematic data, clear methodology, multiple sources
        9-10: Rigorous methodology, peer-reviewed, replicable, large sample
      evidence_focus: "Sample sizes, methodology description, data sources, statistical analysis"
      independence_note: "A rigorous SURVEY can score HIGH here but LOW on workflow_detail"

    practitioner_voice:
      weight: 0.20
      category: quality
      description: "From actual engineers vs. vendors/analysts/academics"
      scale: |
        0-2: Vendor marketing, PR announcements, analyst speculation
        3-4: Academic/analyst perspective without practitioner input
        5-6: Includes some practitioner quotes or references
        7-8: Substantial practitioner perspective, interviews, first-hand accounts
        9-10: Direct practitioner authorship or extensive ethnographic access
      evidence_focus: "Named practitioners, direct quotes, job titles, company context, first-person accounts"
      independence_note: "A rigorous academic study with no practitioner quotes = HIGH rigor, LOW voice"

    educational_applicability:
      weight: 0.15
      category: quality
      description: "Can inform curriculum design, training programs, pedagogy"
      scale: |
        0-2: No educational relevance
        3-4: Tangential implications only
        5-6: Could inform training with some interpretation
        7-8: Directly applicable to curriculum or training design
        9-10: Explicit educational focus - pedagogy, assessment, competency frameworks
      evidence_focus: "Curriculum implications, training applicability, skill development insights, assessment relevance"
      independence_note: "Practitioner anecdote with teachable insight = HIGH applicability, LOW rigor"

  tiers:
    high:
      threshold: 6.0
      description: "High-value research evidence - strong practitioner accounts or rigorous studies"

    medium:
      threshold: 3.5
      description: "Useful supporting evidence - partial workflows, context, triangulation"

    low:
      threshold: 0.0
      description: "Background noise or not relevant to research"

  gatekeepers:
    rigor_gatekeeper:
      dimension: methodological_rigor
      threshold: 3
      max_score: 3.0
      reason: "Pure opinion/marketing cannot provide research evidence regardless of other qualities"

  content_type_caps:
    marketing_fluff:
      max_score: 2.0
      triggers:
        - "Product announcements, press releases, vendor marketing"
      exceptions:
        - "includes practitioner testimonials with specifics"

    listicles:
      max_score: 3.0
      triggers:
        - "Top 10 AI tools, Best AI assistants, AI tools roundup"
      exceptions:
        - "includes comparative analysis with methodology"

training:
  target_samples: 5000
  train_val_test_split: [0.8, 0.1, 0.1]
  quality_threshold: 0.7
  recommended_model: "Qwen2.5-1.5B"
  expected_mae: "<1.0 per dimension"

deployment:
  inference_time: "20-50ms"
  cost_per_article: "$0.00"
  use_cases:
    - "Research evidence aggregation"
    - "Literature monitoring"
    - "Trend identification"
