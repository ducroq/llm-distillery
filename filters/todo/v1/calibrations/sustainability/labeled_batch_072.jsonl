{"id":"science_arxiv_cs_b2d0e350c9d5","title":"Adjusting Initial Noise to Mitigate Memorization in Text","content":"arXiv:2510.08625v1 Announce Type: new Abstract: Despite their impressive generative capabilities, text-to-image diffusion models often memorize and replicate training data, prompting serious concerns over privacy and copyright. Recent work has attributed this memorization to an attraction basin-a region where applying classifier-free guidance (CFG) steers the denoising trajectory toward memorized outputs-and has proposed deferring CFG application until the denoising trajectory escapes this basin. However, such delays often result in non-memorized images that are poorly aligned with the input prompts, highlighting the need to promote earlier escape so that CFG can be applied sooner in the denoising process. In this work, we show that the initial noise sample plays a crucial role in determining when this escape occurs. We empirically observe that different initial samples lead to varying escape times. Building on this insight, we propose two mitigation strategies that adjust the initial noise-either collectively or individually-to find and utilize initial samples that encourage earlier basin escape. These approaches significantly reduce memorization while preserving image-text alignment.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08625","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.559057","language":"en","tags":["preprints","computer-science","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":163,"author":"Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee","raw_content_length":1205,"priority":7,"update_frequency":1,"reading_time_minutes":0.815,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1204,"language_detected":"en","key_concepts":{"key_phrases":["Adjusting","Initial Noise","Memorization","Text","the denoising trajectory","arXiv251008625v1 Announce Type","new Abstract","their impressive generative capabilities","image","training data"],"filter_categories":{"ai_ml":["training data"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Adjusting":2.0,"Initial Noise":2.0,"Memorization":2.0,"Text":2.0,"the denoising trajectory":2.0,"arXiv251008625v1 Announce Type":1.0,"new Abstract":1.0,"their impressive generative capabilities":1.0,"image":1.0,"training data":1.0}},"age_hours":2.760369245,"is_recent":true,"quality_score":0.7,"sentiment_score":4.1085,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1783,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.1727,"joy":0.0027,"surprise":0.0119,"sadness":0.0281,"fear":0.5543,"anger":0.2092,"disgust":0.021},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores methods to reduce memorization in text-to-image diffusion models, addressing privacy and copyright concerns. The concrete action involves adjusting initial noise to improve image-text alignment and reduce memorization. The evidence is based on empirical observations and proposed mitigation strategies, but it remains at the research stage with no deployed technology or real-world data.","key_impact_metrics":["Reduced memorization","Preserved image-text alignment"],"technology_tags":["Text-to-image diffusion models","Noise adjustment algorithms"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:23:12.441592Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_68ca36a666de","title":"From What to Why: Thought","content":"arXiv:2510.08626v1 Announce Type: new Abstract: Large Language Models (LLMs) have advanced recommendation capabilities through enhanced reasoning, but pose significant challenges for real-world deployment due to high inference costs. Conversely, while Small Language Models (SLMs) offer an efficient alternative, their reasoning capabilities for recommendation remain underexplored. Existing systems often use natural language rationales merely as unsupervised descriptive text, failing to harness their full potential as learning signals. In this work our main idea is to create a common understanding of user and items across multiple domains called Thought Space with SLMs instead of using LLMs' distilled knowledge. To that end we propose PULSE (Preference Understanding by Latent Semantic Embeddings), a framework that treats SLM-generated rationales as director learning signals, supervising them with interaction histories to jointly model user actions (what) and their semantic drivers (why). Existing methods consider only interactions such as sequences and embeddings, whereas PULSE treats rationales as first-class signals, this novel design yields embeddings that are more robust and generalizable. Extensive experiments demonstrate that PULSE outperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential recommendation models across multiple benchmark datasets. Furthermore, PULSE exhibits superior transferability in cross-domain recommendation and demonstrates strong performance on downstream tasks such as reasoning-oriented question answering. Our code is available \\href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08626","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.559501","language":"en","tags":["research","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":207,"author":"Prosenjit Biswas, Pervez Shaik, Abhinav Thorat, Ravi Kolla, Niranjan Pedanekar","raw_content_length":1680,"priority":7,"update_frequency":1,"reading_time_minutes":1.035,"robust_parsing_used":true,"entities":{"organizations":["Latent Semantic Embeddings","Small Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1679,"language_detected":"en","key_concepts":{"key_phrases":["What","arXiv251008626v1 Announce Type","new Abstract","Large Language Models","LLMs","advanced recommendation capabilities","enhanced reasoning","significant challenges","real-world deployment","high inference costs"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"What":2.0,"arXiv251008626v1 Announce Type":1.0,"new Abstract":1.0,"Large Language Models":1.0,"LLMs":1.0,"advanced recommendation capabilities":1.0,"enhanced reasoning":1.0,"significant challenges":1.0,"real-world deployment":1.0,"high inference costs":1.0}},"age_hours":2.760384478611111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.3895,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8779,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8421,"joy":0.0068,"surprise":0.058,"sadness":0.0433,"fear":0.0164,"anger":0.0212,"disgust":0.0122},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel framework, PULSE, for improving recommendation systems using small language models. While the research shows improved performance on benchmark datasets, it is still in the applied research stage with no real-world deployments. The potential climate impact is indirect, stemming from potentially optimizing resource consumption of recommendation systems, but this is not quantified.","key_impact_metrics":["Outperforms leading ID, CF, and LLM-based sequential recommendation models"],"technology_tags":["Small Language Models","Recommendation Systems","Machine Learning"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T09:23:15.542752Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_546fecb2bd2c","title":"A Denoising Diffusion","content":"arXiv:2510.08627v1 Announce Type: new Abstract: Denoising diffusion models (DDMs) offer a promising generative approach for combinatorial optimization, yet they often lack the robust exploration capabilities of traditional metaheuristics like evolutionary algorithms (EAs). We propose a Denoising Diffusion-based Evolutionary Algorithm (DDEA) framework that synergistically integrates these paradigms. It utilizes pre-trained DDMs for both high-quality and diverse population initialization and a novel diffusion-based recombination operator, trained via imitation learning against an optimal demonstrator. Evaluating DDEA on the Maximum Independent Set problem on Erd\\H{o}s-R\\'enyi graphs, we demonstrate notable improvements over DIFUSCO, a leading DDM solver. DDEA consistently outperforms it given the same time budget, and surpasses Gurobi on larger graphs under the same time limit, with DDEA's solution sizes being 3.9% and 7.5% larger on the ER-300-400 and ER-700-800 datasets, respectively. In out-of-distribution experiments, DDEA provides solutions of 11.6% higher quality than DIFUSCO under the same time limit. Ablation studies confirm that both diffusion initialization and recombination are crucial. Our work highlights the potential of hybridizing DDMs and EAs, offering a promising direction for the development of powerful machine learning solvers for complex combinatorial optimization problems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08627","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.560158","language":"en","tags":["csne","research","csdm","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":183,"author":"Joan Salv\\`a Soler, G\\\"unther R. Raidl","raw_content_length":1415,"priority":7,"update_frequency":1,"reading_time_minutes":0.915,"robust_parsing_used":true,"entities":{"organizations":["Gurobi","DDEA","the Maximum Independent Set","DIFUSCO","Evolutionary Algorithm (DDEA","DDM"],"persons":[],"locations":["Erd\\H{o}s","R\\'enyi"],"monetary":[]},"char_count":1414,"language_detected":"en","key_concepts":{"key_phrases":["A Denoising Diffusion","arXiv251008627v1 Announce Type","new Abstract","Denoising diffusion models","DDMs","a promising generative approach","combinatorial optimization","the robust exploration capabilities","traditional metaheuristics","evolutionary algorithms"],"filter_categories":{"ai_ml":["evolutionary algorithms"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Denoising Diffusion":2.0,"arXiv251008627v1 Announce Type":1.0,"new Abstract":1.0,"Denoising diffusion models":1.0,"DDMs":1.0,"a promising generative approach":1.0,"combinatorial optimization":1.0,"the robust exploration capabilities":1.0,"traditional metaheuristics":1.0,"evolutionary algorithms":1.0}},"age_hours":2.7604002152777776,"is_recent":true,"quality_score":1.0,"sentiment_score":9.158,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8316,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8376,"joy":0.0359,"surprise":0.0797,"sadness":0.0169,"fear":0.0113,"anger":0.0125,"disgust":0.0062},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel algorithm (DDEA) that improves upon existing methods for solving combinatorial optimization problems. The concrete action is the development and testing of this algorithm, with measurable outcomes including a 3.9% and 7.5% improvement in solution size on specific datasets compared to a leading DDM solver. It is currently in the applied research stage, with no indication of real-world deployment.","key_impact_metrics":["3.9% larger solution size on ER-300-400","7.5% larger solution size on ER-700-800"],"technology_tags":["Denoising Diffusion Models","Evolutionary Algorithms","Combinatorial Optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:23:23.208879Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1d025d7c7047","title":"The Digital Mirror: Gender Bias and Occupational Stereotypes in AI","content":"arXiv:2510.08628v1 Announce Type: new Abstract: Generative AI offers vast opportunities for creating visualisations, such as graphics, videos, and images. However, recent studies around AI-generated visualisations have primarily focused on the creation process and image quality, overlooking representational biases. This study addresses this gap by testing representation biases in AI-generated pictures in an occupational setting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram, compare. Additionally, the study discusses topics such as ageing and emotions in AI-generated images. As AI image tools are becoming more widely used, addressing and mitigating harmful gender biases becomes essential to ensure diverse representation in media and professional settings. In this study, over 750 AI-generated images of occupations were prompted. The thematic analysis results revealed that both DALL-E 3 and Ideogram reinforce traditional gender stereotypes in AI-generated images, although to varying degrees. These findings emphasise that AI visualisation tools risk reinforcing narrow representations. In our discussion section, we propose suggestions for practitioners, individuals and researchers to increase representation when generating images with visible genders.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08628","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.560589","language":"en","tags":["preprints","computer-science","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":171,"author":"Siiri Lepp\\\"alampi, Sonja M. Hyrynsalmi, Erno Vanhala","raw_content_length":1292,"priority":7,"update_frequency":1,"reading_time_minutes":0.855,"robust_parsing_used":true,"entities":{"organizations":["The Digital Mirror","Ideogram"],"persons":["Gender Bias","Occupational Stereotypes"],"locations":[],"monetary":[]},"char_count":1291,"language_detected":"en","key_concepts":{"key_phrases":["The Digital Mirror","Gender Bias","Occupational Stereotypes","arXiv251008628v1 Announce Type","new Abstract","Generative AI","vast opportunities","visualisations","graphics","videos"],"filter_categories":{"ai_ml":["Generative AI"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"The Digital Mirror":2.0,"Gender Bias":2.0,"Occupational Stereotypes":2.0,"arXiv251008628v1 Announce Type":1.0,"new Abstract":1.0,"Generative AI":1.0,"vast opportunities":1.0,"visualisations":1.0,"graphics":1.0,"videos":1.0}},"age_hours":2.7604164225,"is_recent":true,"quality_score":1.0,"sentiment_score":8.982,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7964,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8596,"joy":0.0084,"surprise":0.0505,"sadness":0.0153,"fear":0.0175,"anger":0.0188,"disgust":0.0298},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":4,"justice_equity":7,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This study analyzes AI image generators and finds they reinforce gender stereotypes. While it doesn't directly impact climate change, it addresses justice and equity concerns by highlighting biases in technology. The study prompted over 750 AI-generated images, providing a measurable outcome of bias detection.","key_impact_metrics":["750 AI-generated images prompted","Bias detected in AI-generated images"],"technology_tags":["AI image generation","Bias detection","Gender stereotypes"],"sdg_alignment":[5,10],"analyzed_at":"2025-10-29T09:23:26.252454Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_61c4e7fbcc20","title":"ExPO-HM: Learning to Explain","content":"arXiv:2510.08630v1 Announce Type: new Abstract: Hateful memes have emerged as a particularly challenging form of online abuse, motivating the development of automated detection systems. Most prior approaches rely on direct detection, producing only binary predictions. Such models fail to provide the context and explanations that real-world moderation requires. Recent Explain-then-Detect approaches, using Chain-of-Thought prompting or LMM agents, perform worse than simple SFT baselines, and even advanced post-training methods such as GRPO fail to close the gap. Our analysis identifies two key issues of such systems: important policy-relevant cues such as targets and attack types are not hypothesized by the model as a likely explanation; and the binary reward signal is insufficient to guide reasoning. To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), inspired by the training and evaluation process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum learning, and Conditional Decision Entropy (CDE) as both metric and reward for reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves state-of-the-art performance on binary detection, fine-grained classification, and reasoning quality, with up to 15\\% and 17\\% F1 improvement over the GRPO and DPO baselines, respectively. By moving hateful meme detection from simple binary alarms to explanation-driven detection, ExPO-HM provides accurate, interpretable, and actionable moderation support.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08630","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.561419","language":"en","tags":["research","computer-science","preprints","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":210,"author":"Jingbiao Mei, Mingsheng Sun, Jinghong Chen, Pengda Qin, Yuhong Li, Da Chen, Bill Byrne","raw_content_length":1550,"priority":7,"update_frequency":1,"reading_time_minutes":1.05,"robust_parsing_used":true,"entities":{"organizations":["GRPO","SFT","LMM"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1549,"language_detected":"en","key_concepts":{"key_phrases":["arXiv251008630v1","Announce Type","new Abstract","Hateful memes","a particularly challenging form","online abuse","the development","automated detection systems","Most prior approaches","direct detection"],"filter_categories":{"engineering":["the development"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv251008630v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Hateful memes":1.0,"a particularly challenging form":1.0,"online abuse":1.0,"the development":1.0,"automated detection systems":1.0,"Most prior approaches":1.0,"direct detection":1.0}},"age_hours":2.7604450566666667,"is_recent":true,"quality_score":1.0,"sentiment_score":0.6390000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8722,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.2685,"joy":0.0018,"surprise":0.0062,"sadness":0.0278,"fear":0.0489,"anger":0.1644,"disgust":0.4825},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":7,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes a novel AI algorithm (ExPO-HM) for detecting hateful memes. While not directly related to climate or environmental sustainability, it addresses a societal harm and promotes justice and equity by improving online moderation. The algorithm achieves state-of-the-art performance with up to 17% F1 improvement over baselines, but it is still in the applied research stage with no deployment mentioned.","key_impact_metrics":["F1 improvement over GRPO baseline: 15%","F1 improvement over DPO baseline: 17%"],"technology_tags":["Artificial Intelligence","Machine Learning","Hateful Meme Detection","Policy Optimization"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T09:23:29.345941Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4d5c43f20ec9","title":"Next Semantic Scale Prediction via Hierarchical Diffusion Language Models","content":"arXiv:2510.08632v1 Announce Type: new Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a novel family of discrete diffusion models for language modeling. HDLM builds on a hierarchical vocabulary where low-level tokens with detailed semantics are surjectively mapped to high-level tokens with coarse-grained meanings. In the forward process, each token is independently perturbed to its higher-level ancestor with more abstract semantics according to the scheduler, while in the reverse process the model progressively predicts the next, more detailed semantics. Taken together, HDLM provides a general time-varying next semantic scale prediction process for language modeling. We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner while including the existing MDLM as a special case. We also propose practical training techniques based on the insights. Extensive text generation experiments validate the effectiveness of HDLM, which demonstrates consistently lower validation and generative perplexity than baselines.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08632","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.562227","language":"en","tags":["research","preprints","cscl","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":157,"author":"Cai Zhou, Chenyu Wang, Dinghuai Zhang, Shangyuan Tong, Yifei Wang, Stephen Bates, Tommi Jaakkola","raw_content_length":1141,"priority":7,"update_frequency":1,"reading_time_minutes":0.785,"robust_parsing_used":true,"entities":{"organizations":["HDLM","MDLM","Next Semantic Scale Prediction"],"persons":[],"locations":[],"monetary":[]},"char_count":1140,"language_detected":"en","key_concepts":{"key_phrases":["Hierarchical Diffusion Language Models","Next Semantic Scale Prediction","HDLM","Announce Type","new Abstract","this paper","a novel family","discrete diffusion models","language modeling","a hierarchical vocabulary"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hierarchical Diffusion Language Models":3.0,"Next Semantic Scale Prediction":2.0,"HDLM":2.0,"Announce Type":1.0,"new Abstract":1.0,"this paper":1.0,"a novel family":1.0,"discrete diffusion models":1.0,"language modeling":1.0,"a hierarchical vocabulary":1.0}},"age_hours":2.760476165277778,"is_recent":true,"quality_score":1.0,"sentiment_score":4.8709999999999996,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":-0.0258,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8724,"joy":0.0104,"surprise":0.05,"sadness":0.0038,"fear":0.0234,"anger":0.0248,"disgust":0.0152},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a novel language model. While potentially improving efficiency in language processing, it is currently in the basic research phase with no deployed technology or measurable environmental outcomes. The potential for climate impact is theoretical and indirect, depending on how the model is used.","key_impact_metrics":["Validation perplexity","Generative perplexity"],"technology_tags":["Hierarchical Diffusion Language Models","Discrete Diffusion Models"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T09:23:32.396459Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_232fd0df706f","title":"Detection of high","content":"arXiv:2510.08637v1 Announce Type: new Abstract: High-frequency oscillations (HFOs) are a new biomarker for identifying the epileptogenic zone. Mapping HFO-generating regions can improve the precision of resection sites in patients with refractory epilepsy. However, detecting HFOs remains challenging, and their clinical features are not yet fully defined. Visual identification of HFOs is time-consuming, labor-intensive, and subjective. As a result, developing automated methods to detect HFOs is critical for research and clinical use. In this study, we developed a novel method for detecting HFOs in the ripple and fast ripple frequency bands (80-500 Hz). We validated it using both controlled datasets and data from epilepsy patients. Our method employs an unsupervised clustering technique to categorize events extracted from the time-frequency domain using the S-transform. The proposed detector differentiates HFOs events from spikes, background activity, and artifacts. Compared to existing detectors, our method achieved a sensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the controlled dataset. In epilepsy patients, our results showed a stronger correlation with surgical outcomes, with a ratio of 0.73 between HFOs rates in resected versus non-resected contacts. The study confirmed previous findings that HFOs are promising biomarkers of epileptogenicity in epileptic patients. Removing HFOs, especially fast ripple, leads to seizure freedom, while remaining HFOs lead to seizure recurrence.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08637","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.563137","language":"en","tags":["research","preprints","physicsmed-ph","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":214,"author":"Mostafa Mohammadpour, Mehdi Zekriyapanah Gashti, Yusif S. Gasimov","raw_content_length":1531,"priority":7,"update_frequency":1,"reading_time_minutes":1.07,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1530,"language_detected":"en","key_concepts":{"key_phrases":["HFOs","Detection","arXiv251008637v1 Announce Type","new Abstract","High-frequency oscillations","a new biomarker","the epileptogenic zone","Mapping HFO-generating regions","the precision","resection sites"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"HFOs":4.0,"Detection":2.0,"arXiv251008637v1 Announce Type":1.0,"new Abstract":1.0,"High-frequency oscillations":1.0,"a new biomarker":1.0,"the epileptogenic zone":1.0,"Mapping HFO-generating regions":1.0,"the precision":1.0,"resection sites":1.0}},"age_hours":2.7605083616666666,"is_recent":true,"quality_score":0.7,"sentiment_score":7.7115,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5423,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8303,"joy":0.0073,"surprise":0.0378,"sadness":0.021,"fear":0.0635,"anger":0.0195,"disgust":0.0205},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":8,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article describes a novel method for detecting HFOs, a biomarker for epilepsy. While it shows promising results in controlled datasets and epilepsy patients, it is still in the applied research phase and lacks deployment. The impact on climate change is negligible, as it focuses on medical technology.","key_impact_metrics":["sensitivity of 97.67%","precision of 98.57%"],"technology_tags":["HFO detection","epilepsy diagnosis","unsupervised clustering"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T09:23:35.490743Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_dec4b3e605bf","title":"Into the Rabbit Hull: From Task","content":"arXiv:2510.08638v1 Announce Type: new Abstract: DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts. In the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits \"Elsewhere\" concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular depth cues matching visual neuroscience principles. Following these functional results, we analyze the geometry and statistics of the concepts learned by the SAE. We found that representations are partly dense rather than strictly sparse. The dictionary evolves toward greater coherence and departs from maximally orthogonal ideals (Grassmannian frames). Within an image, tokens occupy a low dimensional, locally connected set persisting after removing position. These signs suggest representations are organized beyond linear sparsity alone. Synthesizing these observations, we propose a refined view: tokens are formed by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). This structure is grounded in Gardenfors' conceptual spaces and in the model's mechanism as multi-head attention produces sums of convex mixtures, defining regions bounded by archetypes. We introduce the Minkowski Representation Hypothesis (MRH) and examine its empirical signatures and implications for interpreting vision-transformer representations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08638","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.563600","language":"en","tags":["research","csai","preprints","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":251,"author":"Thomas Fel, Binxu Wang, Michael A. Lepori, Matthew Kowal, Andrew Lee, Randall Balestriero, Sonia Joseph, Ekdeep S. Lubana, Talia Konkle, Demba Ba, Martin Wattenberg","raw_content_length":1893,"priority":7,"update_frequency":1,"reading_time_minutes":1.255,"robust_parsing_used":true,"entities":{"organizations":["the Linear Representation Hypothesis","SAE"],"persons":[],"locations":[],"monetary":[]},"char_count":1886,"language_detected":"en","key_concepts":{"key_phrases":["the Rabbit Hull","Task","arXiv251008638v1 Announce Type","new Abstract","DINOv2","objects","scenes","actions","the nature","what"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"the Rabbit Hull":2.0,"Task":2.0,"arXiv251008638v1 Announce Type":1.0,"new Abstract":1.0,"DINOv2":1.0,"objects":1.0,"scenes":1.0,"actions":1.0,"the nature":1.0,"what":1.0}},"age_hours":2.760524528611111,"is_recent":true,"quality_score":1.0,"sentiment_score":5.8895,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1779,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.927,"joy":0.0086,"surprise":0.0368,"sadness":0.0031,"fear":0.0076,"anger":0.0094,"disgust":0.0074},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":1,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents research on improving the interpretability of AI models, specifically DINOv2. While improved AI interpretability could indirectly support sustainability efforts by enabling better analysis and optimization of related systems, the paper itself does not describe any concrete actions or measurable outcomes related to climate change or other sustainability dimensions. It is primarily theoretical research.","key_impact_metrics":[],"technology_tags":["AI","computer vision","interpretability","SAEs","transformers"],"sdg_alignment":[],"analyzed_at":"2025-10-29T09:23:38.985541Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7ce4b105c374","title":"Automating Android Build Repair: Bridging the Reasoning","content":"arXiv:2510.08640v1 Announce Type: new Abstract: Android is the largest mobile platform, yet automatically building applications remains a practical challenge. While Large Language Models (LLMs) show promise for code repair, their use for fixing Android build errors remains underexplored. To address this gap, we first introduce AndroidBuildBench, a benchmark of 1,019 build failures curated from the commit histories of 43 open-source Android projects. Each problem is paired with a verified solution from a subsequent commit, ensuring that fixes are feasible. Second, we propose GradleFixer, an LLM agent with domain-specific tools for inspecting and manipulating the Gradle build environment. GradleFixer achieves a resolve rate of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent that relies on a general-purpose shell. GradleFixer's success suggests that while LLMs possess the high-level knowledge to solve these failures, they struggle to translate this knowledge into effective low-level actions using a general-purpose shell. We demonstrate the effectiveness of a strategy we term Tool Bridging, which replaces general-purpose shell commands with domain-aware abstractions. We hypothesize this approach works through two mechanisms: 1) it provides tools in an API-like format that LLMs use more reliably, and 2) it constrains the action space to relevant operations. This approach bridges the gap between the model's high-level reasoning and effective low-level execution.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08640","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.564017","language":"en","tags":["research","csse","csai","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":209,"author":"Ha Min Son, Huan Ren, Xin Liu, Zhe Zhao","raw_content_length":1509,"priority":7,"update_frequency":1,"reading_time_minutes":1.045,"robust_parsing_used":true,"entities":{"organizations":["LLM","GradleFixer","Android"],"persons":["Gradle","Announce Type"],"locations":[],"monetary":[]},"char_count":1508,"language_detected":"en","key_concepts":{"key_phrases":["Android Build Repair","the Reasoning","arXiv251008640v1","Announce Type","Android","the largest mobile platform","applications","a practical challenge","Large Language Models","LLMs"],"filter_categories":{"ai_ml":["Android Build Repair","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Android Build Repair":2.0,"the Reasoning":2.0,"arXiv251008640v1":1.0,"Announce Type":1.0,"Android":1.0,"the largest mobile platform":1.0,"applications":1.0,"a practical challenge":1.0,"Large Language Models":1.0,"LLMs":1.0}},"age_hours":2.7605408125,"is_recent":true,"quality_score":1.0,"sentiment_score":4.2345,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.911,"joy":0.0028,"surprise":0.0275,"sadness":0.0253,"fear":0.0089,"anger":0.0149,"disgust":0.0096},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on automating Android build repair using LLMs. The concrete action is the development of GradleFixer, which achieved an 81.4% resolve rate on a benchmark of build failures. While promising, this is still in the research and prototype phase, and the sustainability impact is indirect and theoretical, potentially reducing energy consumption associated with software development.","key_impact_metrics":["Resolve rate of 81.4%"],"technology_tags":["Large Language Models","Automated Code Repair","Android Development"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:23:41.906133Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9e00575b1e62","title":"Generating Sizing Fields for Mesh Generation via GCN","content":"arXiv:2510.08645v1 Announce Type: new Abstract: The sizing field defined on a triangular background grid is pivotal for controlling the quality and efficiency of unstructured mesh generation. However, creating an optimal background grid that is geometrically conforming, computationally lightweight, and free from artifacts like banding is a significant challenge. This paper introduces a novel, adaptive background grid simplification (ABGS) framework based on a Graph Convolutional Network (GCN). We reformulate the grid simplification task as an edge score regression problem and train a GCN model to efficiently predict optimal edge collapse candidates. The model is guided by a custom loss function that holistically considers both geometric fidelity and sizing field accuracy. This data-driven approach replaces a costly procedural evaluation, accelerating the simplification process. Experimental results demonstrate the effectiveness of our framework across diverse and complex engineering models. Compared to the initial dense grids, our simplified background grids achieve an element reduction of 74%-94%, leading to a 35%-88% decrease in sizing field query times.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08645","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.564448","language":"en","tags":["csgr","research","preprints","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":161,"author":"Xunyang Zhu, Hongfei Ye, Yifei Wang, Taoran Liu, Jianjun Chen","raw_content_length":1175,"priority":7,"update_frequency":1,"reading_time_minutes":0.805,"robust_parsing_used":true,"entities":{"organizations":["a Graph Convolutional Network","ABGS","GCN","GCN arXiv:2510.08645v1 Announce Type: new Abstract"],"persons":["Generating Sizing Fields"],"locations":[],"monetary":[]},"char_count":1174,"language_detected":"en","key_concepts":{"key_phrases":["GCN","Sizing Fields","Mesh Generation","arXiv251008645v1 Announce Type","new Abstract","The sizing field","a triangular background grid","the quality","efficiency","unstructured mesh generation"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"GCN":3.0,"Sizing Fields":2.0,"Mesh Generation":2.0,"arXiv251008645v1 Announce Type":1.0,"new Abstract":1.0,"The sizing field":1.0,"a triangular background grid":1.0,"the quality":1.0,"efficiency":1.0,"unstructured mesh generation":1.0}},"age_hours":2.760555806111111,"is_recent":true,"quality_score":1.0,"sentiment_score":9.685500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9371,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8791,"joy":0.0424,"surprise":0.0416,"sadness":0.0042,"fear":0.0093,"anger":0.016,"disgust":0.0073},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for optimizing mesh generation using GCNs, leading to significant reductions in element count (74%-94%) and query times (35%-88%). While the research is promising and demonstrates quantifiable improvements, it is still in the applied research phase with no evidence of real-world deployment. The reduction in computational resources could indirectly reduce energy consumption in engineering simulations, contributing to sustainability.","key_impact_metrics":["element reduction of 74%-94%","decrease in sizing field query times of 35%-88%"],"technology_tags":["Graph Convolutional Network","Mesh Generation","Adaptive Background Grid Simplification"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:23:45.183064Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_50ce623db2dc","title":"Energy","content":"arXiv:2510.08646v1 Announce Type: new Abstract: Safety alignment of large language models (LLMs) faces a key challenge: current alignment techniques often only focus on improving safety against harmful prompts, causing LLMs to become over-cautious and refuse to respond to benign prompts. Therefore, a key objective of safe alignment is to enhance safety while simultaneously reducing false refusals. In this paper, we introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework designed to resolve this challenge through dynamic, inference-time intervention. We trained a lightweight, external Energy-Based Model (EBM) to assign high energy to undesirable (false refusal or jailbreak) states and low energy to desirable (helpful response or safe reject) ones. During inference, EBM maps the LLM's internal activations to an \"energy landscape\". We use the gradient of the energy function to dynamically steer the LLM's hidden states to low energy regions, correcting the model to generate a desirable response in real-time without modifying its weights. This method decouples behavioral control from the model's core knowledge, offering a flexible solution with minimal computational overhead. Extensive experiments across a wide range of models show our method successfully achieves this objective: it substantially lowers false refusal rates. For example, raising compliance on the ORB-H benchmark from 57.3% to 82.6% while maintaining the baseline safety performance. Our work presents an effective paradigm for building LLMs that achieve both low false refusal rates and high safety.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08646","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.564884","language":"en","tags":["statml","research","csai","preprints","cscl","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":229,"author":"Eric Hanchen Jiang, Weixuan Ou, Run Liu, Shengyuan Pang, Guancheng Wan, Ranjie Duan, Wei Dong, Kai-Wei Chang, XiaoFeng Wang, Ying Nian Wu, Xinfeng Li","raw_content_length":1604,"priority":7,"update_frequency":1,"reading_time_minutes":1.145,"robust_parsing_used":true,"entities":{"organizations":["EBM","EDS","LLM","Energy","Energy-Driven Steering"],"persons":[],"locations":[],"monetary":[]},"char_count":1603,"language_detected":"en","key_concepts":{"key_phrases":["Energy","LLMs","safety","new Abstract","Safety alignment","large language models","a key challenge","current alignment techniques","harmful prompts","benign prompts"],"filter_categories":{"hydrogen_energy":["Energy"],"renewable_energy":["Energy"],"ai_ml":["LLMs","large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Energy":2.0,"LLMs":2.0,"safety":2.0,"new Abstract":1.0,"Safety alignment":1.0,"large language models":1.0,"a key challenge":1.0,"current alignment techniques":1.0,"harmful prompts":1.0,"benign prompts":1.0}},"age_hours":2.7605722105555555,"is_recent":true,"quality_score":1.0,"sentiment_score":9.824,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9648,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7966,"joy":0.0167,"surprise":0.0053,"sadness":0.0072,"fear":0.0873,"anger":0.0564,"disgust":0.0305},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel method (EDS) for improving the safety of LLMs by reducing false refusal rates. The method is demonstrated through experiments, showing an increase in compliance on the ORB-H benchmark from 57.3% to 82.6%. However, this is still early-stage research with no real-world deployment or economic viability demonstrated, and the climate impact is indirect and speculative.","key_impact_metrics":["compliance increase on ORB-H benchmark: 25.3%"],"technology_tags":["Energy-Driven Steering","Large Language Models","AI Safety"],"sdg_alignment":[],"analyzed_at":"2025-10-29T09:23:48.110662Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_77fcab1c6cfd","title":"Upfront Chain-of-Thought: A Cooperative Framework for Chain","content":"arXiv:2510.08647v1 Announce Type: new Abstract: Recent developments have enabled advanced reasoning in Large Language Models (LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high computational costs and significant latency losses owing to the autoregressive nature of generative LLMs. CoT compression aims to improve efficiency in the reasoning process by reducing output length. Previous works trade reasoning efficiency by either laborious discrete prompt designing or the construction of external compressed CoT datasets that sacrifice key reasoning details. In this work, we propose Upfront CoT (UCoT): an efficient reasoning framework with upfront thought embedding to automate CoT compression. UCoT is a cooperative workflow involving a small model (compressor) and a large model (executor). The first stage of UCoT trains compressor to generate upfront thought embeddings rich in reasoning information for the executor, avoiding the drawbacks of manually designed prompts. The second stage optimizes executor to utilize upfront thought embeddings to derive the correct answer with short reasoning, using a reward mechanism. Extensive experiments show that UCoT maintains the powerful reasoning ability of executor while significantly reducing the length of CoT. It is worth mentioning that when applying UCoT to the Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by 50\\%, while the performance is 3.08\\% higher than that of the state-of-the-art (SOTA) method. The code and dataset are in supplementary material.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08647","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.565322","language":"en","tags":["research","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":224,"author":"Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Shaochu Zhang, Shengchao Liu, Guoxin Ma, Yu Lan, Chao Shen","raw_content_length":1566,"priority":7,"update_frequency":1,"reading_time_minutes":1.12,"robust_parsing_used":true,"entities":{"organizations":["UCoT","Large Language Models","Upfront CoT","CoT","Upfront Chain-of-Thought"],"persons":[],"locations":[],"monetary":[]},"char_count":1565,"language_detected":"en","key_concepts":{"key_phrases":["Thought","Upfront Chain","A Cooperative Framework","Chain","new Abstract","Recent developments","advanced reasoning","Large Language Models","LLMs","long CoT"],"filter_categories":{"ai_ml":["Upfront Chain","Large Language Models"],"engineering":["Recent developments"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Thought":3.0,"Upfront Chain":2.0,"A Cooperative Framework":2.0,"Chain":2.0,"new Abstract":1.0,"Recent developments":1.0,"advanced reasoning":1.0,"Large Language Models":1.0,"LLMs":1.0,"long CoT":1.0}},"age_hours":2.7605884105555556,"is_recent":true,"quality_score":1.0,"sentiment_score":7.997000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.5994,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8591,"joy":0.0093,"surprise":0.0135,"sadness":0.0082,"fear":0.0334,"anger":0.0392,"disgust":0.0373},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to compressing Chain-of-Thought in LLMs, reducing computational costs. The concrete action is the development of the UCoT framework and its application to the Qwen2.5-7B-Instruct model. Evidence includes a 50% reduction in token usage on the GSM8K dataset with a 3.08% performance increase, but this is still in the applied research stage without real-world deployment.","key_impact_metrics":["50% reduction in token usage","3.08% performance increase"],"technology_tags":["Large Language Models","Chain-of-Thought","AI Compression"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T09:23:51.200184Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b56ed32213a7","title":"Inverse","content":"arXiv:2510.08648v1 Announce Type: new Abstract: Large language models can change answers under harmless edits that matter in practice: RAG outputs flip when passages are reordered, fine-tuning erodes invariances learned at pretraining, debate or chain-of-thought prompts take path-dependent routes, and compiler fusion or reordering perturbs logits near decision boundaries. These failures violate intended invariances, break continuous integration, and force teams to trade safety for speed. The effects are small yet distributed across layers and positions, sensitive to context length and evaluation order, and costly to repair with retraining or formal verification. We present WILSON, a minimal post-hoc diagnostic suite that converts simple loop and reordering checks on internal representations into system signals. WILSON combines an inverse-free curvature map over positions and layers, computed with JVPs and Hutchinson probes, with activation-level commutators that flag reorder risk. Signals are cheap to compute, model-agnostic for standard Transformers, and exported as thresholds and CSV artifacts for orchestrators. This enables concrete actions: guard RAG against order effects, catch fine-tuning regressions, stabilize debate pathways and long multi-turn contexts, and gate fusions or reorders in deployment. In short, WILSON helps anticipate failures and approve safe optimizations so reliability and throughput can improve together without changing model architecture or training.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08648","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.565738","language":"en","tags":["research","csai","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":201,"author":"Edward Y. Chang, Ethan Y. Chang","raw_content_length":1501,"priority":7,"update_frequency":1,"reading_time_minutes":1.005,"robust_parsing_used":true,"entities":{"organizations":["WILSON","Hutchinson"],"persons":[],"locations":[],"monetary":[]},"char_count":1500,"language_detected":"en","key_concepts":{"key_phrases":["arXiv251008648v1 Announce Type","new Abstract","Large language models","answers","harmless edits","practice","passages","fine-tuning erodes invariances","debate","-thought"],"filter_categories":{"ai_ml":["Large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv251008648v1 Announce Type":1.0,"new Abstract":1.0,"Large language models":1.0,"answers":1.0,"harmless edits":1.0,"practice":1.0,"passages":1.0,"fine-tuning erodes invariances":1.0,"debate":1.0,"-thought":1.0}},"age_hours":2.7606055908333333,"is_recent":true,"quality_score":1.0,"sentiment_score":3.409,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3182,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7158,"joy":0.0088,"surprise":0.0558,"sadness":0.0174,"fear":0.0114,"anger":0.1226,"disgust":0.0683},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a diagnostic tool (WILSON) to improve the reliability of large language models, which indirectly supports sustainability by enabling safer and more efficient deployment of AI systems. The concrete action is the development and application of this diagnostic suite, supported by the description of its components and potential uses. However, it is still in the applied research phase with no deployed units or measured outcomes in a real-world sustainability context.","key_impact_metrics":[],"technology_tags":["Large Language Models","AI Reliability"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:23:54.244186Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_820e660aa55a","title":"Formalizing Style in Personal Narratives","content":"arXiv:2510.08649v1 Announce Type: new Abstract: Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08649","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.566140","language":"en","tags":["research","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Gustave Cortal (ENS Paris Saclay, LISN), Alain Finkel (ENS Paris Saclay)","raw_content_length":1273,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1272,"language_detected":"en","key_concepts":{"key_phrases":["Formalizing Style","Personal Narratives","Announce Type","new Abstract","Personal narratives","stories authors","meaning","their experiences","Style","the distinctive way"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Formalizing Style":2.0,"Personal Narratives":2.0,"Announce Type":1.0,"new Abstract":1.0,"Personal narratives":1.0,"stories authors":1.0,"meaning":1.0,"their experiences":1.0,"Style":1.0,"the distinctive way":1.0}},"age_hours":2.7606203411111108,"is_recent":true,"quality_score":0.7,"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8797,"joy":0.0155,"surprise":0.067,"sadness":0.0057,"fear":0.0145,"anger":0.0105,"disgust":0.0071},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":1,"deployment_readiness":1,"systemic_impact":1,"justice_equity":3,"innovation_quality":5,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This article presents a novel framework for analyzing stylistic choices in personal narratives using language models. While the research is interesting, it does not directly address climate change or environmental sustainability. The framework extracts linguistic features and links them to psychological states, but there is no concrete action or measurable outcome related to sustainability.","key_impact_metrics":[],"technology_tags":["Natural Language Processing","Language Models"],"sdg_alignment":[],"analyzed_at":"2025-10-29T09:23:57.171685Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f9d68b559246","title":"PhyDAE: Physics-Guided Degradation-Adaptive Experts for All","content":"arXiv:2510.08653v1 Announce Type: new Abstract: Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08653","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.566592","language":"en","tags":["preprints","computer-science","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":213,"author":"Zhe Dong, Yuzhe Sun, Haochen Jiang, Tianzhu Liu, Yanfeng Gu","raw_content_length":1927,"priority":7,"update_frequency":1,"reading_time_minutes":1.065,"robust_parsing_used":true,"entities":{"organizations":["Physics-Guided Degradation-Adaptive Experts","Physics-Guided Degradation-Adaptive Experts for All arXiv:2510.08653v1 Announce Type"],"persons":["atmospheric interference"],"locations":[],"monetary":[]},"char_count":1926,"language_detected":"en","key_concepts":{"key_phrases":["PhyDAE","Physics-Guided Degradation-Adaptive Experts","All","arXiv251008653v1 Announce Type","new Abstract","Remote sensing images","various degradation factors","acquisition","atmospheric interference","sensor limitations"],"filter_categories":{"business_innovation":["acquisition"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"PhyDAE":2.0,"Physics-Guided Degradation-Adaptive Experts":2.0,"All":2.0,"arXiv251008653v1 Announce Type":1.0,"new Abstract":1.0,"Remote sensing images":1.0,"various degradation factors":1.0,"acquisition":1.0,"atmospheric interference":1.0,"sensor limitations":1.0}},"age_hours":2.760635137777778,"is_recent":true,"quality_score":1.0,"sentiment_score":0.5330000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8934,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.555,"joy":0.006,"surprise":0.0261,"sadness":0.0991,"fear":0.2194,"anger":0.0456,"disgust":0.0487},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method (PhyDAE) for improving the quality of remote sensing images, which could indirectly contribute to sustainability by enhancing the accuracy of environmental monitoring and resource management. The method demonstrates superior performance on benchmark datasets and achieves efficiency gains, but it is still in the applied research stage with no deployed units. The impact is currently theoretical, as there are no real-world deployments or quantified reductions in environmental impact.","key_impact_metrics":["significant reductions in parameter count","computational complexity reduction"],"technology_tags":["remote sensing","image restoration","machine learning","degradation modeling"],"sdg_alignment":[9,13,15],"analyzed_at":"2025-10-29T09:24:00.391314Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_cb4e73566485","title":"Knowledge Graph Sparsification for GNN","content":"arXiv:2510.08655v1 Announce Type: new Abstract: Rare genetic disease diagnosis faces critical challenges: insufficient patient data, inaccessible full genome sequencing, and the immense number of possible causative genes. These limitations cause prolonged diagnostic journeys, inappropriate treatments, and critical delays, disproportionately affecting patients in resource-limited settings where diagnostic tools are scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires only patient phenotypes to identify the most likely causal gene and retrieve focused patient subgraphs for targeted clinical investigation. RareNet can function as a standalone method or serve as a pre-processing or post-processing filter for other candidate gene prioritization methods, consistently enhancing their performance while potentially enabling explainable insights. Through comprehensive evaluation on two biomedical datasets, we demonstrate competitive and robust causal gene prediction and significant performance gains when integrated with other frameworks. By requiring only phenotypic data, which is readily available in any clinical setting, RareNet democratizes access to sophisticated genetic analysis, offering particular value for underserved populations lacking advanced genomic infrastructure.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08655","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.566996","language":"en","tags":["research","csai","preprints","cslg","computer-science","q-biogn","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":162,"author":"Premt Cara, Kamilia Zaripova, David Bani-Harouni, Nassir Navab, Azade Farshad","raw_content_length":1316,"priority":7,"update_frequency":1,"reading_time_minutes":0.81,"robust_parsing_used":true,"entities":{"organizations":["RareNet","Graph Neural Network"],"persons":[],"locations":[],"monetary":[]},"char_count":1315,"language_detected":"en","key_concepts":{"key_phrases":["Knowledge Graph Sparsification","GNN","arXiv251008655v1 Announce Type","new Abstract","Rare genetic disease diagnosis","critical challenges","insufficient patient data","inaccessible full genome sequencing","the immense number","possible causative genes"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Knowledge Graph Sparsification":2.0,"GNN":2.0,"arXiv251008655v1 Announce Type":1.0,"new Abstract":1.0,"Rare genetic disease diagnosis":1.0,"critical challenges":1.0,"insufficient patient data":1.0,"inaccessible full genome sequencing":1.0,"the immense number":1.0,"possible causative genes":1.0}},"age_hours":2.7606502025,"is_recent":true,"quality_score":1.0,"sentiment_score":2.706,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.4588,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8508,"joy":0.0036,"surprise":0.0318,"sadness":0.0406,"fear":0.0383,"anger":0.0197,"disgust":0.0152},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":7,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"RareNet is a subgraph-based GNN that uses patient phenotypes to identify causal genes for rare genetic diseases. It enhances the performance of other gene prioritization methods and democratizes access to genetic analysis, particularly for underserved populations. The article mentions comprehensive evaluation on two biomedical datasets, but lacks information on real-world deployment and cost-effectiveness.","key_impact_metrics":["Significant performance gains when integrated with other frameworks","Competitive and robust causal gene prediction"],"technology_tags":["Graph Neural Network","Genetic Analysis","Rare Disease Diagnosis"],"sdg_alignment":[3,10],"analyzed_at":"2025-10-29T09:24:03.306596Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_26f378ba42fb","title":"A 3D Generation Framework from Cross Modality to Parameterized Primitive","content":"arXiv:2510.08656v1 Announce Type: new Abstract: Recent advancements in AI-driven 3D model generation have leveraged cross modality, yet generating models with smooth surfaces and minimizing storage overhead remain challenges. This paper introduces a novel multi-stage framework for generating 3D models composed of parameterized primitives, guided by textual and image inputs. In the framework, A model generation algorithm based on parameterized primitives, is proposed, which can identifies the shape features of the model constituent elements, and replace the elements with parameterized primitives with high quality surface. In addition, a corresponding model storage method is proposed, it can ensure the original surface quality of the model, while retaining only the parameters of parameterized primitives. Experiments on virtual scene dataset and real scene dataset demonstrate the effectiveness of our method, achieving a Chamfer Distance of 0.003092, a VIoU of 0.545, a F1-Score of 0.9139 and a NC of 0.8369, with primitive parameter files approximately 6KB in size. Our approach is particularly suitable for rapid prototyping of simple models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08656","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.567418","language":"en","tags":["csgr","research","csai","preprints","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":164,"author":"Yiming Liang, Huan Yu, Zili Wang, Shuyou Zhang, Guodong Yi, Jin Wang, Jianrong Tan","raw_content_length":1155,"priority":7,"update_frequency":1,"reading_time_minutes":0.82,"robust_parsing_used":true,"entities":{"organizations":["Cross Modality"],"persons":[],"locations":[],"monetary":[]},"char_count":1154,"language_detected":"en","key_concepts":{"key_phrases":["A 3D Generation Framework","Cross Modality","Parameterized Primitive","parameterized primitives","arXiv251008656v1 Announce Type","new Abstract","Recent advancements","AI-driven 3D model generation","leveraged cross modality","models"],"filter_categories":{"ai_ml":["AI-driven 3D model generation"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A 3D Generation Framework":2.0,"Cross Modality":2.0,"Parameterized Primitive":2.0,"parameterized primitives":2.0,"arXiv251008656v1 Announce Type":1.0,"new Abstract":1.0,"Recent advancements":1.0,"AI-driven 3D model generation":1.0,"leveraged cross modality":1.0,"models":1.0}},"age_hours":2.760665600277778,"is_recent":true,"quality_score":1.0,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.7637,"joy":0.0901,"surprise":0.101,"sadness":0.0044,"fear":0.0119,"anger":0.0217,"disgust":0.0072},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel framework for generating 3D models using parameterized primitives, which could potentially reduce storage overhead. The effectiveness is demonstrated through experiments with specific metrics like Chamfer Distance, VIoU, F1-Score, and NC. However, it's still in the research phase with no real-world deployment.","key_impact_metrics":["Chamfer Distance of 0.003092","VIoU of 0.545"],"technology_tags":["3D model generation","parameterized primitives","AI"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:06.348908Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b6c1dd13a75f","title":"Provably Robust Adaptation for Language","content":"arXiv:2510.08659v1 Announce Type: new Abstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP, have transformed multimodal learning by aligning visual (or graph) features with textual representations, enabling powerful downstream capabilities like few-shot learning. However, the reliance on small, task-specific support datasets collected in open environments exposes these models to poisoning attacks, where adversaries manipulate the support samples to degrade performance. Existing defenses rely on empirical strategies, which lack formal guarantees and remain vulnerable to unseen and adaptive attacks. Certified robustness offers provable guarantees but has been largely unexplored for few-shot classifiers based on LeFMs. This study seeks to fill these critical gaps by proposing the first provably robust few-shot classifier that is tailored for LeFMs. We term our model Language-empowered Few-shot Certification (\\textbf{LeFCert}). It integrates both textual and feature embeddings with an adaptive blending mechanism. To achieve provable robustness, we propose a twofold trimmed mean prototype and derive provable upper and lower bounds for classification scores, enabling certification under worst-case poisoning scenarios. To further enhance the performance, we extend LeFCert with two variants by considering a more realistic and tighter attack budget: LeFCert-L incorporates randomized smoothing to provide Lipschitz continuity and derive robustness under dual budget constraints, and LeFCert-C provides collective certification for scenarios where attackers distribute a shared poisoning budget across multiple samples. Experiments demonstrate that LeFCert achieves state-of-the-art performance, significantly improving both clean and certified accuracy compared to existing baselines. Despite its advanced robustness mechanisms, LeFCert is computationally efficient, making it practical for real-world applications.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08659","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.568245","language":"en","tags":["research","csai","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":248,"author":"Yuni Lai, Xiaoyu Xue, Linghui Shen, Yulun Wu, Gaolei Li, Song Guo, Kai Zhou, Bin Xiao","raw_content_length":1959,"priority":7,"update_frequency":1,"reading_time_minutes":1.24,"robust_parsing_used":true,"entities":{"organizations":["GraphCLIP","Certification","LeFMs","CLIP"],"persons":[],"locations":[],"monetary":[]},"char_count":1958,"language_detected":"en","key_concepts":{"key_phrases":["Provably Robust Adaptation","Language","Announce Type","new Abstract","Language-empowered foundation models","LeFMs","CLIP","GraphCLIP","multimodal learning","textual representations"],"filter_categories":{"ai_ml":["Language"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Provably Robust Adaptation":2.0,"Language":2.0,"Announce Type":1.0,"new Abstract":1.0,"Language-empowered foundation models":1.0,"LeFMs":1.0,"CLIP":1.0,"GraphCLIP":1.0,"multimodal learning":1.0,"textual representations":1.0}},"age_hours":2.7606973280555556,"is_recent":true,"quality_score":1.0,"sentiment_score":7.202,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4404,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.5901,"joy":0.0052,"surprise":0.0103,"sadness":0.0133,"fear":0.1033,"anger":0.168,"disgust":0.1098},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel approach to improving the robustness of language-empowered foundation models against poisoning attacks. While it doesn't directly address climate change, improved AI robustness could indirectly support sustainability efforts by making climate models and decision-making tools more reliable. The research is in the applied research stage, with experimental results but no real-world deployments yet.","key_impact_metrics":["Improved clean accuracy","Improved certified accuracy"],"technology_tags":["Machine Learning","AI Robustness","Few-shot Learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:09.231583Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_51a94610ee93","title":"How Scale Breaks \"Normalized Stress\" and KL Divergence: Rethinking Quality Metrics","content":"arXiv:2510.08660v1 Announce Type: new Abstract: Complex, high-dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two-dimensional scatter plots that visually capture some properties of the data. Because visually determining the accuracy of these plots is challenging, researchers often use quality metrics to measure the projection's accuracy and faithfulness to the original data. One of the most commonly employed metrics, normalized stress, is sensitive to uniform scaling (stretching, shrinking) of the projection, despite this act not meaningfully changing anything about the projection. Another quality metric, the Kullback--Leibler (KL) divergence used in the popular t-Distributed Stochastic Neighbor Embedding (t-SNE) technique, is also susceptible to this scale sensitivity. We investigate the effect of scaling on stress and KL divergence analytically and empirically by showing just how much the values change and how this affects dimension reduction technique evaluations. We introduce a simple technique to make both metrics scale-invariant and show that it accurately captures expected behavior on a small benchmark.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08660","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.568668","language":"en","tags":["statml","research","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":175,"author":"Kiran Smelser, Kaviru Gunaratne, Jacob Miller, Stephen Kobourov","raw_content_length":1274,"priority":7,"update_frequency":1,"reading_time_minutes":0.875,"robust_parsing_used":true,"entities":{"organizations":["the Kullback--Leibler"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1273,"language_detected":"en","key_concepts":{"key_phrases":["How Scale Breaks Normalized Stress","KL Divergence","Quality Metrics","arXiv251008660v1 Announce Type","new Abstract","Complex high-dimensional data","many scientific disciplines","machine learning","biology","the social sciences"],"filter_categories":{"ai_ml":["machine learning"],"research_academic":["the social sciences"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"How Scale Breaks Normalized Stress":2.0,"KL Divergence":2.0,"Quality Metrics":2.0,"arXiv251008660v1 Announce Type":1.0,"new Abstract":1.0,"Complex high-dimensional data":1.0,"many scientific disciplines":1.0,"machine learning":1.0,"biology":1.0,"the social sciences":1.0}},"age_hours":2.760714303055556,"is_recent":true,"quality_score":1.0,"sentiment_score":3.5199999999999996,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.296,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8431,"joy":0.0066,"surprise":0.0158,"sadness":0.0054,"fear":0.0766,"anger":0.0259,"disgust":0.0265},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":1,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper focuses on improving the quality metrics used in dimensionality reduction techniques, specifically addressing the scale sensitivity of normalized stress and KL divergence. While improved data visualization could indirectly support sustainability efforts by enabling better analysis of climate data, there are no concrete actions or measurable outcomes directly related to GHG emissions reduction or climate adaptation at this stage. The research is primarily theoretical, with a small benchmark used for validation.","key_impact_metrics":["Change in normalized stress value due to scaling","Change in KL divergence value due to scaling"],"technology_tags":["Dimensionality reduction","Data visualization","Machine learning"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:12.679452Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_28d4ee1a5156","title":"DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops","content":"arXiv:2510.08662v1 Announce Type: new Abstract: Genomic Selection (GS) uses whole-genome information to predict crop phenotypes and accelerate breeding. Traditional GS methods, however, struggle with prediction accuracy for complex traits and large datasets. We propose DPCformer, a deep learning model integrating convolutional neural networks with a self-attention mechanism to model complex genotype-phenotype relationships. We applied DPCformer to 13 traits across five crops (maize, cotton, tomato, rice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP data, ordered by chromosome, and employs the PMF algorithm for feature selection. Evaluations show DPCformer outperforms existing methods. In maize datasets, accuracy for traits like days to tasseling and plant height improved by up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On small-sample tomato data, the Pearson Correlation Coefficient for a key trait increased by up to 57.35%. In chickpea, the yield correlation was boosted by 16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample scenarios, and enhanced interpretability, providing a powerful tool for precision breeding and addressing global food security challenges.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08662","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.569474","language":"en","tags":["research","csai","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":170,"author":"Pengcheng Deng, Kening Liu, Mengxi Zhou, Mingxi Li, Rui Yang, Chuzhe Cao, Maojun Wang, Zeyu Zhang","raw_content_length":1257,"priority":7,"update_frequency":1,"reading_time_minutes":0.85,"robust_parsing_used":true,"entities":{"organizations":["An Interpretable Deep Learning Model for Genomic Prediction","PMF","SNP","DPCformer"],"persons":[],"locations":[],"monetary":[]},"char_count":1256,"language_detected":"en","key_concepts":{"key_phrases":["DPCformer An Interpretable Deep Learning Model","Genomic Prediction","Crops","DPCformer","Announce Type","new Abstract","Genomic Selection","whole-genome information","crop phenotypes","breeding"],"filter_categories":{"ai_ml":["DPCformer An Interpretable Deep Learning Model"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"DPCformer An Interpretable Deep Learning Model":2.0,"Genomic Prediction":2.0,"Crops":2.0,"DPCformer":2.0,"Announce Type":1.0,"new Abstract":1.0,"Genomic Selection":1.0,"whole-genome information":1.0,"crop phenotypes":1.0,"breeding":1.0}},"age_hours":2.7607455836111114,"is_recent":true,"quality_score":1.0,"sentiment_score":3.409,"sentiment_category":"negative","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.3182,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.7797,"joy":0.0123,"surprise":0.021,"sadness":0.0075,"fear":0.1151,"anger":0.0407,"disgust":0.0238},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":6,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a deep learning model (DPCformer) for genomic prediction in crops, showing improved accuracy in predicting traits like days to tasseling, plant height, fiber traits, and yield across multiple crops. The model's performance is quantified with specific percentage improvements, but it is still in the applied research stage with no indication of commercial deployment. The potential climate impact comes from improved crop yields and resource efficiency, but this is indirect and depends on broader adoption and sustainable farming practices.","key_impact_metrics":["accuracy improved by up to 2.92% in maize","accuracy gains for fiber traits reached 8.37% in cotton"],"technology_tags":["genomic selection","deep learning","precision breeding"],"sdg_alignment":[2,13],"analyzed_at":"2025-10-29T09:24:16.376275Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_510f656ca701","title":"A Novel Framework for Augmenting Rating Scale Tests with LLM","content":"arXiv:2510.08663v1 Announce Type: new Abstract: Psychological assessments typically rely on structured rating scales, which cannot incorporate the rich nuance of a respondent's natural language. This study leverages recent LLM advances to harness qualitative data within a novel conceptual framework, combining LLM-scored text and traditional rating-scale items to create an augmented test. We demonstrate this approach using depression as a case study, developing and assessing the framework on a real-world sample of upper secondary students (n=693) and corresponding synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved statistically significant improvements in measurement precision and accuracy. The information gain from the LLM items was equivalent to adding between 6.3 (real data) and 16.0 (synthetic data) items to the original 19-item test. Our approach marks a conceptual shift in automated scoring that bypasses its typical bottlenecks: instead of relying on pre-labelled data or complex expert-created rubrics, we empirically select the most informative LLM scoring instructions based on calculations of item information. This framework provides a scalable approach for leveraging the growing stream of transcribed text to enhance traditional psychometric measures, and we discuss its potential utility in clinical health and beyond.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08663","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.569884","language":"en","tags":["research","csai","preprints","cscl","cscy","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":189,"author":"Joe Watson, Ivan O'Conner, Chia-Wen Chen, Luning Sun, Fang Luo, David Stillwell","raw_content_length":1372,"priority":7,"update_frequency":1,"reading_time_minutes":0.945,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":[],"locations":[],"monetary":[]},"char_count":1371,"language_detected":"en","key_concepts":{"key_phrases":["A Novel Framework","Rating Scale Tests","LLM","arXiv251008663v1 Announce Type","new Abstract","Psychological assessments","structured rating scales","which","the rich nuance","a respondents natural language"],"filter_categories":{"ai_ml":["LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"A Novel Framework":2.0,"Rating Scale Tests":2.0,"LLM":2.0,"arXiv251008663v1 Announce Type":1.0,"new Abstract":1.0,"Psychological assessments":1.0,"structured rating scales":1.0,"which":1.0,"the rich nuance":1.0,"a respondents natural language":1.0}},"age_hours":2.7607614977777777,"is_recent":true,"quality_score":1.0,"sentiment_score":5.7355,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1471,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8769,"joy":0.0269,"surprise":0.0583,"sadness":0.0062,"fear":0.0092,"anger":0.0127,"disgust":0.0099},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents a novel framework using LLMs to augment psychological assessments. While the research demonstrates improved measurement precision and accuracy in a specific case study (depression assessment), it's early-stage and lacks clear connection to direct climate impact or economic viability. The study uses real-world data (n=693) and synthetic data (n=3000) to validate the framework.","key_impact_metrics":["Information gain equivalent to adding 6.3-16.0 items","Sample size n=693"],"technology_tags":["LLM","Psychological Assessment","Data Augmentation"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T09:24:19.524474Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1c5bd1bf245c","title":"Faver: Boosting LLM","content":"arXiv:2510.08664v1 Announce Type: new Abstract: LLM-based RTL generation is an interesting research direction, as it holds the potential to liberate the least automated stage in the current chip design. However, due to the substantial semantic gap between high-level specifications and RTL, coupled with limited training data, existing models struggle with generation accuracy. Drawing on human experience, design with verification helps improving accuracy. However, as the RTL testbench data are even more scarce, it is not friendly for LLMs. Although LLMs excel at higher-level languages like Python/C, they have a huge semantic gap from RTL. When implementing the same functionality, Python/C code and hardware code differ significantly in the spatiotemporal granularity, requiring the LLM not only to consider high-level functional semantics but also to ensure the low-level details align with the circuit code. It is not an easy task. In this paper, we propose a function abstracted verifiable middleware (Faver) that streamlines RTL verification in LLM-based workflows. By mixing LLM-friendly code structures with a rule-based template, Faver decouples the details of circuit verification, allowing the LLM to focus on the functionality itself. In our experiments on the SFT model and open-source models, Faver improved the model's generation accuracy by up to 14%.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08664","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.570310","language":"en","tags":["research","csse","csai","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":201,"author":"Jianan Mu, Mingyu Shi, Yining Wang, Tianmeng Yang, Bin Sun, Xing Hu, Jing Ye, Huawei Li","raw_content_length":1372,"priority":7,"update_frequency":1,"reading_time_minutes":1.005,"robust_parsing_used":true,"entities":{"organizations":["RTL","LLM","Python/C"],"persons":[],"locations":[],"monetary":[]},"char_count":1371,"language_detected":"en","key_concepts":{"key_phrases":["Faver","LLM","arXiv251008664v1 Announce Type","new Abstract","LLM-based RTL generation","an interesting research direction","the potential","the least automated stage","the current chip design","the substantial semantic gap"],"filter_categories":{"ai_ml":["LLM"],"research_academic":["an interesting research direction"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Faver":2.0,"LLM":2.0,"arXiv251008664v1 Announce Type":1.0,"new Abstract":1.0,"LLM-based RTL generation":1.0,"an interesting research direction":1.0,"the potential":1.0,"the least automated stage":1.0,"the current chip design":1.0,"the substantial semantic gap":1.0}},"age_hours":2.76077753,"is_recent":true,"quality_score":1.0,"sentiment_score":8.982,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7964,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8877,"joy":0.0247,"surprise":0.0244,"sadness":0.0203,"fear":0.011,"anger":0.0216,"disgust":0.0104},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper proposes a middleware (Faver) to improve the accuracy of LLM-based RTL generation for chip design. The concrete action is the development and testing of this middleware, with a reported 14% improvement in generation accuracy. However, this is still in the research phase with no deployed units or economic viability demonstrated.","key_impact_metrics":["Generation accuracy improvement 14%"],"technology_tags":["LLM","RTL generation","Chip design"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:22.101163Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_0c82b01541eb","title":"dInfer: An Efficient Inference Framework for Diffusion Language Models","content":"arXiv:2510.08666v1 Announce Type: new Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components-model, diffusion iteration manager, decoding strategy, and KV-cache manager-and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to prior systems, dInfer delivers $10\\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared with AR models (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with latest vLLM inference engine, dInfer still deliverers $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08666","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.571131","language":"en","tags":["research","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":186,"author":"Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng","raw_content_length":1463,"priority":7,"update_frequency":1,"reading_time_minutes":0.93,"robust_parsing_used":true,"entities":{"organizations":["Diffusion Language Models arXiv:2510.08666v1"],"persons":[],"locations":[],"monetary":[]},"char_count":1462,"language_detected":"en","key_concepts":{"key_phrases":["dInfer","An Efficient Inference Framework","Diffusion Language Models","arXiv251008666v1 Announce Type","new Abstract","Diffusion-based large language models","a promising alternative","autoregressive AR LLMs","denoising-based generation","inherent parallelism"],"filter_categories":{"ai_ml":["Diffusion-based large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"dInfer":4.0,"An Efficient Inference Framework":2.0,"Diffusion Language Models":2.0,"arXiv251008666v1 Announce Type":1.0,"new Abstract":1.0,"Diffusion-based large language models":1.0,"a promising alternative":1.0,"autoregressive AR LLMs":1.0,"denoising-based generation":1.0,"inherent parallelism":1.0}},"age_hours":2.7608085469444443,"is_recent":true,"quality_score":1.0,"sentiment_score":9.063,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8126,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8305,"joy":0.0846,"surprise":0.0616,"sadness":0.0066,"fear":0.0044,"anger":0.0085,"disgust":0.0038},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents dInfer, an efficient inference framework for diffusion language models, achieving significant speedups compared to existing systems (10x over Fast-dLLM, 2-3x over QWen2.5-3B). While the framework is open-sourced, it's still in the applied research stage, lacking deployment data and customer contracts. The climate impact is indirect, potentially reducing energy consumption of large language model inference, but not directly quantified.","key_impact_metrics":["1,100 tokens per second on HumanEval","800 tokens per second across six benchmarks"],"technology_tags":["diffusion language models","inference framework","model optimization"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T09:24:25.259678Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_11661378f842","title":"RAG4Tickets: AI","content":"arXiv:2510.08667v1 Announce Type: new Abstract: Modern software teams frequently encounter delays in resolving recurring or related issues due to fragmented knowledge scattered across JIRA tickets, developer discussions, and GitHub pull requests (PRs). To address this challenge, we propose a Retrieval-Augmented Generation (RAG) framework that integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to deliver context-aware ticket resolution recommendations. The approach embeds historical JIRA tickets, user comments, and linked PR metadata to retrieve semantically similar past cases, which are then synthesized by a Large Language Model (LLM) into grounded and explainable resolution suggestions. The framework contributes a unified pipeline linking JIRA and GitHub data, an embedding and FAISS indexing strategy for heterogeneous software artifacts, and a resolution generation module guided by retrieved evidence. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics shows that the proposed system significantly improves resolution accuracy, fix quality, and knowledge reuse in modern DevOps environments.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08667","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.571547","language":"en","tags":["research","csse","csai","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":153,"author":"Mohammad Baqar","raw_content_length":1201,"priority":7,"update_frequency":1,"reading_time_minutes":0.765,"robust_parsing_used":true,"entities":{"organizations":["Sentence-Transformers","GitHub","AI arXiv:2510.08667v1 Announce Type: new Abstract"],"persons":["a Large Language Model"],"locations":[],"monetary":[]},"char_count":1200,"language_detected":"en","key_concepts":{"key_phrases":["RAG4Tickets","arXiv251008667v1","Announce Type","new Abstract","Modern software teams","delays","recurring or related issues","fragmented knowledge","JIRA tickets","developer discussions"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"RAG4Tickets":2.0,"arXiv251008667v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Modern software teams":1.0,"delays":1.0,"recurring or related issues":1.0,"fragmented knowledge":1.0,"JIRA tickets":1.0,"developer discussions":1.0}},"age_hours":2.7608243575,"is_recent":true,"quality_score":1.0,"sentiment_score":7.2715,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4543,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.89,"joy":0.0032,"surprise":0.0386,"sadness":0.023,"fear":0.0244,"anger":0.0152,"disgust":0.0056},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":5,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a RAG framework for improving software development efficiency. While it mentions resolution time reduction as a metric, the direct link to climate impact is weak and theoretical, relying on the assumption that faster software development indirectly reduces energy consumption. The technology is in the applied research phase, with experimental evaluation but no real-world deployment data.","key_impact_metrics":["resolution time reduction","developer acceptance metrics"],"technology_tags":["AI","Retrieval-Augmented Generation","Large Language Model"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:28.759232Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_bec2cae76704","title":"Hulu","content":"arXiv:2510.08668v1 Announce Type: new Abstract: Real-world clinical decision-making grapples with integrating information from diverse data modalities, including medical text, 2D/3D images, and video, leading to inefficiencies and potential diagnostic oversights. While generalist vision-language models (VLMs) offer promise, their medical development faces challenges of opaque pipelines, data scarcity, and architectural inflexibility. Here we present Hulu-Med, a transparent medical VLM that unifies understanding across all these modalities. Built upon a unified patch-based vision encoder and an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M) samples to scale from 2D to 3D and video comprehension. The medical-aware token reduction enables efficient training, requiring only 4,000 to 40,000 GPU hours for 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks exhibits state-of-the-art performance, surpassing leading open-source models and competing with proprietary systems in tasks spanning visual question-answering, medical report generation, and complex reasoning in multilingual and rare disease scenarios. By open-sourcing our complete pipeline, we establish that high-performance medical VLM can be achieved transparently, providing a foundational tool for accessible and impactful clinical AI. Code is released on \\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08668","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.571947","language":"en","tags":["preprints","computer-science","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":178,"author":"Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao Tang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao, Bin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun, Jian Wu, Zuozhu Liu","raw_content_length":1453,"priority":7,"update_frequency":1,"reading_time_minutes":0.89,"robust_parsing_used":true,"entities":{"organizations":["VLM","LLM","Hulu-Med"],"persons":[],"locations":[],"monetary":[]},"char_count":1452,"language_detected":"en","key_concepts":{"key_phrases":["Hulu","arXiv251008668v1 Announce Type","new Abstract","Real-world clinical decision-making grapples","information","diverse data modalities","medical text","2D3D images","video","inefficiencies"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hulu":2.0,"arXiv251008668v1 Announce Type":1.0,"new Abstract":1.0,"Real-world clinical decision-making grapples":1.0,"information":1.0,"diverse data modalities":1.0,"medical text":1.0,"2D3D images":1.0,"video":1.0,"inefficiencies":1.0}},"age_hours":2.760840321111111,"is_recent":true,"quality_score":0.7,"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9291,"joy":0.0143,"surprise":0.0194,"sadness":0.005,"fear":0.018,"anger":0.009,"disgust":0.0051},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a new medical VLM, Hulu-Med, trained on 16.7M samples. While it shows promise in improving clinical decision-making, its direct climate impact is minimal as it's primarily focused on improving healthcare efficiency. It's still in the applied research stage with no deployment data available, hence the vaporware flag.","key_impact_metrics":["16.7M training samples","4,000 to 40,000 GPU hours"],"technology_tags":["Vision-Language Model","Medical AI"],"sdg_alignment":[3],"analyzed_at":"2025-10-29T09:24:31.553892Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_b0e125a696f1","title":"FreqCa: Accelerating Diffusion Models via Frequency","content":"arXiv:2510.08669v1 Announce Type: new Abstract: The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timesteps. However, previous feature caching assumes that features in adjacent timesteps are similar or continuous, which does not always hold in all settings. To investigate this, this paper begins with an analysis from the frequency domain, which reveal that different frequency bands in the features of diffusion models exhibit different dynamics across timesteps. Concretely, low-frequency components, which decide the structure of images, exhibit higher similarity but poor continuity. In contrast, the high-frequency bands, which decode the details of images, show significant continuity but poor similarity. These interesting observations motivate us to propose Frequency-aware Caching (FreqCa) which directly reuses features of low-frequency components based on their similarity, while using a second-order Hermite interpolator to predict the volatile high-frequency ones based on its continuity. Besides, we further propose to cache Cumulative Residual Feature (CRF) instead of the features in all the layers, which reduces the memory footprint of feature caching by 99%. Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit demonstrate its effectiveness in both generation and editing. Codes are available in the supplementary materials and will be released on GitHub.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08669","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.572389","language":"en","tags":["research","csai","preprints","cslg","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":217,"author":"Jiacheng Liu, Peiliang Cai, Qinming Zhou, Yuqi Lin, Deyang Kong, Benhao Huang, Yupei Pan, Haowen Xu, Chang Zou, Junshu Tang, Shikang Zheng, Linfeng Zhang","raw_content_length":1614,"priority":7,"update_frequency":1,"reading_time_minutes":1.085,"robust_parsing_used":true,"entities":{"organizations":["Frequency arXiv:2510.08669v1"],"persons":[],"locations":[],"monetary":[]},"char_count":1607,"language_detected":"en","key_concepts":{"key_phrases":["FreqCa","Diffusion Models","Frequency","arXiv251008669v1 Announce Type","new Abstract","The application","diffusion transformers","their significant inference costs","this problem","features"],"filter_categories":{"ai_ml":["diffusion transformers"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"FreqCa":2.0,"Diffusion Models":2.0,"Frequency":2.0,"arXiv251008669v1 Announce Type":1.0,"new Abstract":1.0,"The application":1.0,"diffusion transformers":1.0,"their significant inference costs":1.0,"this problem":1.0,"features":1.0}},"age_hours":2.760855290555556,"is_recent":true,"quality_score":1.0,"sentiment_score":2.195,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.561,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.764,"joy":0.006,"surprise":0.0555,"sadness":0.1073,"fear":0.0171,"anger":0.0222,"disgust":0.028},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The paper presents a novel approach to accelerating diffusion models, potentially reducing the computational resources and energy needed for image generation. The concrete action is the development of the FreqCa algorithm and its evaluation on several datasets. The claim of a 99% reduction in memory footprint is a significant metric, but deployment readiness is low as it's still in the research phase.","key_impact_metrics":["99% reduction in memory footprint"],"technology_tags":["diffusion models","feature caching","image generation","machine learning"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T09:24:34.610935Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_35c9d6da8900","title":"Optimizing delivery for quick commerce factoring qualitative assessment of generated routes","content":"arXiv:2510.08671v1 Announce Type: new Abstract: Indias e-commerce market is projected to grow rapidly, with last-mile delivery accounting for nearly half of operational expenses. Although vehicle routing problem (VRP) based solvers are widely used for delivery planning, their effectiveness in real-world scenarios is limited due to unstructured addresses, incomplete maps, and computational constraints in distance estimation. This study proposes a framework that employs large language models (LLMs) to critique VRP-generated routes against policy-based criteria, allowing logistics operators to evaluate and prioritise more efficient delivery plans. As a illustration of our approach we generate, annotate and evaluated 400 cases using large language models. Our study found that open-source LLMs identified routing issues with 79% accuracy, while proprietary reasoning models achieved reach upto 86%. The results demonstrate that LLM-based evaluation of VRP-generated routes can be an effective and scalable layer of evaluation which goes beyond beyond conventional distance and time based metrics. This has implications for improving cost efficiency, delivery reliability, and sustainability in last-mile logistics, especially for developing countries like India.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08671","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.572791","language":"en","tags":["research","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":169,"author":"Milon Bhattacharya, Milan Kumar","raw_content_length":1269,"priority":7,"update_frequency":1,"reading_time_minutes":0.845,"robust_parsing_used":true,"entities":{"organizations":["VRP"],"persons":[],"locations":[],"monetary":[]},"char_count":1268,"language_detected":"en","key_concepts":{"key_phrases":["delivery","quick commerce factoring","qualitative assessment","generated routes","arXiv251008671v1 Announce Type","new Abstract","Indias e-commerce market","last-mile delivery accounting","nearly half","operational expenses"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"delivery":2.0,"quick commerce factoring":2.0,"qualitative assessment":2.0,"generated routes":2.0,"arXiv251008671v1 Announce Type":1.0,"new Abstract":1.0,"Indias e-commerce market":1.0,"last-mile delivery accounting":1.0,"nearly half":1.0,"operational expenses":1.0}},"age_hours":2.7608715369444443,"is_recent":true,"quality_score":1.0,"sentiment_score":4.2345,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8559,"joy":0.0112,"surprise":0.0906,"sadness":0.0084,"fear":0.0144,"anger":0.0153,"disgust":0.0043},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":6,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This article presents a framework using LLMs to improve VRP-generated delivery routes, potentially leading to more efficient logistics and reduced operational expenses. The study evaluated 400 cases and achieved 79-86% accuracy in identifying routing issues. While promising, it's still in the applied research phase with no evidence of real-world deployment or third-party verification.","key_impact_metrics":["79% accuracy","86% accuracy"],"technology_tags":["Large Language Models","Vehicle Routing Problem","Last-mile Delivery"],"sdg_alignment":[9,11,12],"analyzed_at":"2025-10-29T09:24:37.581735Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_66bbba5985a9","title":"Thinking with Camera: A Unified Multimodal Model for Camera","content":"arXiv:2510.08673v1 Announce Type: new Abstract: Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08673","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.573190","language":"en","tags":["preprints","computer-science","cscv","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":170,"author":"Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy","raw_content_length":1335,"priority":7,"update_frequency":1,"reading_time_minutes":0.85,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1334,"language_detected":"en","key_concepts":{"key_phrases":["Camera","Puffin","Announce Type","new Abstract","Camera-centric understanding","generation","two cornerstones","spatial intelligence","isolation","a unified camera-centric multimodal model"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Camera":4.0,"Puffin":2.0,"Announce Type":1.0,"new Abstract":1.0,"Camera-centric understanding":1.0,"generation":1.0,"two cornerstones":1.0,"spatial intelligence":1.0,"isolation":1.0,"a unified camera-centric multimodal model":1.0}},"age_hours":2.7608868805555553,"is_recent":true,"quality_score":0.7,"sentiment_score":9.01,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.802,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8116,"joy":0.0225,"surprise":0.1298,"sadness":0.006,"fear":0.01,"anger":0.015,"disgust":0.005},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel multimodal model (Puffin) for camera-centric understanding and generation, trained on a large dataset (Puffin-4M). While the model shows superior performance in experiments, it is still in the research phase with no deployed units or real-world applications. The potential climate impact is indirect, relying on future applications that could potentially improve resource management or environmental monitoring, but this is not yet demonstrated.","key_impact_metrics":["Puffin-4M dataset size: 4 million triplets"],"technology_tags":["multimodal AI","computer vision","spatial intelligence"],"sdg_alignment":[9,11,13],"analyzed_at":"2025-10-29T09:24:41.045560Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_6482bb492908","title":"Don't Waste Mistakes: Leveraging Negative RL","content":"arXiv:2510.08696v1 Announce Type: new Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as \\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples (\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08696","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.573612","language":"en","tags":["computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":186,"author":"Yunzhen Feng, Parag Jain, Anthony Hartshorn, Yaqi Duan, Julia Kempe","raw_content_length":1394,"priority":7,"update_frequency":1,"reading_time_minutes":0.93,"robust_parsing_used":true,"entities":{"organizations":["MLE","GRPO","Group Relative Policy Optimization"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1393,"language_detected":"en","key_concepts":{"key_phrases":["Dont Waste Mistakes","Negative RL","GRPO","negative groups","arXiv251008696v1 Announce Type","new Abstract","verifiable rewards","RLVR","a standard recipe","large language models"],"filter_categories":{"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Dont Waste Mistakes":2.0,"Negative RL":2.0,"GRPO":2.0,"negative groups":2.0,"arXiv251008696v1 Announce Type":1.0,"new Abstract":1.0,"verifiable rewards":1.0,"RLVR":1.0,"a standard recipe":1.0,"large language models":1.0}},"age_hours":2.7609034350000003,"is_recent":true,"quality_score":1.0,"sentiment_score":4.0395,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":-0.1921,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9134,"joy":0.0031,"surprise":0.013,"sadness":0.0111,"fear":0.0036,"anger":0.0328,"disgust":0.0231},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel reinforcement learning algorithm (LENS) that improves the efficiency of training large language models. The concrete action is the modification of the GRPO algorithm to leverage previously wasted samples, leading to performance gains on the MATH benchmark. The evidence is based on experimental results with Llama-3.1-8B and Qwen-2.5-3B, but it's still in the research phase.","key_impact_metrics":["Significant gains on harder items in MATH benchmark","Improved efficiency in RLVR"],"technology_tags":["Reinforcement Learning","Large Language Models","Algorithm Optimization"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T09:24:44.078485Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c6c52a72f3be","title":"BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution","content":"arXiv:2510.08697v1 Announce Type: new Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08697","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.574051","language":"en","tags":["research","csse","csai","preprints","cscl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":255,"author":"Terry Yue Zhuo, Xiaolong Jin, Hange Liu, Juyong Jiang, Tianyang Liu, Chen Gong, Bhupesh Bishnoi, Vaisakhi Mishra, Marek Suppa, Noah Ziems, Saiteja Utpala, Ming Xu, Guangyu Song, Kaixin Li, Yuhan Cao, Bo Liu, Zheng Liu, Sabina Abdurakhmanova, Wenhao Yu, Mengzhao Jia, Jihan Yao, Kenneth Hamilton, Kumar Shridhar, Minh Chien Vu, Dingmin Wang, Jiawei Liu, Zijian Wang, Qian Liu, Binyuan Hui, Meg Risdal, Ahsen Khaliq, Atin Sood, Zhenchang Xing, Wasi Uddin Ahmad, John Grundy, David Lo, Banghua Zhu, Xiaoning Du, Torsten Scholak, Leandro von Werra","raw_content_length":1887,"priority":7,"update_frequency":1,"reading_time_minutes":1.275,"robust_parsing_used":true,"entities":{"organizations":["LLM"],"persons":["Chatbot Arena"],"locations":[],"monetary":[]},"char_count":1886,"language_detected":"en","key_concepts":{"key_phrases":["BigCodeArena","More Reliable Human Preferences","Code Generation","Execution","the quality","arXiv251008697v1 Announce Type","new Abstract","Crowdsourced model evaluation platforms","Chatbot Arena","real-time evaluation"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"BigCodeArena":3.0,"More Reliable Human Preferences":2.0,"Code Generation":2.0,"Execution":2.0,"the quality":2.0,"arXiv251008697v1 Announce Type":1.0,"new Abstract":1.0,"Crowdsourced model evaluation platforms":1.0,"Chatbot Arena":1.0,"real-time evaluation":1.0}},"age_hours":2.760918923611111,"is_recent":true,"quality_score":1.0,"sentiment_score":6.1235,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2247,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.868,"joy":0.0399,"surprise":0.0521,"sadness":0.0055,"fear":0.0124,"anger":0.018,"disgust":0.0041},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the reliability of human preferences in code generation using LLMs, which could indirectly contribute to sustainability by improving the efficiency of software development for climate-related technologies. The article presents a new platform, BigCodeArena, and benchmarks based on collected data, but there is no direct deployment of technology or measured outcomes related to environmental impact. The vaporware flag is set because it's a platform and benchmark, not a deployed technology with operational data.","key_impact_metrics":["14,000 raw code-centric conversation sessions","4,700 multi-turn samples with pairwise human preferences"],"technology_tags":["LLMs","Code Generation","Human Evaluation"],"sdg_alignment":[4,9],"analyzed_at":"2025-10-29T09:24:47.247608Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_1aa8d5175ddc","title":"Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System","content":"arXiv:2510.08700v1 Announce Type: new Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic voting systems, yet achieving strong secrecy guarantees in decentralized, large-scale elections remains challenging. This paper proposes the concept of collectively secure voting, in which voters themselves can opt in as secret holders to protect ballot secrecy. A practical blockchain-based collectively secure voting system is designed and implemented. Our design strikes a balance between strong confidentiality guarantees and real-world applicability. The proposed system combines threshold cryptography and smart contracts to ensure ballots remain confidential during voting, while all protocol steps remain transparent and verifiable. Voters can use the system without prior blockchain knowledge through an intuitive user interface that hides underlying complexity. To evaluate this approach, a user testing is conducted. Results show a high willingness to act as secret holders, reliable participation in share release, and high security confidence in the proposed system. The findings demonstrate that voters can collectively maintain secrecy and that such a practical deployment is feasible.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08700","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.574473","language":"en","tags":["cscr","csdc","research","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":165,"author":"Zhuolun Li, Haluk Sonmezler, Faiza Shirazi, Febin Shaji, Tymoteusz Mroczkowski, Dexter Lardner, Matthew Alain Camus, Evangelos Pournaras","raw_content_length":1221,"priority":7,"update_frequency":1,"reading_time_minutes":0.825,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":[],"monetary":[]},"char_count":1220,"language_detected":"en","key_concepts":{"key_phrases":["Voters","Elections","a Practical Blockchain Voting System","arXiv251008700v1 Announce Type","new Abstract","Ensuring ballot secrecy","fair and trustworthy electronic voting systems","strong secrecy guarantees","large-scale elections","This paper"],"filter_categories":{"ai_ml":["a Practical Blockchain Voting System"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Voters":2.0,"Elections":2.0,"a Practical Blockchain Voting System":2.0,"arXiv251008700v1 Announce Type":1.0,"new Abstract":1.0,"Ensuring ballot secrecy":1.0,"fair and trustworthy electronic voting systems":1.0,"strong secrecy guarantees":1.0,"large-scale elections":1.0,"This paper":1.0}},"age_hours":2.7609343699999997,"is_recent":true,"quality_score":0.7,"sentiment_score":9.716000000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9432,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9228,"joy":0.0165,"surprise":0.0238,"sadness":0.0033,"fear":0.0167,"anger":0.0106,"disgust":0.0063},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":6,"economic_viability":2,"deployment_readiness":3,"systemic_impact":2,"justice_equity":5,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper proposes a blockchain-based voting system with a user testing conducted. While it has potential for improving trust in elections, it has minimal direct climate impact and is in early stages of development, lacking real-world deployment or economic viability data. The user testing shows 'high willingness' but lacks concrete data on long-term adoption or scalability.","key_impact_metrics":["high willingness to act as secret holders","high security confidence"],"technology_tags":["blockchain","threshold cryptography","smart contracts"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T09:24:50.334514Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_eb1257591d13","title":"Scaling Laws for Code: A More Data","content":"arXiv:2510.08702v1 Announce Type: new Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering. However, scaling laws that guide the efficient training are predominantly analyzed on Natural Language (NL). Given the fundamental differences like strict syntax between code and NL, it is unclear whether these laws are directly applicable to code. To address this gap, we conduct the first large-scale empirical study of scaling laws for code, comprising 117 experimental runs with model sizes from 0.2B to 3.8B and training tokens from 2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results show that the more expressive Farseer law offers greater accuracy. Second, the analysis reveals that Code LLMs scale effectively with model size. Crucially, code represents a more data-hungry regime, requiring a substantially higher data-to-parameter ratio than NL. Finally, two additional sets of experiments on code-NL mixtures show that NL benefits resource-constrained scenarios, but becomes a detriment at higher compute budgets.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08702","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.574860","language":"en","tags":["research","computer-science","preprints","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":157,"author":"Xianzhen Luo, Wenzhen Zheng, Qingfu Zhu, Rongyi Zhang, Houyi Li, Siming Huang, YuanTao Fan, Wanxiang Che","raw_content_length":1075,"priority":7,"update_frequency":1,"reading_time_minutes":0.785,"robust_parsing_used":true,"entities":{"organizations":["Natural Language"],"persons":["Chinchilla","Farsser"],"locations":[],"monetary":[]},"char_count":1074,"language_detected":"en","key_concepts":{"key_phrases":["Laws","Code","laws","code","arXiv251008702v1 Announce Type","new Abstract","Code Large Language Models","LLMs","software engineering","the efficient training"],"filter_categories":{"ai_ml":["Code Large Language Models"],"engineering":["software engineering"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Laws":2.0,"Code":2.0,"laws":2.0,"code":2.0,"arXiv251008702v1 Announce Type":1.0,"new Abstract":1.0,"Code Large Language Models":1.0,"LLMs":1.0,"software engineering":1.0,"the efficient training":1.0}},"age_hours":2.760950783888889,"is_recent":true,"quality_score":1.0,"sentiment_score":8.5015,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7003,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8958,"joy":0.0099,"surprise":0.0558,"sadness":0.006,"fear":0.0108,"anger":0.0151,"disgust":0.0065},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores scaling laws for code LLMs, finding that code is more data-hungry than natural language. While this could lead to more efficient training and resource allocation for AI development, its direct climate impact is currently theoretical and related to energy consumption of training. The study is based on experimental runs with specific model sizes and training tokens, providing some metrics, but lacks deployment or real-world application data.","key_impact_metrics":["model sizes from 0.2B to 3.8B","training tokens from 2B to 128B"],"technology_tags":["Large Language Models","Code LLMs"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:24:53.492406Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_9825cdad07a7","title":"ConPoSe: LLM","content":"arXiv:2510.08705v1 Announce Type: new Abstract: Object transportation in cluttered environments is a fundamental task in various domains, including domestic service and warehouse logistics. In cooperative object transport, multiple robots must coordinate to move objects that are too large for a single robot. One transport strategy is pushing, which only requires simple robots. However, careful selection of robot-object contact points is necessary to push the object along a preplanned path. Although this selection can be solved analytically, the solution space grows combinatorially with the number of robots and object size, limiting scalability. Inspired by how humans rely on common-sense reasoning for cooperative transport, we propose combining the reasoning capabilities of Large Language Models with local search to select suitable contact points. Our LLM-guided local search method for contact point selection, ConPoSe, successfully selects contact points for a variety of shapes, including cuboids, cylinders, and T-shapes. We demonstrate that ConPoSe scales better with the number of robots and object size than the analytical approach, and also outperforms pure LLM-based selection.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08705","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.575250","language":"en","tags":["research","csai","preprints","csro","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":168,"author":"Noah Steinkr\\\"uger, Nisarga Nilavadi, Wolfram Burgard, Tanja Katharina Kaiser","raw_content_length":1199,"priority":7,"update_frequency":1,"reading_time_minutes":0.84,"robust_parsing_used":true,"entities":{"organizations":["Large Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1198,"language_detected":"en","key_concepts":{"key_phrases":["ConPoSe","LLM","arXiv251008705v1 Announce Type","new Abstract","Object transportation","cluttered environments","a fundamental task","various domains","domestic service","warehouse logistics"],"filter_categories":{"ai_ml":["LLM"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"ConPoSe":2.0,"LLM":2.0,"arXiv251008705v1 Announce Type":1.0,"new Abstract":1.0,"Object transportation":1.0,"cluttered environments":1.0,"a fundamental task":1.0,"various domains":1.0,"domestic service":1.0,"warehouse logistics":1.0}},"age_hours":2.7609664194444443,"is_recent":true,"quality_score":1.0,"sentiment_score":5.7655,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1531,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9337,"joy":0.0057,"surprise":0.0228,"sadness":0.0056,"fear":0.0126,"anger":0.0113,"disgust":0.0082},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":5,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a novel approach to robot coordination using LLMs, demonstrating improved scalability compared to analytical methods. While the research shows promise in optimizing object transport, it's currently in the early stages of development with no deployed units or real-world impact data. The potential climate impact is indirect, related to optimizing logistics, but not directly reducing emissions.","key_impact_metrics":["Scalability with number of robots","Scalability with object size"],"technology_tags":["Large Language Models","Robotics","Logistics Optimization"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T09:24:56.889700Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_bda92d0b8a5b","title":"Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning","content":"arXiv:2510.08710v1 Announce Type: new Abstract: Case-based reasoning is a cornerstone of U.S. legal practice, requiring professionals to argue about a current case by drawing analogies to and distinguishing from past precedents. While Large Language Models (LLMs) have shown remarkable capabilities, their proficiency in this complex, nuanced form of reasoning needs further investigation. We propose a formal framework that decomposes the process of identifying significant distinctions between cases into three-stage reasoning tasks. Our framework models cases using factual predicates called factors, organizes them into a legal knowledge hierarchy, and defines verifiable rules for identifying distinctions, analyzing their argumentative support, and evaluating their significance. Through comprehensive evaluation of modern reasoning LLMs, we reveal a paradox: while models achieve high accuracy on surface-level reasoning (Task 1), performance degrades on hierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated analysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models consistently expend more computational resources on incorrect responses than correct ones, suggesting that \"thinking longer\" does not always mean \"thinking smarter.\" Our work provides a methodology for fine-grained analysis of LLM reasoning capabilities in complex domains and reveals fundamental limitations that must be addressed for robust and trustworthy legal AI.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08710","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.575676","language":"en","tags":["research","computer-science","preprints","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Li Zhang, Matthias Grabmair, Morgan Gray, Kevin Ashley","raw_content_length":1477,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Thinking Longer"],"locations":["U.S."],"monetary":[]},"char_count":1476,"language_detected":"en","key_concepts":{"key_phrases":["Hierarchical Legal Reasoning","arXiv251008710v1 Announce Type","new Abstract","Case-based reasoning","a cornerstone","US legal practice","professionals","a current case","analogies","past precedents"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hierarchical Legal Reasoning":2.0,"arXiv251008710v1 Announce Type":1.0,"new Abstract":1.0,"Case-based reasoning":1.0,"a cornerstone":1.0,"US legal practice":1.0,"professionals":1.0,"a current case":1.0,"analogies":1.0,"past precedents":1.0}},"age_hours":2.7609817772222223,"is_recent":true,"quality_score":1.0,"sentiment_score":5.914000000000001,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1828,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8971,"joy":0.0086,"surprise":0.045,"sadness":0.0031,"fear":0.0238,"anger":0.0161,"disgust":0.0063},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on evaluating LLM capabilities in legal reasoning, specifically identifying distinctions between cases. While the research provides a framework and reveals limitations in LLM performance (Task 3: 11.46%-33.99%), it is still in the basic research stage with no deployed technology or measurable environmental outcomes. The vaporware flag is set because it's about early-stage concepts without deployed units.","key_impact_metrics":["Task 2 accuracy: 64.82%-92.09%","Task 3 accuracy: 11.46%-33.99%"],"technology_tags":["Large Language Models","Case-based reasoning","Artificial Intelligence"],"sdg_alignment":[16],"analyzed_at":"2025-10-29T09:25:00.203872Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f9ef14230971","title":"Unified World Models: Memory","content":"arXiv:2510.08713v1 Announce Type: new Abstract: Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08713","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.576530","language":"en","tags":["research","csai","preprints","csro","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Yifei Dong, Fengyi Wu, Guangyu Chen, Zhi-Qi Cheng, Qiyu Hu, Yuxuan Zhou, Jingdong Sun, Jun-Yan He, Qi Dai, Alexander G Hauptmann","raw_content_length":1382,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":["UniWM","Unified World Models: Memory arXiv:2510.08713v1 Announce Type"],"persons":[],"locations":["UniWM"],"monetary":[]},"char_count":1381,"language_detected":"en","key_concepts":{"key_phrases":["Unified World Models","Memory","arXiv251008713v1 Announce Type","new Abstract","Enabling embodied agents","future states","robust and generalizable visual navigation","the-art","modular architectures","visual world modeling"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Unified World Models":2.0,"Memory":2.0,"arXiv251008713v1 Announce Type":1.0,"new Abstract":1.0,"Enabling embodied agents":1.0,"future states":1.0,"robust and generalizable visual navigation":1.0,"the-art":1.0,"modular architectures":1.0,"visual world modeling":1.0}},"age_hours":2.7610121566666668,"is_recent":true,"quality_score":1.0,"sentiment_score":9.329,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8658,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9304,"joy":0.0077,"surprise":0.0351,"sadness":0.0045,"fear":0.0091,"anger":0.0084,"disgust":0.0047},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new AI model (UniWM) that improves navigation success rates in embodied agents. While this could potentially lead to more efficient resource use in robotics and automation, the direct climate impact is currently theoretical. The model's performance is demonstrated across several benchmarks, providing some evidence of its effectiveness, but it is still in the applied research stage with no real-world deployments.","key_impact_metrics":["navigation success rates improved by up to 30%","significantly reduces trajectory errors"],"technology_tags":["AI","embodied agents","visual navigation","world model"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:03.233352Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_f823affba7ca","title":"Search","content":"arXiv:2508.15327v3 Announce Type: replace Abstract: Offline reinforcement learning refers to the process of learning policies from fixed datasets, without requiring additional environment interaction. However, it often relies on well-defined reward functions, which are difficult and expensive to design. Human feedback is an appealing alternative, but its two common forms, expert demonstrations and preferences, have complementary limitations. Demonstrations provide stepwise supervision, but they are costly to collect and often reflect limited expert behavior modes. In contrast, preferences are easier to collect, but it is unclear which parts of a behavior contribute most to a trajectory segment, leaving credit assignment unresolved. In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to unify these two feedback sources. For each transition in a preference labeled trajectory, SPW searches for the most similar state-action pairs from expert demonstrations and directly derives stepwise importance weights based on their similarity scores. These weights are then used to guide standard preference learning, enabling more accurate credit assignment that traditional approaches struggle to achieve. We demonstrate that SPW enables effective joint learning from preferences and demonstrations, outperforming prior methods that leverage both feedback types on challenging robot manipulation tasks.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2508.15327","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.907757","language":"en","tags":["research","csai","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":192,"author":"Xiancheng Gao, Yufeng Shi, Wengang Zhou, Houqiang Li","raw_content_length":1429,"priority":7,"update_frequency":1,"reading_time_minutes":0.96,"robust_parsing_used":true,"entities":{"organizations":["Search-Based Preference Weighting","SPW"],"persons":[],"locations":[],"monetary":[]},"char_count":1428,"language_detected":"en","key_concepts":{"key_phrases":["Search","arXiv250815327v3 Announce Type","Abstract","Offline reinforcement learning","the process","policies","fixed datasets","additional environment interaction","well-defined reward functions","which"],"filter_categories":{"healthcare_tech":["Search"],"research_academic":["Search"],"ai_ml":["Offline reinforcement learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Search":2.0,"arXiv250815327v3 Announce Type":1.0,"Abstract":1.0,"Offline reinforcement learning":1.0,"the process":1.0,"policies":1.0,"fixed datasets":1.0,"additional environment interaction":1.0,"well-defined reward functions":1.0,"which":1.0}},"age_hours":2.772877946666667,"is_recent":true,"quality_score":1.0,"sentiment_score":5.45,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.09,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8318,"joy":0.0038,"surprise":0.0084,"sadness":0.0233,"fear":0.0341,"anger":0.0293,"disgust":0.0694},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper introduces a novel search-based preference weighting scheme for offline reinforcement learning, aiming to improve robot manipulation tasks by unifying expert demonstrations and preferences. While the research shows promise in improving learning algorithms, it is currently in the applied research stage with no evidence of deployment or quantified environmental impact. The lack of concrete deployment data and economic analysis limits its current sustainability score.","key_impact_metrics":[],"technology_tags":["reinforcement learning","robot manipulation"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:06.206795Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c623f2de4775","title":"How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary","content":"arXiv:2510.08720v1 Announce Type: new Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs) is a critical yet challenging task. Existing benchmarks suffer from high computational costs, score inflation, and a bias towards trivial bugs over rare, critical faults. In this work, we ask two fundamental questions: (1) What is the minimal set of wrong codes sufficient to represent the entire error space? and (2) What is the minimal set of test cases needed to distinguish them? We introduce a framework that formalizes benchmark construction as finding an optimal diagnostic basis in a binary code-test matrix. The rank of this matrix specifies the minimal number of independent error patterns (wrong codes) and provides a tight upper bound on the number of test cases required for complete fault coverage. Our objective is to identify a basis of size equal to the matrix rank that maximizes internal diversity. To tackle this NP-hard problem, we propose WrongSelect, an efficient approximation algorithm to select maximally diverse wrong codes. Applying this framework to millions of competitive programming submissions, we construct TC-Bench, a compact, diverse, and inflation-resistant benchmark. Extensive experiments show that even the most advanced test case generation methods achieve only ~60% exclusion rates on TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is at: https://github.com/Luowaterbi/TC-Bench.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08720","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.577337","language":"en","tags":["research","computer-science","preprints","cscl","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":222,"author":"Xianzhen Luo, Jinyang Huang, Wenzhen Zheng, Qingfu Zhu, Mingzheng Xu, Yiheng Xu, Yuantao Fan, Libo Qin, Wanxiang Che","raw_content_length":1556,"priority":7,"update_frequency":1,"reading_time_minutes":1.11,"robust_parsing_used":true,"entities":{"organizations":["Large Language Models"],"persons":[],"locations":[],"monetary":[]},"char_count":1555,"language_detected":"en","key_concepts":{"key_phrases":["How Many Code and Test Cases","Test Cases Generation","a Binary","What","the minimal set","arXiv251008720v1 Announce Type","new Abstract","Evaluating test cases","Large Language Models","LLMs"],"filter_categories":{"ai_ml":["Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"How Many Code and Test Cases":2.0,"Test Cases Generation":2.0,"a Binary":2.0,"What":2.0,"the minimal set":2.0,"arXiv251008720v1 Announce Type":1.0,"new Abstract":1.0,"Evaluating test cases":1.0,"Large Language Models":1.0,"LLMs":1.0}},"age_hours":2.761043631388889,"is_recent":true,"quality_score":1.0,"sentiment_score":0.3915000000000002,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.9217,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.5607,"joy":0.0029,"surprise":0.014,"sadness":0.0281,"fear":0.2074,"anger":0.0734,"disgust":0.1135},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research focuses on improving the efficiency of test case generation for software, which could indirectly reduce energy consumption by optimizing code and reducing computational waste. The framework is tested on millions of competitive programming submissions, providing a strong basis for its claims. However, it's still in the research phase with no immediate deployment.","key_impact_metrics":["exclusion rates on TC-Bench ~60%"],"technology_tags":["LLMs","test case generation","software optimization"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:09.290978Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_7fdc9f58e6e4","title":"Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs","content":"arXiv:2510.08726v1 Announce Type: new Abstract: Operator fusion has become a key optimization for deep learning, which combines multiple deep learning operators to improve data reuse and reduce global memory transfers. However, existing tensor compilers struggle to fuse complex reduction computations involving loop-carried dependencies, such as attention mechanisms. The paper introduces Neptune, a tensor compiler for advanced operator fusion for sequences of reduction operators. Neptune presents a new approach for advanced operator fusion, which intentionally breaks some existing dependencies and compensates by constructing algebraic correction expressions that allow the kernel to produce the correct result. On ten attention-based benchmarks, Neptune, starting from simple attention code and a high-level scheduling template, outperforms existing compilers like Triton, TVM, and FlexAttention, including Triton-based implementations of FlashAttention. Across four different GPU architectures from NVIDIA and AMD, Neptune-generated kernels have average speedup of $1.35\\times$ over the next best alternative, demonstrating its effectiveness for deep learning workloads.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08726","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.579499","language":"en","tags":["research","preprints","cslg","cspl","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":150,"author":"Yifan Zhao, Egan Johnson, Prasanth Chatarasi, Vikram Adve, Sasa Misailovic","raw_content_length":1183,"priority":7,"update_frequency":1,"reading_time_minutes":0.75,"robust_parsing_used":true,"entities":{"organizations":["Neptune","FlexAttention"],"persons":[],"locations":["Triton"],"monetary":[]},"char_count":1178,"language_detected":"en","key_concepts":{"key_phrases":["Neptune","Advanced ML Operator Fusion","Locality","Parallelism","GPUs","arXiv251008726v1 Announce Type","new Abstract","Operator fusion","a key optimization","deep learning"],"filter_categories":{"ai_ml":["Advanced ML Operator Fusion","deep learning"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Neptune":4.0,"Advanced ML Operator Fusion":2.0,"Locality":2.0,"Parallelism":2.0,"GPUs":2.0,"arXiv251008726v1 Announce Type":1.0,"new Abstract":1.0,"Operator fusion":1.0,"a key optimization":1.0,"deep learning":1.0}},"age_hours":2.761114485833333,"is_recent":true,"quality_score":1.0,"sentiment_score":8.6755,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7351,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9082,"joy":0.0144,"surprise":0.0355,"sadness":0.0128,"fear":0.0073,"anger":0.0148,"disgust":0.0071},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":4,"technical_credibility":7,"economic_viability":4,"deployment_readiness":3,"systemic_impact":5,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"Neptune is a tensor compiler that improves the efficiency of deep learning models, leading to faster computation and potentially lower energy consumption for training and inference. The article presents performance metrics ($1.35\times$ speedup) on attention-based benchmarks across different GPU architectures, but it is still in the applied research stage with no deployment data. The vaporware flag is set because it's a prototype with no deployed units or customer contracts.","key_impact_metrics":["$1.35\\times$ speedup","Ten attention-based benchmarks"],"technology_tags":["Tensor Compiler","Operator Fusion","Deep Learning","GPU"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:12.843519Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_fb1c207aed2d","title":"How Reliable is Language Model Micro","content":"arXiv:2510.08730v1 Announce Type: new Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost of language model development: evaluate on a very small subset of existing benchmarks. Can these micro-benchmarks, however, rank models as consistently as the full benchmarks they replace? And can they rank models more consistently than selecting a random subset of data points? In many scenarios, we find that the answer is no. We introduce a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. This approach can determine which model pairs can be ranked correctly by a micro-benchmark, allowing for a finer-grained analysis of the trade-off between micro-benchmark size and reliability. Prior work has suggested selecting as few as 10 examples; we find that no micro-benchmarking method can consistently rank model pairs 3.5 points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In order to consistently rank model pairs with relatively similar performances, we show that often as many as 250 examples must be selected, at which point random sampling is competitive with existing micro-benchmarking methods. When comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25 examples, we find that more than half of pairwise comparisons are not likely to be preserved. Our work provides actionable guidance for both micro-benchmark users and developers in navigating the trade-off between evaluation efficiency and reliability.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08730","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.580344","language":"en","tags":["research","preprints","cscl","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":239,"author":"Gregory Yauney, Shahzaib Saqib Warraich, Swabha Swayamdipta","raw_content_length":1622,"priority":7,"update_frequency":1,"reading_time_minutes":1.195,"robust_parsing_used":true,"entities":{"organizations":["Language Model Micro"],"persons":[],"locations":[],"monetary":[]},"char_count":1621,"language_detected":"en","key_concepts":{"key_phrases":["Reliable","Language Model Micro","arXiv251008730v1","Announce Type","new Abstract","Micro","benchmarking","a solution","the often prohibitive time","cost"],"filter_categories":{"engineering":["Micro"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Reliable":2.0,"Language Model Micro":2.0,"arXiv251008730v1":1.0,"Announce Type":1.0,"new Abstract":1.0,"Micro":1.0,"benchmarking":1.0,"a solution":1.0,"the often prohibitive time":1.0,"cost":1.0}},"age_hours":2.761145873333333,"is_recent":true,"quality_score":1.0,"sentiment_score":6.9695,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3939,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8986,"joy":0.0079,"surprise":0.0688,"sadness":0.0036,"fear":0.0045,"anger":0.0111,"disgust":0.0055},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper analyzes the reliability of micro-benchmarking for language models, finding that small subsets of data may not accurately reflect full benchmark performance. The research identifies the number of examples needed for reliable ranking and provides guidance for micro-benchmark users. While it doesn't directly reduce emissions, it could improve the efficiency of AI development, potentially reducing energy consumption in the long run.","key_impact_metrics":["3.5 points of accuracy on MMLU-Pro","4 points apart on BIG-bench Hard"],"technology_tags":["Language Models","Micro-benchmarking","AI Efficiency"],"sdg_alignment":[9,12],"analyzed_at":"2025-10-29T09:25:15.819893Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_e65e1abf8e9f","title":"Transmuting prompts into weights","content":"arXiv:2510.08734v1 Announce Type: new Abstract: A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt's influence can be mathematically mapped to implicit weight updates (Dherin et al., 2025), we generalize this theory to deep, multi-block transformers. We show how the information contained in any chunk of a user prompt is represented and composed internally through weight vectors and weight matrices. We then derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector- and matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08734","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.581143","language":"en","tags":["computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":187,"author":"Hanna Mazzawi, Benoit Dherin, Michael Munn, Michael Wunder, Javier Gonzalvo","raw_content_length":1354,"priority":7,"update_frequency":1,"reading_time_minutes":0.935,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["wei"],"locations":[],"monetary":[]},"char_count":1353,"language_detected":"en","key_concepts":{"key_phrases":["Transmuting prompts","weights","arXiv251008734v1 Announce Type","new Abstract","A growing body","research","the behavior","large language models","inference time","their internal states"],"filter_categories":{"healthcare_tech":["research"],"research_academic":["research"],"ai_ml":["large language models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Transmuting prompts":2.0,"weights":2.0,"arXiv251008734v1 Announce Type":1.0,"new Abstract":1.0,"A growing body":1.0,"research":1.0,"the behavior":1.0,"large language models":1.0,"inference time":1.0,"their internal states":1.0}},"age_hours":2.7611749486111115,"is_recent":true,"quality_score":1.0,"sentiment_score":8.753,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7506,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9109,"joy":0.0199,"surprise":0.0412,"sadness":0.0027,"fear":0.0048,"anger":0.0114,"disgust":0.0091},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a theoretical framework for improving the efficiency of large language models. While potentially impactful in reducing the energy consumption of AI, it is currently in the basic research stage with no deployed technology or measured outcomes. The vaporware flag is raised because it describes a novel concept without evidence of deployment.","key_impact_metrics":[],"technology_tags":["large language models","transformer architecture","model editing"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:18.960584Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_c1d3deb20224","title":"Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis","content":"arXiv:2510.08739v1 Announce Type: new Abstract: Modern time series forecasting increasingly relies on complex ensemble models generated by AutoML systems like AutoGluon, delivering superior accuracy but with significant costs to transparency and interpretability. This paper introduces a comprehensive, dual-approach framework that addresses both the explainability and forecastability challenges in complex time series ensembles. First, we develop a surrogate-based explanation methodology that bridges the accuracy-interpretability gap by training a LightGBM model to faithfully mimic AutoGluon's time series forecasts, enabling stable SHAP-based feature attributions. We rigorously validated this approach through feature injection experiments, demonstrating remarkably high faithfulness between extracted SHAP values and known ground truth effects. Second, we integrated spectral predictability analysis to quantify each series' inherent forecastability. By comparing each time series' spectral predictability to its pure noise benchmarks, we established an objective mechanism to gauge confidence in forecasts and their explanations. Our empirical evaluation on the M5 dataset found that higher spectral predictability strongly correlates not only with improved forecast accuracy but also with higher fidelity between the surrogate and the original forecasting model. These forecastability metrics serve as effective filtering mechanisms and confidence scores, enabling users to calibrate their trust in both the forecasts and their explanations. We further demonstrated that per-item normalization is essential for generating meaningful SHAP explanations across heterogeneous time series with varying scales. The resulting framework delivers interpretable, instance-level explanations for state-of-the-art ensemble forecasts, while equipping users with forecastability metrics that serve as reliability indicators for both predictions and their explanations.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08739","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.582017","language":"en","tags":["computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":245,"author":"Yikai Zhao, Jiekai Ma","raw_content_length":1965,"priority":7,"update_frequency":1,"reading_time_minutes":1.225,"robust_parsing_used":true,"entities":{"organizations":["SHAP","Surrogate Models","Interpretable Explanations","AutoGluon"],"persons":[],"locations":[],"monetary":[]},"char_count":1964,"language_detected":"en","key_concepts":{"key_phrases":["Faithful and Interpretable Explanations","Complex Ensemble Time Series Forecasts","Surrogate Models","Forecastability Analysis","arXiv251008739v1 Announce Type","new Abstract","Modern time series forecasting","complex ensemble models","AutoML systems","AutoGluon"],"filter_categories":{"ai_ml":["Faithful and Interpretable Explanations"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Faithful and Interpretable Explanations":2.0,"Complex Ensemble Time Series Forecasts":2.0,"Surrogate Models":2.0,"Forecastability Analysis":2.0,"arXiv251008739v1 Announce Type":1.0,"new Abstract":1.0,"Modern time series forecasting":1.0,"complex ensemble models":1.0,"AutoML systems":1.0,"AutoGluon":1.0}},"age_hours":2.7612045658333333,"is_recent":true,"quality_score":1.0,"sentiment_score":9.124,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8248,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9337,"joy":0.0276,"surprise":0.016,"sadness":0.0035,"fear":0.0083,"anger":0.0077,"disgust":0.0033},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a methodology for improving the interpretability of time series forecasts, which could indirectly support sustainability efforts by enabling better decision-making based on those forecasts. However, it is still in the applied research phase with no mention of deployment or concrete impact on emissions. The 'vaporware' flag is raised due to the lack of deployed units or operational data.","key_impact_metrics":["fidelity between surrogate and original forecasting model","spectral predictability correlation with forecast accuracy"],"technology_tags":["time series forecasting","machine learning","explainable AI"],"sdg_alignment":[9,13],"analyzed_at":"2025-10-29T09:25:22.053922Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_aed782b664b5","title":"Unending Sequential Auctions","content":"arXiv:2510.08742v1 Announce Type: new Abstract: Sequential auctions for identical items with unit-demand, private-value buyers are common and often occur periodically without end, as new bidders replace departing ones. We model bidder uncertainty by introducing a probability that a bidder must exit the auction in each period. Treating the sequential auction as a Markov process, we demonstrate the existence of a unique steady state. In the absence of uncertainty, the steady state resembles a posted-price mechanism: bidders with values above a threshold almost surely win items by repeatedly bidding the threshold price, while those below the threshold almost surely do not. The equilibrium price corresponds to the threshold value that balances supply (bidders with values above the threshold) and demand (auction winners). When uncertainty is introduced, the threshold value persists but becomes less precise, growing \"fuzzier\" as uncertainty increases. This uncertainty benefits low-value bidders, those below the threshold, by giving them a significant chance of winning. Surprisingly, high-value bidders also benefit from uncertainty, up to a certain value limit, as it lowers equilibrium bids and increases their expected utility. On the other hand, this bidder uncertainty often reduces the auctioneer's utility.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08742","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.582829","language":"en","tags":["computer-science","csgt","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":190,"author":"Amir Ban","raw_content_length":1328,"priority":7,"update_frequency":1,"reading_time_minutes":0.95,"robust_parsing_used":true,"entities":{"organizations":[],"persons":["Markov"],"locations":["Sequential"],"monetary":[]},"char_count":1323,"language_detected":"en","key_concepts":{"key_phrases":["Unending Sequential Auctions","arXiv251008742v1 Announce Type","new Abstract","Sequential auctions","identical items","unit-demand","private-value buyers","end","new bidders","departing ones"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Unending Sequential Auctions":2.0,"arXiv251008742v1 Announce Type":1.0,"new Abstract":1.0,"Sequential auctions":1.0,"identical items":1.0,"unit-demand":1.0,"private-value buyers":1.0,"end":1.0,"new bidders":1.0,"departing ones":1.0}},"age_hours":2.7612342791666666,"is_recent":true,"quality_score":1.0,"sentiment_score":2.0705,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5859,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8933,"joy":0.0064,"surprise":0.0407,"sadness":0.0069,"fear":0.0388,"anger":0.0091,"disgust":0.0048},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":1,"systemic_impact":2,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":false,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":false,"fossil_transition":false},"reasoning":"This paper presents a theoretical model of sequential auctions and their impact on bidder behavior under uncertainty. While the research is technically sound and peer-reviewed, it does not directly translate into concrete actions or measurable outcomes related to sustainability. The model could potentially inform the design of carbon markets or resource allocation mechanisms, but it is currently at a basic research stage with no deployment.","key_impact_metrics":[],"technology_tags":["auction theory","mechanism design"],"sdg_alignment":[],"analyzed_at":"2025-10-29T09:25:25.028048Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_3038db1403c1","title":"Graph Diffusion Transformers are In","content":"arXiv:2510.08744v1 Announce Type: new Abstract: In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5$\\times$ fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000$\\times$ larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08744","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.583235","language":"en","tags":["research","csai","preprints","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":182,"author":"Gang Liu, Jie Chen, Yihan Zhu, Michael Sun, Tengfei Luo, Nitesh V Chawla, Meng Jiang","raw_content_length":1346,"priority":7,"update_frequency":1,"reading_time_minutes":0.91,"robust_parsing_used":true,"entities":{"organizations":["Transformer","DemoDiff","Node Pair Encoding"],"persons":[],"locations":[],"monetary":["0.7-billion"]},"char_count":1345,"language_detected":"en","key_concepts":{"key_phrases":["Graph Diffusion Transformers","arXiv251008744v1 Announce Type","new Abstract","context learning","large models","new tasks","a few demonstrations","limited success","molecular design","Existing databases"],"filter_categories":{"ai_ml":["Graph Diffusion Transformers"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Graph Diffusion Transformers":2.0,"arXiv251008744v1 Announce Type":1.0,"new Abstract":1.0,"context learning":1.0,"large models":1.0,"new tasks":1.0,"a few demonstrations":1.0,"limited success":1.0,"molecular design":1.0,"Existing databases":1.0}},"age_hours":2.7612521683333333,"is_recent":true,"quality_score":1.0,"sentiment_score":5.577,"sentiment_category":"neutral","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.1154,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.8796,"joy":0.0036,"surprise":0.0155,"sadness":0.0161,"fear":0.0352,"anger":0.0254,"disgust":0.0247},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":2,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article presents a new machine learning model for molecular design, which could potentially lead to the discovery of new materials or drugs with improved environmental properties. However, it is currently in the research stage with no deployed technology or measured environmental outcomes. The model achieves an average rank of 3.63 compared to other approaches, indicating a performance improvement.","key_impact_metrics":["5.5x fewer nodes required for molecular tokenizer","Average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches"],"technology_tags":["machine learning","molecular design","diffusion models"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:28.379682Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_51fcc744e719","title":"RFOD: Random Forest","content":"arXiv:2510.08747v1 Announce Type: new Abstract: Outlier detection in tabular data is crucial for safeguarding data integrity in high-stakes domains such as cybersecurity, financial fraud detection, and healthcare, where anomalies can cause serious operational and economic impacts. Despite advances in both data mining and deep learning, many existing methods struggle with mixed-type tabular data, often relying on encoding schemes that lose important semantic information. Moreover, they frequently lack interpretability, offering little insight into which specific values cause anomalies. To overcome these challenges, we introduce \\textsf{\\textbf{RFOD}}, a novel \\textsf{\\textbf{R}}andom \\textsf{\\textbf{F}}orest-based \\textsf{\\textbf{O}}utlier \\textsf{\\textbf{D}}etection framework tailored for tabular data. Rather than modeling a global joint distribution, \\textsf{RFOD} reframes anomaly detection as a feature-wise conditional reconstruction problem, training dedicated random forests for each feature conditioned on the others. This design robustly handles heterogeneous data types while preserving the semantic integrity of categorical features. To further enable precise and interpretable detection, \\textsf{RFOD} combines Adjusted Gower's Distance (AGD) for cell-level scoring, which adapts to skewed numerical data and accounts for categorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate cell-level scores into robust row-level anomaly scores. Extensive experiments on 15 real-world datasets demonstrate that \\textsf{RFOD} consistently outperforms state-of-the-art baselines in detection accuracy while offering superior robustness, scalability, and interpretability for mixed-type tabular data.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08747","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.583686","language":"en","tags":["research","preprints","csdb","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":205,"author":"Yihao Ang, Peicheng Yao, Yifan Bao, Yushuo Feng, Qiang Huang, Anthony K. H. Tung, Zhiyong Huang","raw_content_length":1732,"priority":7,"update_frequency":1,"reading_time_minutes":1.025,"robust_parsing_used":true,"entities":{"organizations":["Random Forest arXiv:2510.08747v1 Announce Type"],"persons":["Outlier"],"locations":[],"monetary":[]},"char_count":1731,"language_detected":"en","key_concepts":{"key_phrases":["RFOD","Random Forest","arXiv251008747v1 Announce Type","new Abstract","Outlier detection","tabular data","data integrity","high-stakes domains","cybersecurity","financial fraud detection"],"filter_categories":{"ai_ml":["high-stakes domains"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"RFOD":2.0,"Random Forest":2.0,"arXiv251008747v1 Announce Type":1.0,"new Abstract":1.0,"Outlier detection":1.0,"tabular data":1.0,"data integrity":1.0,"high-stakes domains":1.0,"cybersecurity":1.0,"financial fraud detection":1.0}},"age_hours":2.761267158888889,"is_recent":true,"quality_score":1.0,"sentiment_score":2.213,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.5574,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.3898,"joy":0.0032,"surprise":0.0207,"sadness":0.0436,"fear":0.4717,"anger":0.0553,"disgust":0.0157},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel outlier detection method (RFOD) for tabular data, which can be applied to various domains including cybersecurity, financial fraud detection, and healthcare. While the method shows improved detection accuracy on 15 real-world datasets, its direct climate impact is indirect, as it improves data integrity which can then be used for sustainability applications. The technology is in the applied research stage, with experiments on datasets but no deployed units.","key_impact_metrics":["Detection accuracy improvement","Scalability for mixed-type data"],"technology_tags":["Outlier detection","Random Forest","Machine Learning","Data Integrity"],"sdg_alignment":[9,16],"analyzed_at":"2025-10-29T09:25:31.375390Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_4f6cb28ce439","title":"Conformal Risk Training: End","content":"arXiv:2510.08748v1 Announce Type: new Abstract: While deep learning models often achieve high predictive accuracy, their predictions typically do not come with any provable guarantees on risk or reliability, which are critical for deployment in high-stakes applications. The framework of conformal risk control (CRC) provides a distribution-free, finite-sample method for controlling the expected value of any bounded monotone loss function and can be conveniently applied post-hoc to any pre-trained deep learning model. However, many real-world applications are sensitive to tail risks, as opposed to just expected loss. In this work, we develop a method for controlling the general class of Optimized Certainty-Equivalent (OCE) risks, a broad class of risk measures which includes as special cases the expected loss (generalizing the original CRC method) and common tail risks like the conditional value-at-risk (CVaR). Furthermore, standard post-hoc CRC can degrade average-case performance due to its lack of feedback to the model. To address this, we introduce \"conformal risk training,\" an end-to-end approach that differentiates through conformal OCE risk control during model training or fine-tuning. Our method achieves provable risk guarantees while demonstrating significantly improved average-case performance over post-hoc approaches on applications to controlling classifiers' false negative rate and controlling financial risk in battery storage operation.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08748","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.584108","language":"en","tags":["computer-science","cslg","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":202,"author":"Christopher Yeh, Nicolas Christianson, Adam Wierman, Yisong Yue","raw_content_length":1473,"priority":7,"update_frequency":1,"reading_time_minutes":1.01,"robust_parsing_used":true,"entities":{"organizations":["Optimized Certainty-Equivalent","CRC"],"persons":[],"locations":[],"monetary":[]},"char_count":1472,"language_detected":"en","key_concepts":{"key_phrases":["Conformal Risk Training","End","arXiv251008748v1 Announce Type","new Abstract","deep learning models","high predictive accuracy","their predictions","any provable guarantees","risk","reliability"],"filter_categories":{"ai_ml":["Conformal Risk Training","deep learning models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Conformal Risk Training":2.0,"End":2.0,"arXiv251008748v1 Announce Type":1.0,"new Abstract":1.0,"deep learning models":1.0,"high predictive accuracy":1.0,"their predictions":1.0,"any provable guarantees":1.0,"risk":1.0,"reliability":1.0}},"age_hours":2.7612834675,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2105,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7579,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.9166,"joy":0.0068,"surprise":0.0064,"sadness":0.0045,"fear":0.0315,"anger":0.0214,"disgust":0.0127},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":5,"technical_credibility":7,"economic_viability":5,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":7,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper presents a novel method for improving the reliability of deep learning models in high-stakes applications, with demonstrated improvements in controlling classifiers' false negative rate and financial risk in battery storage operation. The method is validated through experiments, but deployment readiness is still at the pilot stage. The use of conformal risk training could improve the efficiency and reliability of various sustainability applications, such as grid management and resource allocation.","key_impact_metrics":["Improved average-case performance","Provable risk guarantees"],"technology_tags":["Conformal risk control","Deep learning","Battery storage optimization"],"sdg_alignment":[7,9,13],"analyzed_at":"2025-10-29T09:25:34.821221Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_80090ce6d0c8","title":"Exploring Cross","content":"arXiv:2510.08750v1 Announce Type: new Abstract: Federated learning (FL) enables collaborative training without raw data sharing, but still risks training data memorization. Existing FL memorization detection techniques focus on one sample at a time, underestimating more subtle risks of cross-sample memorization. In contrast, recent work on centralized learning (CL) has introduced fine-grained methods to assess memorization across all samples in training data, but these assume centralized access to data and cannot be applied directly to FL. We bridge this gap by proposing a framework that quantifies both intra- and inter-client memorization in FL using fine-grained cross-sample memorization measurement across all clients. Based on this framework, we conduct two studies: (1) measuring subtle memorization across clients and (2) examining key factors that influence memorization, including decoding strategies, prefix length, and FL algorithms. Our findings reveal that FL models do memorize client data, particularly intra-client data, more than inter-client data, with memorization influenced by training and inferencing factors.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08750","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.584540","language":"en","tags":["research","preprints","cscl","cslg","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":155,"author":"Tinnakit Udsa, Can Udomcharoenchaikit, Patomporn Payoungkhamdee, Sarana Nutanong, Norrathep Rattanavipanon","raw_content_length":1140,"priority":7,"update_frequency":1,"reading_time_minutes":0.775,"robust_parsing_used":true,"entities":{"organizations":["Exploring Cross"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1139,"language_detected":"en","key_concepts":{"key_phrases":["Cross","arXiv251008750v1 Announce Type","new Abstract","Federated learning","collaborative training","raw data sharing","data memorization","Existing FL memorization detection techniques","one sample","a time"],"filter_categories":{"ai_ml":["collaborative training"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Cross":2.0,"arXiv251008750v1 Announce Type":1.0,"new Abstract":1.0,"Federated learning":1.0,"collaborative training":1.0,"raw data sharing":1.0,"data memorization":1.0,"Existing FL memorization detection techniques":1.0,"one sample":1.0,"a time":1.0}},"age_hours":2.761299556388889,"is_recent":true,"quality_score":1.0,"sentiment_score":1.2530000000000001,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.7494,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8802,"joy":0.0045,"surprise":0.02,"sadness":0.0087,"fear":0.0582,"anger":0.0183,"disgust":0.0102},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"basic_research","climate_impact_potential":2,"technical_credibility":7,"economic_viability":2,"deployment_readiness":2,"systemic_impact":3,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This research explores memorization in federated learning models, showing that FL models memorize client data. While it identifies a potential risk in data privacy, it doesn't directly translate to concrete climate action or measurable environmental outcomes. The research is at a basic research stage with no deployment or economic viability demonstrated.","key_impact_metrics":["Intra-client memorization rate","Inter-client memorization rate"],"technology_tags":["Federated Learning","Data Privacy"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:37.753379Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_a36f3a335813","title":"Wireless Datasets for Aerial Networks","content":"arXiv:2510.08752v1 Announce Type: new Abstract: The integration of unmanned aerial vehicles (UAVs) into 5G-Advanced and future 6G networks presents a transformative opportunity for wireless connectivity, enabling agile deployment and improved LoS communications. However, the effective design and optimization of these aerial networks depend critically on high-quality, empirical data. This paper provides a comprehensive survey of publicly available wireless datasets collected from an airborne platform called Aerial Experimentation and Research Platform on Advanced Wireless (AERPAW). We highlight the unique challenges associated with generating reproducible aerial wireless datasets, and review the existing related works in the literature. Subsequently, for each dataset considered, we explain the hardware and software used, present the dataset format, provide representative results, and discuss how these datasets can be used to conduct additional research. The specific aerial wireless datasets presented include raw I/Q samples from a cellular network over different UAV trajectories, spectrum measurements at different altitudes, flying 4G base station (BS), a 5G-NSA Ericsson network, a LoRaWAN network, an radio frequency (RF) sensor network for source localization, wireless propagation data for various scenarios, and comparison of ray tracing and real-world propagation scenarios. References to all datasets and post-processing scripts are provided to enable full reproducibility of the results. Ultimately, we aim to guide the community toward effective dataset utilization for validating propagation models, developing machine learning algorithms, and advancing the next generation of aerial wireless systems.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08752","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.584971","language":"en","tags":["research","preprints","csni","eesssp","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":229,"author":"Amir Hossein Fahim Raouf, Donggu Lee, Mushfiqur Rahman, Saad Masrur, Gautham Reddy, Cole Dickerson, Md Sharif Hossen, Sergio Vargas Villar, An{\\i}l G\\\"urses, Simran Singh, Sung Joon Maeng, Martins Ezuma, Christopher Roberts, Mohamed Rabeek Sarbudeen, Thomas J. Zajkowski, Magreth Mushi, Ozgur Ozdemir, Ram Asokan, Ismail Guvenc, Mihail L. Sichitiu, Rudra Dutta","raw_content_length":1729,"priority":7,"update_frequency":1,"reading_time_minutes":1.145,"robust_parsing_used":true,"entities":{"organizations":["LoS","UAVs","Aerial Experimentation and Research Platform"],"persons":[],"locations":[],"monetary":[]},"char_count":1728,"language_detected":"en","key_concepts":{"key_phrases":["Wireless Datasets","Aerial Networks","arXiv251008752v1 Announce Type","new Abstract","The integration","unmanned aerial vehicles","UAVs","5G-Advanced and future 6G networks","a transformative opportunity","wireless connectivity"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Wireless Datasets":2.0,"Aerial Networks":2.0,"arXiv251008752v1 Announce Type":1.0,"new Abstract":1.0,"The integration":1.0,"unmanned aerial vehicles":1.0,"UAVs":1.0,"5G-Advanced and future 6G networks":1.0,"a transformative opportunity":1.0,"wireless connectivity":1.0}},"age_hours":2.7613145819444447,"is_recent":true,"quality_score":1.0,"sentiment_score":9.559,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.9118,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8976,"joy":0.0205,"surprise":0.048,"sadness":0.0047,"fear":0.0109,"anger":0.0127,"disgust":0.0058},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":3,"technical_credibility":7,"economic_viability":3,"deployment_readiness":3,"systemic_impact":4,"justice_equity":3,"innovation_quality":6,"evidence_strength":7,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"This paper focuses on providing datasets for aerial wireless networks, which can indirectly contribute to sustainability by enabling more efficient communication and resource management. The datasets are from a research platform (AERPAW) and include raw I/Q samples, spectrum measurements, and propagation data. While the paper itself doesn't deploy technology, it provides the foundation for future research and development in areas like smart agriculture or disaster response, which could have sustainability benefits.","key_impact_metrics":["Spectrum measurements at different altitudes","Wireless propagation data for various scenarios"],"technology_tags":["UAV","5G-Advanced","6G","Wireless Networks","LoRaWAN"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:40.746202Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_162d85dc4210","title":"Point and Go: Intuitive Reference Frame Reallocation in Mode Switching for Assistive Robotics","content":"arXiv:2510.08753v1 Announce Type: new Abstract: Operating high degree of freedom robots can be difficult for users of wheelchair mounted robotic manipulators. Mode switching in Cartesian space has several drawbacks such as unintuitive control reference frames, separate translation and orientation control, and limited movement capabilities that hinder performance. We propose Point and Go mode switching, which reallocates the Cartesian mode switching reference frames into a more intuitive action space comprised of new translation and rotation modes. We use a novel sweeping motion to point the gripper, which defines the new translation axis along the robot base frame's horizontal plane. This creates an intuitive `point and go' translation mode that allows the user to easily perform complex, human-like movements without switching control modes. The system's rotation mode combines position control with a refined end-effector oriented frame that provides precise and consistent robot actions in various end-effector poses. We verified its effectiveness through initial experiments, followed by a three-task user study that compared our method to Cartesian mode switching and a state of the art learning method. Results show that Point and Go mode switching reduced completion times by 31\\%, pauses by 41\\%, and mode switches by 33\\%, while receiving significantly favorable responses in user surveys.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08753","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.585403","language":"en","tags":["csro","computer-science","preprints","research","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":203,"author":"A. Wang, C. Jiang, M. Przystupa, J. Valentine, M. Jagersand","raw_content_length":1409,"priority":7,"update_frequency":1,"reading_time_minutes":1.015,"robust_parsing_used":true,"entities":{"organizations":["Point"],"persons":["Mode"],"locations":[],"monetary":[]},"char_count":1408,"language_detected":"en","key_concepts":{"key_phrases":["Point","Intuitive Reference Frame Reallocation","Mode Switching","Assistive Robotics","new Abstract","Operating high degree","freedom robots","users","wheelchair","robotic manipulators"],"filter_categories":{"ai_ml":["wheelchair"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Point":3.0,"Intuitive Reference Frame Reallocation":2.0,"Mode Switching":2.0,"Assistive Robotics":2.0,"new Abstract":1.0,"Operating high degree":1.0,"freedom robots":1.0,"users":1.0,"wheelchair":1.0,"robotic manipulators":1.0}},"age_hours":2.7613302880555555,"is_recent":true,"quality_score":1.0,"sentiment_score":6.0115,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.2023,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8434,"joy":0.0059,"surprise":0.0334,"sadness":0.0529,"fear":0.0237,"anger":0.0183,"disgust":0.0223},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":2,"technical_credibility":6,"economic_viability":3,"deployment_readiness":3,"systemic_impact":3,"justice_equity":5,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":true,"has_deployment":false},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel method for controlling assistive robots, showing a reduction in completion times, pauses, and mode switches in a user study. While the technology could improve the lives of disabled individuals, its direct climate impact is minimal. The study provides some evidence, but it is still in the early stages of deployment.","key_impact_metrics":["completion times reduced by 31%","pauses reduced by 41%"],"technology_tags":["assistive robotics","human-robot interaction","robot control"],"sdg_alignment":[3,10],"analyzed_at":"2025-10-29T09:25:43.838617Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
{"id":"science_arxiv_cs_acfb341f67ed","title":"Whole Body Model Predictive Control for Spin","content":"arXiv:2510.08754v1 Announce Type: new Abstract: Developing table tennis robots that mirror human speed, accuracy, and ability to predict and respond to the full range of ball spins remains a significant challenge for legged robots. To demonstrate these capabilities we present a system to play dynamic table tennis for quadrupedal robots that integrates high speed perception, trajectory prediction, and agile control. Our system uses external cameras for high-speed ball localization, physical models with learned residuals to infer spin and predict trajectories, and a novel model predictive control (MPC) formulation for agile full-body control. Notably, a continuous set of stroke strategies emerge automatically from different ball return objectives using this control paradigm. We demonstrate our system in the real world on a Spot quadruped, evaluate accuracy of each system component, and exhibit coordination through the system's ability to aim and return balls with varying spin types. As a further demonstration, the system is able to rally with human players.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.08754","published_date":"2025-10-13T04:00:00","collected_date":"2025-10-13T06:41:04.585803","language":"en","tags":["research","preprints","eesssy","cssy","csro","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":157,"author":"David Nguyen, Zulfiqar Zaidi, Kevin Karol, Jessica Hodgins, Zhaoming Xie","raw_content_length":1072,"priority":7,"update_frequency":1,"reading_time_minutes":0.785,"robust_parsing_used":true,"entities":{"organizations":["MPC","Spot"],"persons":["Whole Body Model Predictive Control"],"locations":[],"monetary":[]},"char_count":1071,"language_detected":"en","key_concepts":{"key_phrases":["Whole Body Model Predictive Control","Spin","Announce Type","new Abstract","Developing table tennis robots","human speed","accuracy","ability","the full range","ball spins"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Whole Body Model Predictive Control":2.0,"Spin":2.0,"Announce Type":1.0,"new Abstract":1.0,"Developing table tennis robots":1.0,"human speed":1.0,"accuracy":1.0,"ability":1.0,"the full range":1.0,"ball spins":1.0}},"age_hours":2.7613465255555556,"is_recent":true,"quality_score":1.0,"sentiment_score":9.063,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.8126,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9073,"joy":0.0144,"surprise":0.058,"sadness":0.0032,"fear":0.0058,"anger":0.008,"disgust":0.0033},"emotion_method":"local"},"sustainability_analysis":{"content_type":"breakthrough_research","innovation_stage":"applied_research","climate_impact_potential":1,"technical_credibility":7,"economic_viability":2,"deployment_readiness":3,"systemic_impact":1,"justice_equity":3,"innovation_quality":6,"evidence_strength":6,"investment_signals":{"has_funding":false,"has_patents":false,"has_customers":false,"has_metrics":true,"has_peer_review":false,"has_deployment":true},"verification_indicators":{"owid_indicator":false,"ipcc_alignment":false,"iea_data":false,"third_party_verified":false,"regulatory_approved":false},"flags":{"greenwashing_risk":false,"vaporware_risk":true,"fossil_transition":false},"reasoning":"The article describes a novel control system for quadrupedal robots, demonstrated on a Spot robot playing table tennis. While innovative, its direct climate impact is negligible. The system demonstrates accuracy in ball localization and trajectory prediction, but is in an early stage of deployment.","key_impact_metrics":["Accuracy of ball localization","Accuracy of trajectory prediction"],"technology_tags":["robotics","model predictive control","computer vision"],"sdg_alignment":[9],"analyzed_at":"2025-10-29T09:25:46.933910Z","analyzed_by":"gemini-api-batch","filter_name":"sustainability"}}
