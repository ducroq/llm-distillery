# LLM Distillery ğŸ¥ƒ

**Transform large language model expertise into fast, specialized local models**

LLM Distillery is a framework for distilling knowledge from large foundation models (Claude, Gemini, GPT-4) into small, domain-specific classifiers that run locally at 100x lower cost and 50x faster inference.

## Overview

Large language models excel at nuanced judgment tasks but are expensive and slow for production use. This framework:

1. **Generates ground truth datasets** using LLM oracles (Claude, Gemini)
2. **Fine-tunes small models** (BERT, DeBERTa, T5) on the ground truth
3. **Validates quality** by comparing small model predictions to LLM oracle
4. **Deploys locally** for fast, cost-effective inference

### Use Cases

- **Content filtering**: Sustainability impact, uplifting news, policy relevance
- **Investment intelligence**: Technology readiness, greenwashing detection, evidence strength
- **Quality assessment**: Clinical evidence, regulatory status, technical credibility
- **Multi-dimensional scoring**: Rate content on 8+ dimensions simultaneously

## Current Status (October 2025)

### âœ… Completed
- **Model Calibration**: Compare Claude vs Gemini to select best oracle
- **Generic Batch Labeler**: Universal labeling engine for any semantic filter
- **Secrets Management**: Secure API key handling (env vars + secrets.ini)
- **Uplifting Filter**: 8-dimension framework with prompt and validation
- **Timeout Protection**: 60s timeouts for LLM API calls
- **Caching System**: Save calibration results to `calibrations/<filter>/`
- **Comprehensive Documentation**: Architecture, guides, API reference

### ğŸš§ In Progress
- Ground truth generation CLI (`generate.py`)
- Stratified sampling strategies

### ğŸ“ Planned
- Training pipeline (BERT, DeBERTa, T5)
- Evaluation framework
- Inference server

## Quick Start

### 1. Install Dependencies

```bash
cd C:\local_dev\llm-distillery

# Using pip
pip install anthropic google-generativeai

# Or install all planned dependencies
pip install -r requirements.txt  # (when available)
```

### 2. Set Up API Keys

Create `config/credentials/secrets.ini`:

```ini
[api_keys]
anthropic_api_key = sk-ant-your_key_here
gemini_api_key = AIza_your_key_here
gemini_billing_api_key = AIza_billing_key  # Optional: 150 RPM vs 2 RPM
```

**Important**: This file is git-ignored for security.

### 3. Run Model Calibration

Compare Claude vs Gemini to choose the best LLM for ground truth:

```bash
python -m ground_truth.calibrate_models \
    --prompt prompts/uplifting.md \
    --source ../content-aggregator/data/content_items_20251022_145619.jsonl \
    --sample-size 100 \
    --output reports/uplifting_calibration.md \
    --seed 42
```

**Output**:
- Calibration report: `reports/uplifting_calibration.md`
- Cached labels: `calibrations/uplifting/{claude,gemini}_labels.jsonl`

See [Calibration Guide](docs/guides/calibration.md) for details.

### 4. Generate Ground Truth (Planned)

```bash
# Coming soon
python -m ground_truth.generate \
    --prompt prompts/uplifting.md \
    --input ../content-aggregator/data/content_items_*.jsonl \
    --output datasets/uplifting_50k.jsonl \
    --num-samples 50000 \
    --llm claude  # or gemini, based on calibration
```

### 5. Train a Model (Planned)

```bash
# Coming soon
python -m training.train \
    --config training/configs/uplifting_deberta.yaml \
    --dataset datasets/uplifting_50k.jsonl \
    --output inference/models/uplifting_v1
```

### 6. Evaluate Quality (Planned)

```bash
# Coming soon
python -m evaluation.evaluate \
    --model inference/models/uplifting_v1 \
    --test-set datasets/splits/uplifting_test.jsonl \
    --oracle claude
```

### 7. Run Inference (Planned)

```bash
# Coming soon - single prediction
python -m inference.predict \
    --model inference/models/uplifting_v1 \
    --text "Community members organize climate action workshop..."

# Batch prediction
python -m inference.batch_predict \
    --model inference/models/uplifting_v1 \
    --input articles.jsonl \
    --output predictions.jsonl
```

## Documentation

- **[Full Documentation](docs/README.md)** - Complete docs index
- **[Calibration Guide](docs/guides/calibration.md)** - How to compare Claude vs Gemini
- **[Architecture Overview](docs/architecture/overview.md)** - System design and components
- **[Project Structure](#project-structure)** - Directory organization

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GROUND TRUTH GENERATION                   â”‚
â”‚                                                              â”‚
â”‚  Raw Articles  â†’  LLM Oracle  â†’  Labeled Dataset            â”‚
â”‚  (50K samples)    (Claude)       (JSON ratings)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FINE-TUNING                            â”‚
â”‚                                                              â”‚
â”‚  Ground Truth  â†’  Small Model  â†’  Trained Classifier        â”‚
â”‚  (JSONL)          (DeBERTa)       (90%+ accuracy)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      VALIDATION                              â”‚
â”‚                                                              â”‚
â”‚  Test Set  â†’  Compare  â†’  Quality Metrics                   â”‚
â”‚               (Model vs Oracle)   (Accuracy, MAE, F1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPLOYMENT                                â”‚
â”‚                                                              â”‚
â”‚  Article  â†’  Local Model  â†’  Predictions                    â”‚
â”‚  (input)     (<50ms)          (8 dimensions)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
llm-distillery/
â”œâ”€â”€ prompts/                    # LLM evaluation prompts
â”‚   â”œâ”€â”€ sustainability.md       # Climate tech & impact assessment
â”‚   â”œâ”€â”€ uplifting.md           # Positive/uplifting content detection
â”‚   â”œâ”€â”€ eu_policy.md           # EU policy relevance
â”‚   â””â”€â”€ healthcare_ai.md       # Healthcare AI readiness
â”‚
â”œâ”€â”€ ground_truth/              # Ground truth generation
â”‚   â”œâ”€â”€ generate.py           # Main CLI for dataset creation
â”‚   â”œâ”€â”€ samplers.py           # Stratified sampling strategies
â”‚   â”œâ”€â”€ llm_evaluators.py     # Claude/Gemini API wrappers
â”‚   â””â”€â”€ validators.py         # Quality checks, consistency
â”‚
â”œâ”€â”€ datasets/                  # Generated datasets
â”‚   â”œâ”€â”€ sustainability_50k.jsonl
â”‚   â””â”€â”€ splits/               # Train/val/test splits
â”‚
â”œâ”€â”€ training/                  # Model fine-tuning
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ models.py             # Model architectures
â”‚   â”œâ”€â”€ datasets.py           # PyTorch Dataset classes
â”‚   â””â”€â”€ configs/              # Training configurations
â”‚       â””â”€â”€ sustainability_deberta.yaml
â”‚
â”œâ”€â”€ evaluation/                # Quality assessment
â”‚   â”œâ”€â”€ evaluate.py           # Model vs oracle comparison
â”‚   â”œâ”€â”€ calibration.py        # Drift detection
â”‚   â””â”€â”€ metrics.py            # Custom metrics
â”‚
â”œâ”€â”€ inference/                 # Deployed models
â”‚   â”œâ”€â”€ predict.py            # Single prediction
â”‚   â”œâ”€â”€ batch_predict.py      # Batch processing
â”‚   â”œâ”€â”€ serve.py              # FastAPI server
â”‚   â””â”€â”€ models/               # Model checkpoints
â”‚       â””â”€â”€ sustainability_v1/
â”‚
â””â”€â”€ tests/                     # Unit tests
    â”œâ”€â”€ test_ground_truth.py
    â”œâ”€â”€ test_training.py
    â””â”€â”€ test_inference.py
```

## Available Filters

### 1. Sustainability Impact Filter
**Dimensions**: Climate impact, technical credibility, economic viability, deployment readiness, systemic impact, justice & equity, innovation quality, evidence strength

**Use Cases**:
- Climate tech investment intelligence
- Greenwashing detection
- Progress tracking for climate solutions

**Model**: `inference/models/sustainability_v1`

### 2. Uplifting Content Filter
**Dimensions**: Agency, progress, collective benefit, connection, innovation, justice, resilience, wonder

**Use Cases**:
- Positive news aggregation
- Solutions journalism
- Progress indicators

**Model**: `inference/models/uplifting_v1`

### 3. EU Policy Relevance Filter
**Dimensions**: Regulatory impact, compliance relevance, timeline urgency, affected sectors

**Use Cases**:
- Policy intelligence
- Compliance tracking
- Regulatory monitoring

**Model**: `inference/models/eu_policy_v1` *(planned)*

### 4. Healthcare AI Readiness Filter
**Dimensions**: Clinical evidence level, regulatory status, patient safety, adoption readiness

**Use Cases**:
- Healthcare AI due diligence
- FDA clearance tracking
- Clinical validation assessment

**Model**: `inference/models/healthcare_ai_v1` *(planned)*

## Cost Analysis

### Ground Truth Generation (One-time)
- **50K articles** Ã— **$0.003/article** (Claude 3.5 Sonnet)
- **Total**: ~$150 per filter

### Local Model Inference (Ongoing)
- **Cost**: $0 (runs locally)
- **Speed**: 20-50ms per article
- **Accuracy**: 90-95% vs. Claude

### ROI Calculation
If processing **4,000 articles/day**:
- **Claude API**: 4,000 Ã— $0.003 Ã— 365 = **$4,380/year**
- **Local Model**: Training cost $150 + hosting ~$10/month = **$270/year**
- **Savings**: **$4,110/year per filter** (94% cost reduction)

## Performance Benchmarks

| Model | Size | Inference Time | Memory | Accuracy vs Claude |
|-------|------|----------------|--------|--------------------|
| DistilBERT | 66M params | 15ms | 500MB | 88-92% |
| DeBERTa-v3-small | 44M params | 25ms | 400MB | 90-94% |
| BERT-base | 110M params | 35ms | 800MB | 91-95% |
| Flan-T5-base | 250M params | 80ms | 1.5GB | 93-96% |

**Recommended**: DeBERTa-v3-small (best quality/speed tradeoff)

## Roadmap

### Phase 1: Ground Truth Generation âœ… (Current)
- [x] Model calibration (Claude vs Gemini)
- [x] Generic batch labeler
- [x] Uplifting filter prompt & validation
- [x] Secrets management
- [x] Timeout protection & caching
- [ ] Full ground truth generation CLI
- [ ] Stratified sampling

### Phase 2: Training Pipeline (Next)
- [ ] PyTorch training script
- [ ] Model architectures (BERT, DeBERTa, T5)
- [ ] Training configs
- [ ] Experiment tracking (W&B)

### Phase 3: Evaluation & Deployment
- [ ] Model vs oracle comparison
- [ ] Calibration drift detection
- [ ] FastAPI inference server
- [ ] Docker deployment

### Phase 4: Additional Filters
- [ ] Sustainability filter
- [ ] EU policy relevance filter
- [ ] Healthcare AI readiness filter

### Phase 5: Advanced Features
- [ ] Multi-task learning
- [ ] Active learning
- [ ] Model compression (quantization, pruning)
- [ ] Model registry with versioning

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Citation

If you use LLM Distillery in your research or project, please cite:

```bibtex
@software{llm_distillery,
  title = {LLM Distillery: Knowledge Distillation from Large Language Models},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/llm-distillery}
}
```

## Acknowledgments

- Built for the [Content Aggregator](https://github.com/yourusername/content-aggregator) project
- Inspired by model distillation research from Hinton et al.
- LLM evaluation powered by Anthropic Claude and Google Gemini
