# LLM Distillery ğŸ¥ƒ

**Transform large language model expertise into fast, specialized local models**

LLM Distillery is a framework for distilling knowledge from large foundation models (Claude, Gemini, GPT-4) into small, domain-specific classifiers that run locally at 100x lower cost and 50x faster inference.

## Overview

Large language models excel at nuanced judgment tasks but are expensive and slow for production use. This framework:

1. **Generates ground truth datasets** using LLM oracles (Claude, Gemini)
2. **Fine-tunes small models** (BERT, DeBERTa, T5) on the ground truth
3. **Validates quality** by comparing small model predictions to LLM oracle
4. **Deploys locally** for fast, cost-effective inference

### Use Cases

- **Content filtering**: Sustainability impact, uplifting news, policy relevance
- **Investment intelligence**: Technology readiness, greenwashing detection, evidence strength
- **Quality assessment**: Clinical evidence, regulatory status, technical credibility
- **Multi-dimensional scoring**: Rate content on 8+ dimensions simultaneously

## Quick Start

### 1. Install Dependencies

```bash
# Using pip
pip install -r requirements.txt

# Or using Poetry
poetry install
```

### 2. Set Up API Keys

```bash
cp .env.example .env
# Edit .env and add your API keys:
# ANTHROPIC_API_KEY=your_key_here
```

### 3. Generate Ground Truth

```bash
python ground_truth/generate.py \
    --prompt prompts/sustainability.md \
    --input-dir ../content-aggregator/data/collected \
    --output datasets/sustainability_50k.jsonl \
    --num-samples 50000 \
    --llm claude-3.5-sonnet
```

### 4. Train a Model

```bash
python training/train.py \
    --config training/configs/sustainability_deberta.yaml \
    --dataset datasets/sustainability_50k.jsonl \
    --output inference/models/sustainability_v1
```

### 5. Evaluate Quality

```bash
python evaluation/evaluate.py \
    --model inference/models/sustainability_v1 \
    --test-set datasets/splits/sustainability_test.jsonl \
    --oracle claude-3.5-sonnet
```

### 6. Run Inference

```bash
# Single prediction
python inference/predict.py \
    --model inference/models/sustainability_v1 \
    --text "Tesla announces new battery recycling facility..."

# Batch prediction
python inference/batch_predict.py \
    --model inference/models/sustainability_v1 \
    --input articles.jsonl \
    --output predictions.jsonl
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GROUND TRUTH GENERATION                   â”‚
â”‚                                                              â”‚
â”‚  Raw Articles  â†’  LLM Oracle  â†’  Labeled Dataset            â”‚
â”‚  (50K samples)    (Claude)       (JSON ratings)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FINE-TUNING                            â”‚
â”‚                                                              â”‚
â”‚  Ground Truth  â†’  Small Model  â†’  Trained Classifier        â”‚
â”‚  (JSONL)          (DeBERTa)       (90%+ accuracy)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      VALIDATION                              â”‚
â”‚                                                              â”‚
â”‚  Test Set  â†’  Compare  â†’  Quality Metrics                   â”‚
â”‚               (Model vs Oracle)   (Accuracy, MAE, F1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPLOYMENT                                â”‚
â”‚                                                              â”‚
â”‚  Article  â†’  Local Model  â†’  Predictions                    â”‚
â”‚  (input)     (<50ms)          (8 dimensions)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
llm-distillery/
â”œâ”€â”€ prompts/                    # LLM evaluation prompts
â”‚   â”œâ”€â”€ sustainability.md       # Climate tech & impact assessment
â”‚   â”œâ”€â”€ uplifting.md           # Positive/uplifting content detection
â”‚   â”œâ”€â”€ eu_policy.md           # EU policy relevance
â”‚   â””â”€â”€ healthcare_ai.md       # Healthcare AI readiness
â”‚
â”œâ”€â”€ ground_truth/              # Ground truth generation
â”‚   â”œâ”€â”€ generate.py           # Main CLI for dataset creation
â”‚   â”œâ”€â”€ samplers.py           # Stratified sampling strategies
â”‚   â”œâ”€â”€ llm_evaluators.py     # Claude/Gemini API wrappers
â”‚   â””â”€â”€ validators.py         # Quality checks, consistency
â”‚
â”œâ”€â”€ datasets/                  # Generated datasets
â”‚   â”œâ”€â”€ sustainability_50k.jsonl
â”‚   â””â”€â”€ splits/               # Train/val/test splits
â”‚
â”œâ”€â”€ training/                  # Model fine-tuning
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ models.py             # Model architectures
â”‚   â”œâ”€â”€ datasets.py           # PyTorch Dataset classes
â”‚   â””â”€â”€ configs/              # Training configurations
â”‚       â””â”€â”€ sustainability_deberta.yaml
â”‚
â”œâ”€â”€ evaluation/                # Quality assessment
â”‚   â”œâ”€â”€ evaluate.py           # Model vs oracle comparison
â”‚   â”œâ”€â”€ calibration.py        # Drift detection
â”‚   â””â”€â”€ metrics.py            # Custom metrics
â”‚
â”œâ”€â”€ inference/                 # Deployed models
â”‚   â”œâ”€â”€ predict.py            # Single prediction
â”‚   â”œâ”€â”€ batch_predict.py      # Batch processing
â”‚   â”œâ”€â”€ serve.py              # FastAPI server
â”‚   â””â”€â”€ models/               # Model checkpoints
â”‚       â””â”€â”€ sustainability_v1/
â”‚
â””â”€â”€ tests/                     # Unit tests
    â”œâ”€â”€ test_ground_truth.py
    â”œâ”€â”€ test_training.py
    â””â”€â”€ test_inference.py
```

## Available Filters

### 1. Sustainability Impact Filter
**Dimensions**: Climate impact, technical credibility, economic viability, deployment readiness, systemic impact, justice & equity, innovation quality, evidence strength

**Use Cases**:
- Climate tech investment intelligence
- Greenwashing detection
- Progress tracking for climate solutions

**Model**: `inference/models/sustainability_v1`

### 2. Uplifting Content Filter
**Dimensions**: Agency, progress, collective benefit, connection, innovation, justice, resilience, wonder

**Use Cases**:
- Positive news aggregation
- Solutions journalism
- Progress indicators

**Model**: `inference/models/uplifting_v1`

### 3. EU Policy Relevance Filter
**Dimensions**: Regulatory impact, compliance relevance, timeline urgency, affected sectors

**Use Cases**:
- Policy intelligence
- Compliance tracking
- Regulatory monitoring

**Model**: `inference/models/eu_policy_v1` *(planned)*

### 4. Healthcare AI Readiness Filter
**Dimensions**: Clinical evidence level, regulatory status, patient safety, adoption readiness

**Use Cases**:
- Healthcare AI due diligence
- FDA clearance tracking
- Clinical validation assessment

**Model**: `inference/models/healthcare_ai_v1` *(planned)*

## Cost Analysis

### Ground Truth Generation (One-time)
- **50K articles** Ã— **$0.003/article** (Claude 3.5 Sonnet)
- **Total**: ~$150 per filter

### Local Model Inference (Ongoing)
- **Cost**: $0 (runs locally)
- **Speed**: 20-50ms per article
- **Accuracy**: 90-95% vs. Claude

### ROI Calculation
If processing **4,000 articles/day**:
- **Claude API**: 4,000 Ã— $0.003 Ã— 365 = **$4,380/year**
- **Local Model**: Training cost $150 + hosting ~$10/month = **$270/year**
- **Savings**: **$4,110/year per filter** (94% cost reduction)

## Performance Benchmarks

| Model | Size | Inference Time | Memory | Accuracy vs Claude |
|-------|------|----------------|--------|--------------------|
| DistilBERT | 66M params | 15ms | 500MB | 88-92% |
| DeBERTa-v3-small | 44M params | 25ms | 400MB | 90-94% |
| BERT-base | 110M params | 35ms | 800MB | 91-95% |
| Flan-T5-base | 250M params | 80ms | 1.5GB | 93-96% |

**Recommended**: DeBERTa-v3-small (best quality/speed tradeoff)

## Roadmap

- [x] Sustainability filter (v1.0)
- [x] Uplifting content filter (v1.0)
- [ ] EU policy relevance filter (v1.1)
- [ ] Healthcare AI readiness filter (v1.1)
- [ ] Multi-task learning (single model, multiple dimensions)
- [ ] Active learning for continuous improvement
- [ ] Model compression (quantization, pruning)
- [ ] FastAPI inference server
- [ ] Docker deployment
- [ ] Model registry with versioning

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Citation

If you use LLM Distillery in your research or project, please cite:

```bibtex
@software{llm_distillery,
  title = {LLM Distillery: Knowledge Distillation from Large Language Models},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/llm-distillery}
}
```

## Acknowledgments

- Built for the [Content Aggregator](https://github.com/yourusername/content-aggregator) project
- Inspired by model distillation research from Hinton et al.
- LLM evaluation powered by Anthropic Claude and Google Gemini
