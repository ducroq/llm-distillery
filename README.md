# LLM Distillery ðŸ¥ƒ

**Transform large language model expertise into fast, specialized local models**

LLM Distillery is a framework for distilling knowledge from large foundation models (Gemini Flash) into small, domain-specific classifiers that run locally at 100x lower cost and 50x faster inference.

## Overview

Large language models excel at nuanced judgment tasks but are expensive and slow for production use. This framework:

1. **Generates ground truth datasets** using Gemini Flash as oracle (dimensional scoring only)
2. **Fine-tunes Qwen2.5-7B-Instruct** for multi-dimensional regression
3. **Validates quality** through comprehensive training data validation
4. **Deploys locally** for fast, cost-effective batch inference (150x faster than oracle)

### Use Cases

- **Content filtering**: Uplifting news, sustainability tech deployment, investment risk signals
- **Multi-dimensional scoring**: Rate content on 8 dimensions simultaneously (0-10 scale)
- **Tier classification**: Flexible postfilter tier assignment without model retraining

## Current Status (November 2025)

### ðŸš€ Production Ready Filters

- **sustainability_technology v1**: âœ… **DEPLOYED**
  - 6 dimensions (LCSA framework): technology_readiness, technical_performance, economic_competitiveness, life_cycle_environmental_impact, social_equity_impact, governance_systemic_impact
  - Model: Qwen2.5-1.5B + LoRA (18.5M trainable params)
  - Test MAE: 0.690 | All dimensions < 1.0
  - Deployed: [HuggingFace Hub](https://huggingface.co/jeergrvgreg/sustainability-technology-v1)

### ðŸŽ¯ Training Data Ready

- **uplifting v4**: 6,705 examples (validated, ready for training)
  - 8 dimensions: agency, progress, collective_benefit, connection, innovation, justice, resilience, wonder
  - Philosophy: MEANING not TONE

- **investment-risk v4**: 4,880 examples (validated, ready for training)
  - 8 dimensions: macro_risk, credit_stress, sentiment, valuation, policy, systemic, evidence, actionability
  - Philosophy: "You can't predict crashes, but you can prepare for them"

### âœ… Architecture Harmonization Complete (Nov 2025)

All filters now follow consistent oracle output discipline:
- âœ… **Oracle outputs dimensional scores ONLY** (0-10 per dimension + reasoning)
- âœ… **Tier classification in postfilters** (enables flexible thresholds without retraining)
- âœ… **Harmonized prompt structure** (scope â†’ gatekeepers â†’ article â†’ dimensions)
- âœ… **Inline filters for every dimension** (fast model compatibility)

**Result**: Clean separation of concerns - oracle scores, postfilter classifies. Change tier thresholds without re-labeling training data.

### âœ… Training Pipeline Complete

- **Data preparation**: `training/prepare_data.py` with stratified splitting (tier or score-bin based)
- **Data validation**: `training/validate_training_data.py` with comprehensive quality checks
- **Deduplication**: `training/deduplicate_training_data.py` for cross-split duplicate removal
- **Validation reports**: Auto-generated summaries saved to filter directories

### âœ… Development Tools & Agents

- **Filter Development Guide Agent**: End-to-end lifecycle guidance (9 phases: planning â†’ deployment)
- **Filter Harmonizer Agent**: Automated consistency checking and validation
- **Batch Scoring**: Generic `ground_truth.batch_scorer` supporting all filter packages
- **Dataset Profiling**: Master dataset with 402K articles (Oct-Nov 2025)

### ðŸš§ Next Steps

- **Train remaining filters**: uplifting v4, investment-risk v4
- **Production deployment**: Batch processing pipeline for high-volume scoring

## Quick Start

### 1. Install Dependencies

```bash
cd C:\local_dev\llm-distillery

# Install required packages
pip install anthropic google-generativeai pyyaml
```

### 2. Set Up API Keys

Create `config/credentials/secrets.ini`:

```ini
[api_keys]
anthropic_api_key = sk-ant-your_key_here
gemini_api_key = AIza_your_key_here
gemini_billing_api_key = AIza_billing_key  # Optional: 150 RPM vs 2 RPM
```

**Important**: This file is git-ignored for security.

### 3. Score Training Data

Score 5K+ articles with oracle:

```bash
python -m ground_truth.batch_scorer \
  --filter filters/uplifting/v4 \
  --source datasets/raw/master_dataset.jsonl \
  --output-dir datasets/scored/uplifting_v4_training \
  --llm gemini-flash \
  --target-count 5000 \
  --batch-size 100
```

**Process**: Stream articles â†’ Prefilter â†’ Oracle scores â†’ Save to batches

### 4. Prepare Training Data

Split scored data into train/val/test sets with stratification:

```bash
python training/prepare_data.py \
  --filter filters/uplifting/v4 \
  --data-source datasets/scored/uplifting_v4_training \
  --output-dir datasets/training/uplifting_v4
```

**Output**: train.jsonl (80%), val.jsonl (10%), test.jsonl (10%) with stratified sampling

### 5. Validate Training Data

Run comprehensive quality checks:

```bash
# Full validation with detailed report
python training/validate_training_data.py \
  --data-dir datasets/training/uplifting_v4 \
  --filter filters/uplifting/v4

# If duplicates found, deduplicate
python training/deduplicate_training_data.py datasets/training/uplifting_v4

# Generate summary report for filter documentation
python scripts/validation/generate_validation_summary.py \
  --data-dir datasets/training/uplifting_v4 \
  --filter-name uplifting \
  --version v4 \
  --output filters/uplifting/v4/TRAINING_DATA_VALIDATION.md
```

**Checks performed**:
- Structural integrity (required fields, ID uniqueness, label array length)
- Data distribution (train/val/test splits at 80/10/10)
- Label quality (score range [0-10], no NaN values, sufficient variance)
- Content quality (non-empty titles/content, reasonable lengths)
- Consistency (dimension names match across splits and config)
- Score distributions per dimension

### 6. Train a Model (Coming Soon)

Fine-tune Qwen2.5-7B on prepared dataset:

```bash
python -m training.train \
  --filter filters/uplifting/v4 \
  --data-dir datasets/training/uplifting_v4 \
  --output-dir models/uplifting_v4 \
  --base-model unsloth/Qwen2.5-7B-Instruct \
  --epochs 3 \
  --batch-size 4
```

**Requirements**: 16GB+ GPU (RTX 4090, A100), ~2-4 hours training time

## Documentation

- **[Full Documentation](docs/README.md)** - Complete docs index
- **[Architecture](docs/ARCHITECTURE.md)** - System design and oracle output discipline
- **[System Overview](docs/SYSTEM_OVERVIEW.md)** - Current state and datasets
- **[Filter Development Guide](docs/agents/README.md)** - 9-phase filter development lifecycle
- **[Repository Structure](docs/REPOSITORY_STRUCTURE.md)** - Directory organization
- **[Decisions Log](docs/DECISIONS.md)** - Strategic decisions and rationale

## Architecture

### Oracle â†’ Student Model Knowledge Distillation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GROUND TRUTH GENERATION                   â”‚
â”‚                                                              â”‚
â”‚  Raw Articles  â†’  Oracle (Gemini Flash)  â†’  Labeled Dataset â”‚
â”‚  (5K+ samples)    8 dimensional scores      (JSONL)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA PREPARATION & VALIDATION              â”‚
â”‚                                                              â”‚
â”‚  Scored Data  â†’  Stratified Split  â†’  Quality Validation    â”‚
â”‚  (batches)       (80/10/10)           (dedupe, checks)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       KNOWLEDGE DISTILLATION                 â”‚
â”‚                                                              â”‚
â”‚  Training Data  â†’  Student Model  â†’  Trained Classifier     â”‚
â”‚  (validated)       (Qwen2.5-7B)      (90%+ MAE â‰¤1.5)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DEPLOYMENT                              â”‚
â”‚                                                              â”‚
â”‚  Article  â†’  [Prefilter]  â†’  [Model]  â†’  [Postfilter]      â”‚
â”‚  (input)     <5ms Python      20-50ms      <10ms Python     â”‚
â”‚              rule-based        Qwen2.5     tier classify    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Runtime Inference Pipeline

Once trained, the deployed system processes articles through **3 stages**:

```
Article â†’ [Prefilter] â†’ [Student Model] â†’ [Postfilter] â†’ Tier + Scores
           <5ms          20-50ms            <10ms
           (Python)      (Qwen2.5-7B)       (Python)
```

**Stage 1: Prefilter** (Fast, Rule-Based)
- Blocks obvious out-of-scope content
- ~5ms per article
- Target: <10% false negative rate

**Stage 2: Student Model** (LLM Inference)
- Scores 8 dimensions (0-10 each)
- Uses fine-tuned Qwen2.5-7B (local)
- 20-50ms per article

**Stage 3: Postfilter** (Tier Classification)
- Maps dimensional scores â†’ tier
- Applies gatekeeper rules from config.yaml
- Flexible thresholds without retraining
- <10ms per article

**Key Benefit**: Change tier definitions by updating config only, no model retraining needed.

## Project Structure

```
llm-distillery/
â”œâ”€â”€ filters/                    # Versioned filter packages
â”‚   â”œâ”€â”€ uplifting/v4/          # Uplifting content filter
â”‚   â”œâ”€â”€ sustainability_tech_innovation/v2/  # Sustainability tech
â”‚   â”œâ”€â”€ investment-risk/v4/    # Investment risk signals
â”‚   â””â”€â”€ README.md              # Filter development guide
â”‚
â”œâ”€â”€ ground_truth/              # Oracle scoring pipeline
â”‚   â”œâ”€â”€ batch_scorer.py        # Universal scoring engine
â”‚   â”œâ”€â”€ llm_client.py         # Gemini/Claude API clients
â”‚   â””â”€â”€ prefilter_runner.py   # Prefilter execution
â”‚
â”œâ”€â”€ training/                  # Model training pipeline
â”‚   â”œâ”€â”€ prepare_data.py       # Stratified train/val/test splits
â”‚   â”œâ”€â”€ validate_training_data.py  # Quality validation
â”‚   â”œâ”€â”€ deduplicate_training_data.py  # Cross-split deduplication
â”‚   â””â”€â”€ train.py              # Model fine-tuning (planned)
â”‚
â”œâ”€â”€ datasets/                  # Generated datasets
â”‚   â”œâ”€â”€ raw/                  # Raw article collections
â”‚   â”œâ”€â”€ scored/               # Oracle-scored batches
â”‚   â””â”€â”€ training/             # Prepared train/val/test splits
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ agents/               # Development guide agents
â”‚   â”œâ”€â”€ decisions/            # Architecture decision records
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # System architecture
â”‚   â””â”€â”€ SYSTEM_OVERVIEW.md    # Current state & datasets
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts (organized by phase)
â”‚   â”œâ”€â”€ validation/           # Phase 3-5: Validation utilities
â”‚   â”œâ”€â”€ training/             # Phase 6-7: Training utilities
â”‚   â”œâ”€â”€ oracle/               # Phase 3: Oracle calibration
â”‚   â”œâ”€â”€ deployment/           # Phase 9: Model deployment
â”‚   â””â”€â”€ dataset/              # General dataset utilities
â”‚
â””â”€â”€ config/                    # Configuration
    â””â”€â”€ credentials/          # API keys (git-ignored)
```

## Filter Development Workflow

See [Filter Development Guide](docs/agents/README.md) for the complete 9-phase workflow:

1. **Planning** - Define dimensions, tiers, gatekeepers
2. **Architecture** - Harmonize prompt structure, inline filters
3. **Validation** - Oracle calibration on sample articles
4. **Prefilter** - Test false negative/positive rates
5. **Training Data** - Score 5K+ articles, validate quality âœ… (current)
6. **Training** - Fine-tune student model
7. **Testing** - Benchmark vs oracle, integration tests
8. **Documentation** - Complete all reports and guides
9. **Deployment** - Production release with monitoring

## Cost Analysis

### Ground Truth Generation (One-time per filter)
- **5K articles** Ã— **$0.001/article** (Gemini Flash)
- **Total**: ~$5-10 per filter

### Local Model Inference (Ongoing)
- **Cost**: $0 (runs locally on GPU)
- **Speed**: 20-50ms per article
- **Accuracy**: 90-95% agreement with oracle (MAE â‰¤1.5)

### ROI Calculation
If processing **4,000 articles/day**:
- **Oracle API**: 4,000 Ã— $0.001 Ã— 365 = **$1,460/year**
- **Local Model**: Training cost $10 + hosting ~$0 = **$10/year**
- **Savings**: **$1,450/year per filter** (99% cost reduction)

## Roadmap

### âœ… Phase 1: Ground Truth Generation (Complete)
- [x] Harmonized filter architecture
- [x] Generic batch scorer
- [x] Oracle calibration (Flash vs Pro)
- [x] Prefilter validation
- [x] Training data collection (5K+ per filter)
- [x] Data validation pipeline

### ðŸš§ Phase 2: Model Training (Current)
- [x] Data preparation with stratification
- [x] Training data validation
- [ ] Qwen2.5-7B fine-tuning script
- [ ] Training monitoring & checkpointing
- [ ] Model evaluation framework

### ðŸ“ Phase 3: Deployment (Planned)
- [ ] Inference server (prefilter + model + postfilter)
- [ ] Batch processing pipeline
- [ ] Production monitoring
- [ ] Model registry & versioning

## Contributing

This is a personal research project. Contributions and suggestions welcome via issues.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- Built for the Content Aggregator project
- Oracle scoring powered by Google Gemini Flash
- Student models based on Qwen2.5-7B-Instruct
- Architecture inspired by knowledge distillation research
