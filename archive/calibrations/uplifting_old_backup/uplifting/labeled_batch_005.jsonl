{"id":"science_arxiv_cs_6ec27ad929ec","title":"Chimera: Compositional Image Generation using Part","content":"arXiv:2510.18083v1 Announce Type: new Abstract: Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18083","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.125933","language":"en","tags":["research","preprints","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":174,"author":"Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral","raw_content_length":1217,"priority":7,"update_frequency":1,"reading_time_minutes":0.87,"robust_parsing_used":true,"entities":{"organizations":["Chimera"],"persons":["Announce Type"],"locations":[],"monetary":[]},"char_count":1216,"language_detected":"en","key_concepts":{"key_phrases":["Chimera","Compositional Image Generation","Part","arXiv251018083v1 Announce Type","new Abstract","Personalized image generative models","images","text","a single image","explicit control"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Chimera":3.0,"Compositional Image Generation":2.0,"Part":2.0,"arXiv251018083v1 Announce Type":1.0,"new Abstract":1.0,"Personalized image generative models":1.0,"images":1.0,"text":1.0,"a single image":1.0,"explicit control":1.0}},"age_hours":9.393017543055555,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"839196d8c86264f988de22ee30a294c5","title_md5":"be1e5eec5bfc96741541bb29005721ec","url_normalized":"1d9d8b30e5c25a31aeb28ba705345da0","minhash_signature":["9837667","3421805","14366","8077882","12890366","8408238","4333348","2181150","2753670","8724351","5968903","3207698","8506405","1166689","4983052","4356152","8894202","2890877","20397664","8708371","1196901","88593","27204064","3483270","8492544","13340951","4070773","13763274","29576415","3101601","4713903","587492","5807442","17637209","1090859","2422815","5241720","5605276","309467","17515630","4708391","3329444","9869035","1785966","12507014","5244114","1691750","4152219","13808451","4849158","190238","843096","10589055","7571683","136855","1909511","226173","6045037","12978240","1151625","4677246","7144066","1936","4050641","7601","3789593","21803903","4355909","7564697","3378698","19680712","13264497","15056664","3985712","7951082","360466","3542338","814536","5530344","6351248","7874076","1969868","4401012","20005884","442394","3087819","4147746","260590","1442738","8661640","7521029","2175767","19499623","436779","27555526","5560495","631649","479007","24302950","5743954","4227644","22172094","3331841","2458004","1405939","3344019","2193094","145406","40761208","669879","4034866","5937854","3010402","76161","8717165","13721683","926978","6972897","4846284","2936898","14121665","373488","952385","23261548","15152023","7375688","15147703","5198470"],"title_minhash":["42893312","252650981","60062056","29398940","343093616","588050683","60256810","17819323","45571546","134695259","6171369","3207698","8506405","1166689","16595777","95952243","68468203","18326172","82464397","8708371","95203104","9954399","109241454","11641973","244396979","296307002","33024378","17179148","29576415","19963369","53969032","52498760","5807442","42644042","84142666","15811535","142168664","111340586","291825547","61769993","142597498","39707560","278028673","312711743","107005966","10914927","91788617","28538266","51574523","33350054","98535417","843096","168612740","171085764","140606859","1914246","226173","85749854","74515183","46610253","78238733","31752063","77937664","94226683","379021936","362179333","324755198","42470960","63628992","24036355","19680712","235345887","118008268","61521131","82943311","108570676","41548460","61666238","256232859","59760279","10619331","396490980","172950305","66438777","31328343","26465583","143897938","15301898","27655721","11879887","66873785","17372915","19499623","86205232","304512338","25322995","33770585","134421378","200885761","147625420","214954921","82154835","87485699","109307212","185597596","152851793","94978857","35171132","55138371","137355306","25572248","121951088","21965285","65819346","141642552","13721683","41580991","74139433","93577921","77499487","31571808","79678431","75794758","57589434","348746819","7375688","55062599","180679399"],"combined_hash":"53a0af4e80361a29e3fb8731463ad407"},"sentiment_score":5.0,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.7327,"joy":0.012,"surprise":0.0211,"sadness":0.0156,"fear":0.078,"anger":0.0668,"disgust":0.0739},"emotion_method":"local"},"uplifting_analysis":{"content_type":"solutions_story","agency":7,"progress":8,"collective_benefit":8,"connection":6,"innovation":9,"justice":3,"resilience":4,"wonder":7,"reasoning":"The article describes the development of a new AI model, Chimera, that allows for more precise and controlled image generation by combining parts from different source images. This innovation enables users to create novel objects and images according to textual instructions, potentially democratizing creative processes and expanding access to personalized image generation. The model is trained on a large dataset of semantic atoms, and its performance is evaluated using a new metric, PartEval, demonstrating a commitment to improving the accuracy and quality of image generation.","key_markers":["image generation","AI model"],"dimensions":{"agency":7,"progress":8,"collective_benefit":8,"connection":6,"innovation":9,"justice":3,"resilience":4,"wonder":7},"overall_uplift_score":7.41,"tier":"impact","analyzed_at":"2025-10-28T18:58:44.309177Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_79e044aa158d","title":"RL-Driven Security-Aware Resource Allocation Framework for UAV","content":"arXiv:2510.18084v1 Announce Type: new Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access Networks (O-RAN) enhances communication in disaster management and Search and Rescue (SAR) operations by ensuring connectivity when infrastructure fails. However, SAR scenarios demand stringent security and low-latency communication, as delays or breaches can compromise mission success. While UAVs serve as mobile relays, they introduce challenges in energy consumption and resource management, necessitating intelligent allocation strategies. Existing UAV-assisted O-RAN approaches often overlook the joint optimization of security, latency, and energy efficiency in dynamic environments. This paper proposes a novel Reinforcement Learning (RL)-based framework for dynamic resource allocation in UAV relays, explicitly addressing these trade-offs. Our approach formulates an optimization problem that integrates security-aware resource allocation, latency minimization, and energy efficiency, which is solved using RL. Unlike heuristic or static methods, our framework adapts in real-time to network dynamics, ensuring robust communication. Simulations demonstrate superior performance compared to heuristic baselines, achieving enhanced security and energy efficiency while maintaining ultra-low latency in SAR scenarios.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18084","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.126367","language":"en","tags":["research","preprints","computer-science","csai","cscr","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":167,"author":"Zaineh Abughazzah, Emna Baccour, Loay Ismail, Amr Mohamed, Mounir Hamdi","raw_content_length":1347,"priority":7,"update_frequency":1,"reading_time_minutes":0.835,"robust_parsing_used":true,"entities":{"organizations":["Unmanned Aerial Vehicles","Search and Rescue (SAR","RL-Driven Security-Aware Resource Allocation Framework","UAVs","UAV","Open Radio Access Networks","Reinforcement Learning"],"persons":[],"locations":[],"monetary":[]},"char_count":1346,"language_detected":"en","key_concepts":{"key_phrases":["RL-Driven Security-Aware Resource Allocation Framework","UAV","UAVs","arXiv251018084v1 Announce Type","new Abstract","The integration","Unmanned Aerial Vehicles","Open Radio Access Networks","O-RAN","communication"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"RL-Driven Security-Aware Resource Allocation Framework":2.0,"UAV":2.0,"UAVs":2.0,"arXiv251018084v1 Announce Type":1.0,"new Abstract":1.0,"The integration":1.0,"Unmanned Aerial Vehicles":1.0,"Open Radio Access Networks":1.0,"O-RAN":1.0,"communication":1.0}},"age_hours":9.393037898611112,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"ed3f2f5471aeaa2a98c6bc7217fe7905","title_md5":"32234cc96ea987e11fc97a6bca9508c2","url_normalized":"9ab45fc92cfc15cdc5653106b68772de","minhash_signature":["9837667","10475553","14366","2691314","6697520","19580689","8307069","11040509","1062486","10835656","9991314","7332789","2343024","1166689","953500","2144153","4551842","4282388","2213100","5224575","1196901","88593","1386815","2113985","3593204","14494170","4070773","597785","354593","8720670","2169913","16000752","5807442","19349603","1090859","1256664","16026051","6836305","309467","6977948","15124641","3329444","6374721","1785966","10502397","22056152","646834","31283371","1132398","16941830","9116743","843096","5050670","4194521","136855","5446141","2780349","7750562","3605868","1151625","2318646","16671966","14326557","2997998","7601","5848190","10287772","2551573","7564697","3378698","4258427","827165","13012937","2758499","8191742","990","10343841","4232416","12221818","721726","5332671","1969868","2296366","10833656","1753692","2319043","4602115","260590","6103206","8318141","343266","2175767","9012833","786109","5979754","1881726","1608076","479007","23024916","5834464","5622147","10009000","3331841","2458004","1405939","3344019","2193094","4666992","1815488","6841584","4034866","120729","3010402","76161","8717165","1943155","873230","13762129","1592955","2957551","5479488","4372651","952385","2307778","8174376","11981119","9098556","5084848"],"title_minhash":["120896502","128115012","4229889","135265314","6697520","273684737","82901546","22639287","151785591","158584001","44689149","70138921","33479172","13957442","154087977","5696696","108783736","4282388","20397664","5224575","8008058","9954399","194473295","2113985","44720605","114318686","117647569","55808072","109523414","8720670","3753653","71213894","216379599","34963712","63886098","171142481","84633014","9182311","29069047","116034679","89809589","21571789","2661647","6136579","75692856","44457988","16347790","70975080","208914320","46572762","47935054","279156700","22471672","56267700","31305931","7359241","207817948","20648376","90792398","13305582","11267373","35379242","38275179","39264249","184871727","15998261","240560491","3973728","32471428","50311971","17995224","83788643","101537581","19721446","20588532","38585523","77147920","6134367","69656094","363271011","275619882","309445859","139415915","10833656","15871323","26465583","74075798","30823812","19829501","77792551","111165131","8766052","19499623","83084836","13402462","1881726","130545698","6217644","117868583","35992185","25632859","54578859","14554737","79748302","82960973","131246310","30081848","89860963","118933817","44001","27753175","344972746","49854310","29581038","27271784","13031838","83431805","13762129","168940944","21280434","5479488","34239847","45462023","128771503","164033760","95832515","9098556","77597240"],"combined_hash":"f4c20138d3bed8a16ce333e2b6897978"},"sentiment_score":7.383500000000001,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4767,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8389,"joy":0.0047,"surprise":0.0141,"sadness":0.0182,"fear":0.0925,"anger":0.0228,"disgust":0.0087},"emotion_method":"local"},"uplifting_analysis":{"content_type":"solutions_story","agency":8,"progress":7,"collective_benefit":9,"connection":6,"innovation":8,"justice":0,"resilience":7,"wonder":0,"reasoning":"The article describes a new AI-driven framework for UAVs that improves communication during disaster management and search and rescue operations. This framework enhances security, reduces latency, and improves energy efficiency, leading to better outcomes in critical situations. The system adapts in real-time to network dynamics, ensuring robust communication and demonstrating superior performance compared to existing methods.","key_markers":["disaster management","search and rescue"],"dimensions":{"agency":8,"progress":7,"collective_benefit":9,"connection":6,"innovation":8,"justice":0,"resilience":7,"wonder":0},"overall_uplift_score":7.25,"tier":"impact","analyzed_at":"2025-10-28T18:58:45.810360Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_5415bbf5993f","title":"Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery","content":"arXiv:2510.18256v1 Announce Type: new Abstract: 3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18256","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.159673","language":"en","tags":["research","preprints","computer-science","cscv","csai","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":231,"author":"Xiang Zhang, Suping Wu, Weibin Qiu, Zhaocheng Jin, Sheng Yang","raw_content_length":1626,"priority":7,"update_frequency":1,"reading_time_minutes":1.155,"robust_parsing_used":true,"entities":{"organizations":["Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery arXiv:2510.18256v1"],"persons":[],"locations":[],"monetary":[]},"char_count":1625,"language_detected":"en","key_concepts":{"key_phrases":["Hyperbolic Space Learning Method","Temporal Motion Priors","Human Mesh Recovery","3D human meshes","arXiv251018256v1 Announce Type","new Abstract","a natural hierarchical structure","torso-limbs-fingers","mesh features","Euclidean space"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Hyperbolic Space Learning Method":2.0,"Temporal Motion Priors":2.0,"Human Mesh Recovery":2.0,"3D human meshes":2.0,"arXiv251018256v1 Announce Type":1.0,"new Abstract":1.0,"a natural hierarchical structure":1.0,"torso-limbs-fingers":1.0,"mesh features":1.0,"Euclidean space":1.0}},"age_hours":9.39465411138889,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"959fe47ab133c6dbffdac883b4bbc30d","title_md5":"d4ddbfc09f47502a33f26d617b42f81b","url_normalized":"d2b053a9befa5d42669cbd1904c03db5","minhash_signature":["9837667","10475553","14366","8077882","1635470","4631126","14786822","187877","2645416","1989266","3134393","3207698","2343024","1166689","15744853","4356152","4551842","4282388","23111531","113500","6058284","88593","1386815","2113985","16164381","14494170","4070773","10526923","21652241","4572792","14164804","1613769","5807442","582111","1053967","880599","10844376","5474159","309467","6977948","20427502","17035396","33852562","1785966","12507014","5244114","1691750","2416017","1132398","11619362","7563562","3222690","2871081","214699","9899859","6632445","226173","6045037","12978240","1151625","11267373","1440799","6372115","2997998","623882","18039960","3035295","2551573","1815715","5493975","4258427","827165","4549584","3985712","20588532","2595826","10923713","814536","10458408","7421795","5332671","4072191","3972917","1555049","1595927","18232490","16694763","12545075","5524105","8318141","7678043","2175767","16349222","1541951","12924892","5388347","631649","479007","11483721","713149","2876666","14172480","986263","5292934","1405939","3344019","2193094","145406","10621501","669879","9498389","12224548","2086119","14892483","18977785","5843300","873230","2726960","4846284","12207048","1131087","7895850","11219179","7407467","11988192","809487","9098556","33668"],"title_minhash":["42893312","55054725","14366","39082721","57666577","201740842","37917810","17819323","2965733","76243419","27785284","3207698","21585717","5950415","15744853","75053161","61346300","16264169","24092098","21767673","42548792","52186064","83501608","40747836","17372101","70883859","35879647","26792221","90905533","8720670","14164804","28723960","135690367","2423718","1053967","43428288","51437858","9182311","1925772","33858910","161790114","46312510","49136923","85803247","46213406","40813195","129456160","41358520","11618208","30873407","25266503","8355167","10820278","35380209","48009272","6632445","226173","151758212","32365264","48875666","27865386","27169898","204179042","4050641","129223356","88815761","161030559","8905379","8082044","71498094","19680712","15390952","11204675","3985712","41265082","17574547","269839","84454483","13987518","31241231","63364378","4072191","3972917","210333452","50440605","26426332","83841107","37264318","5524105","75709137","44546656","13743457","24349389","1541951","23757542","5388347","631649","7514544","37727624","29812689","5622147","79870237","18444329","5292934","26261115","3376316","84906638","85854287","122904129","74535621","18172923","49495959","2086119","44769960","69431874","49510724","41580991","13762129","32933298","82920941","35842031","34239847","45462023","17441585","67134068","41210887","278466583","74578893"],"combined_hash":"568f8195532528246b861e3f413b6c4f"},"sentiment_score":0.8939999999999998,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.8212,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.6623,"joy":0.0013,"surprise":0.0209,"sadness":0.0269,"fear":0.0173,"anger":0.1912,"disgust":0.08},"emotion_method":"local"},"uplifting_analysis":{"content_type":"environmental","agency":7,"progress":8,"collective_benefit":7,"connection":5,"innovation":9,"justice":0,"resilience":6,"wonder":6,"reasoning":"The article describes a new method for recovering 3D human meshes from videos using hyperbolic space learning. This innovation aims to improve the accuracy of human mesh reconstruction, potentially benefiting fields like motion capture, virtual reality, and human-computer interaction, which could lead to more accessible and realistic digital experiences for many people.","key_markers":["3D human mesh recovery","hyperbolic space learning"],"dimensions":{"agency":7,"progress":8,"collective_benefit":7,"connection":5,"innovation":9,"justice":0,"resilience":6,"wonder":6},"overall_uplift_score":6.8,"tier":"connection","analyzed_at":"2025-10-28T18:58:47.119874Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_9559b35a34d2","title":"When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution","content":"arXiv:2510.18270v1 Announce Type: new Abstract: Test suites in real-world projects are often large and achieve high code coverage, yet they remain insufficient for detecting all bugs. The abundance of unresolved issues in open-source project trackers highlights this gap. While regression tests are typically designed to ensure past functionality is preserved in the new version, they can also serve a complementary purpose: debugging the current version. Specifically, regression tests can (1) enhance the generation of reproduction tests for newly reported issues, and (2) validate that patches do not regress existing functionality. We present TestPrune, a fully automated technique that leverages issue tracker reports and strategically reuses regression tests for both bug reproduction and patch validation. A key contribution of TestPrune is its ability to automatically minimize the regression suite to a small, highly relevant subset of tests. Due to the predominance of LLM-based debugging techniques, this minimization is essential as large test suites exceed context limits, introduce noise, and inflate inference costs. TestPrune can be plugged into any agentic bug repair pipeline and orthogonally improve overall performance. As a proof of concept, we show that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction rate within the Otter framework and a 9.4% - 12.9% relative increase in issue resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench Verified benchmarks, capturing fixes that were correctly produced by agents but not submitted as final patches. Compared to the benefits, the cost overhead of using TestPrune is minimal, i.e., \\$0.02 and \\$0.05 per SWE-Bench instance, using GPT-4o and Claude-3.7-Sonnet models, respectively.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18270","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.163834","language":"en","tags":["research","preprints","computer-science","csse","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":260,"author":"Yang Chen, Toufique Ahmed, Reyhaneh Jabbarvand, Martin Hirzel","raw_content_length":1798,"priority":7,"update_frequency":1,"reading_time_minutes":1.3,"robust_parsing_used":true,"entities":{"organizations":["TestPrune"],"persons":[],"locations":[],"monetary":[]},"char_count":1795,"language_detected":"en","key_concepts":{"key_phrases":["Old","the Impact","Regression Tests","SWE Issue Resolution","regression tests","Announce Type","new Abstract","Test suites","real-world projects","high code coverage"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Old":2.0,"the Impact":2.0,"Regression Tests":2.0,"SWE Issue Resolution":2.0,"regression tests":2.0,"Announce Type":1.0,"new Abstract":1.0,"Test suites":1.0,"real-world projects":1.0,"high code coverage":1.0}},"age_hours":9.39485185888889,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"f95ceaa63d87e5f2ca9d9d52ce606306","title_md5":"9ff04d672848de4e96fd7379716a7955","url_normalized":"92bc853ab8ed7db6c000f4dd2db4beda","minhash_signature":["2913784","21419052","2507878","9501201","1635470","5329195","5451088","2181150","2753670","10835656","9991314","10764267","2343024","1166689","953500","5696696","11186338","4282388","178409","1852662","1196901","5115231","1386815","2113985","6579678","14494170","4070773","9160874","6998512","8720670","522500","15963955","5807442","582111","1053967","5205139","4868133","6218001","309467","174348","3082913","3329444","26590676","1785966","20548697","5244114","3043650","31283371","1132398","6774644","11104544","843096","6725716","7574389","136855","5146736","7015836","4479771","7700446","1151625","4677246","1440799","1936","4886012","1886331","8627931","13636731","4355909","1815715","9385305","473069","827165","13012937","4528274","4991972","3487483","5240481","2143585","5530344","24531359","1246369","1544236","7379873","20005884","442394","6694534","2225166","9222483","134600","8661640","7678043","2175767","9012833","16277282","13402462","2999489","1608076","479007","14860642","713149","583082","3028635","7665946","5758998","1405939","3344019","2193094","2447857","4355487","6841584","968633","7086138","3010402","4438756","23567886","1943155","873230","1144143","4846284","2957551","6057172","4372651","1129826","1072927","11770568","809487","9098556","33668"],"title_minhash":["24942206","54287048","14366","9501201","86242848","162442357","82901546","380837027","2965733","11743625","29949824","5250560","2343024","126523630","16595777","213667269","71065416","133263627","319640524","15931653","8008058","9954399","51852650","71002335","296638374","232427369","125204312","34131197","143728637","67802430","29272303","76506381","152446476","20375603","37667665","6548540","28724667","9182311","120569569","34868937","152294126","174590963","42674028","21037209","102621355","36428735","3043650","101939165","29621049","32145703","25266503","197320422","33431511","10539494","38860551","5146736","7015836","85749854","32790638","4797292","35002035","173398000","56514723","18669793","73765893","34548017","2940560","42470960","13478095","71498094","19680712","77934695","27502735","29342391","22643791","4797850","77147920","21510571","156773572","59760279","5332671","3546788","16231830","162834571","98062811","8605786","74150130","37264318","19749232","72879694","7521029","70772993","19499623","49518307","49453443","17822880","18969841","43573138","117868583","35992185","20165302","28182962","3331841","16291765","6760750","105017105","10483071","47418031","6462857","27769953","19995117","96104821","2086119","178023885","43774221","77127911","19613706","125500163","1592955","81768654","31571808","34239847","52264841","47406777","31592508","243980632","9098556","45227134"],"combined_hash":"4f46557dc18e667fac5b45ad6f0b44ff"},"sentiment_score":6.909,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.3818,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8168,"joy":0.0036,"surprise":0.0125,"sadness":0.0126,"fear":0.0552,"anger":0.0445,"disgust":0.0549},"emotion_method":"local"},"uplifting_analysis":{"content_type":"solutions_story","agency":8,"progress":9,"collective_benefit":8,"connection":6,"innovation":9,"justice":7,"resilience":7,"wonder":6,"reasoning":"The article describes an automated technique, TestPrune, that improves bug reproduction and issue resolution in software projects by strategically reusing regression tests. This leads to more reliable software, benefiting a wide range of users by improving software quality and reducing errors, and the technique is designed to be integrated into existing bug repair pipelines, enhancing overall performance.","key_markers":["bug resolution","software reliability"],"dimensions":{"agency":8,"progress":9,"collective_benefit":8,"connection":6,"innovation":9,"justice":7,"resilience":7,"wonder":6},"overall_uplift_score":7.91,"tier":"impact","analyzed_at":"2025-10-28T18:58:48.548201Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_18c9a6e09069","title":"BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks","content":"arXiv:2510.18288v1 Announce Type: new Abstract: Braille plays a vital role in education and information accessibility for visually impaired individuals. However, Braille information processing faces challenges such as data scarcity and ambiguities in mixed-text contexts. We construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with mathematical formulas to support diverse Braille domain research, and propose a syntax tree-based augmentation method tailored for Braille data. To address the underperformance of traditional fine-tuning methods in Braille-related tasks, we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the learning difficulty of Braille contextual features. BrailleLLM employs BKFT via instruction tuning to achieve unified Braille translation, formula-to-Braille conversion, and mixed-text translation. Experiments demonstrate that BKFT achieves significant performance improvements over conventional fine-tuning in Braille translation scenarios. Our open-sourced datasets and methodologies establish a foundation for low-resource multilingual Braille research.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18288","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.168785","language":"en","tags":["cscl","research","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":133,"author":"Tianyuan Huang, Zepeng Zhu, Hangdi Xing, Zirui Shao, Zhi Yu, Chaoxiong Yang, Jiaxian He, Xiaozhong Liu, Jiajun Bu","raw_content_length":1117,"priority":7,"update_frequency":1,"reading_time_minutes":0.665,"robust_parsing_used":true,"entities":{"organizations":[],"persons":[],"locations":["Braille"],"monetary":[]},"char_count":1116,"language_detected":"en","key_concepts":{"key_phrases":["BrailleLLM","Braille Instruction Tuning","Large Language Models","Braille Domain Tasks","arXiv251018288v1 Announce Type","new Abstract","Braille","a vital role","education","information accessibility"],"filter_categories":{"ai_ml":["BrailleLLM","Large Language Models"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"BrailleLLM":2.0,"Braille Instruction Tuning":2.0,"Large Language Models":2.0,"Braille Domain Tasks":2.0,"arXiv251018288v1 Announce Type":1.0,"new Abstract":1.0,"Braille":1.0,"a vital role":1.0,"education":1.0,"information accessibility":1.0}},"age_hours":9.395090474166668,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"fca832b924730d1fc93a4eb003b31eb4","title_md5":"90e579a9710d3582d35733cd6cc4a222","url_normalized":"640d83ab1f7d4d6e451da9cc4e130103","minhash_signature":["9837667","10475553","14366","292436","13624839","19580689","5451088","2181150","2753670","11080308","9991314","7332789","2343024","1166689","2901","2834307","4551842","3742938","4032655","53635610","225209","88593","1386815","3483270","6817298","14494170","4070773","6529113","8348822","8720670","14164804","3486836","2518573","2423718","1090859","23886555","10844376","6836305","296674","4529533","3082913","3329444","6374721","1785966","20548697","24601664","1691750","23451935","1132398","11619362","40025517","8181152","12018203","7574389","136855","1909511","7015836","16699325","3516270","8043868","4334793","18372664","6578137","2997998","7601","3789593","9862989","1563117","7564697","4625726","1210516","15390952","13012937","4528274","612253","3487483","3542338","2433691","8848590","721726","301782","2330191","3972917","8572837","11399451","379140","3156757","2033222","3320655","8318141","7521029","2175767","8261531","1541951","8563580","5560495","1608076","479007","11483721","713149","1534879","551087","5119106","5883140","1405939","3344019","2193094","145406","13734988","44001","4034866","120729","1550051","4438756","756656","28227271","926978","1949350","4846284","2957551","15165020","3100555","1129826","17441585","23467574","8478807","9098556","5084848"],"title_minhash":["9837667","10475553","64336003","112903883","65076605","157220167","95059972","17862952","41359210","11743625","11895153","22661111","60405795","53989586","16595777","2834307","11186338","10786263","20565192","87305006","1868191","4962795","459618","25530671","6817298","70795149","7507083","19091918","40491931","201850319","14164804","65027209","5807442","85384897","3102556","137264030","51437858","6836305","20219880","18336194","67726702","241049971","37328139","1785966","35475695","22056152","119679498","32330908","47211822","145856767","2447249","8181152","22685869","43544270","74658870","18435302","14906348","131366753","123693004","102627154","4677246","95068232","11453037","9364271","307061980","88815761","82728934","4355909","21731071","9418937","19680712","116331293","114957399","19721446","29780025","3487483","69290267","280832374","13987518","31636475","159499760","2330191","79330819","66402872","23577047","5335593","83841107","23754247","70492635","8661640","74938439","76711016","28682820","72380012","126459200","99639946","156259243","479007","117345612","92957535","53674721","39855598","197592545","9029590","33098540","58208387","51478744","17178449","40761208","66078171","27753175","33950793","229217253","19559198","90010294","49510724","120732","13762129","48657907","25820802","6057172","22429195","1129826","140218992","70385328","7375688","23644728","33668"],"combined_hash":"7b4fbaf695f2542202ec3af19e8d59ed"},"sentiment_score":8.6755,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.7351,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.9414,"joy":0.0081,"surprise":0.0166,"sadness":0.0066,"fear":0.0179,"anger":0.006,"disgust":0.0034},"emotion_method":"local"},"uplifting_analysis":{"content_type":"community_building","agency":9,"progress":9,"collective_benefit":10,"connection":7,"innovation":9,"justice":8,"resilience":8,"wonder":8,"reasoning":"This research directly addresses the needs of visually impaired individuals by improving access to information and education. The development of BrailleLLM and the open-sourcing of datasets and methodologies provide a foundation for further research and development in low-resource multilingual Braille, promoting equity and inclusion.","key_markers":["accessibility","inclusion"],"dimensions":{"agency":9,"progress":9,"collective_benefit":10,"connection":7,"innovation":9,"justice":8,"resilience":8,"wonder":8},"overall_uplift_score":9.07,"tier":"impact","analyzed_at":"2025-10-28T18:58:49.916711Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_d72121480cac","title":"Enhancing Few","content":"arXiv:2510.18326v1 Announce Type: new Abstract: The increasing frequency of natural and human-induced disasters necessitates advanced visual recognition techniques capable of analyzing critical photographic data. With progress in artificial intelligence and resilient computational systems, rapid and accurate disaster classification has become crucial for efficient rescue operations. However, visual recognition in disaster contexts faces significant challenges due to limited and diverse data from the difficulties in collecting and curating comprehensive, high-quality disaster imagery. Few-Shot Learning (FSL) provides a promising approach to data scarcity, yet current FSL research mainly relies on generic benchmark datasets lacking remote-sensing disaster imagery, limiting its practical effectiveness. Moreover, disaster images exhibit high intra-class variation and inter-class similarity, hindering the performance of conventional metric-based FSL methods. To address these issues, this paper introduces the Attention-based Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), which linearly combines the Bhattacharyya coefficient and Hellinger distances to compare and aggregate feature probability distributions for robust prototype formation. The Bhattacharyya coefficient serves as a contrastive margin that enhances inter-class separability, while the Hellinger distance regularizes same-class alignment. This framework parallels contrastive learning but operates over probability distributions rather than embedded feature points. Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss is proposed as a distributional counterpart to cosine similarity loss, used jointly with categorical cross-entropy to significantly improve FSL performance. Experiments on four FSL benchmarks and two disaster image datasets demonstrate the superior effectiveness and generalization of ATTBHFA-Net compared to existing approaches.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18326","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.177435","language":"en","tags":["research","preprints","computer-science","cscv","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":233,"author":"Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong","raw_content_length":1963,"priority":7,"update_frequency":1,"reading_time_minutes":1.165,"robust_parsing_used":true,"entities":{"organizations":["Few-Shot Learning","FSL"],"persons":[],"locations":[],"monetary":[]},"char_count":1962,"language_detected":"en","key_concepts":{"key_phrases":["arXiv251018326v1 Announce Type","new Abstract","The increasing frequency","natural and human-induced disasters","advanced visual recognition techniques","critical photographic data","progress","artificial intelligence","computational systems","rapid and accurate disaster classification"],"filter_categories":{"ai_ml":["artificial intelligence"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"arXiv251018326v1 Announce Type":1.0,"new Abstract":1.0,"The increasing frequency":1.0,"natural and human-induced disasters":1.0,"advanced visual recognition techniques":1.0,"critical photographic data":1.0,"progress":1.0,"artificial intelligence":1.0,"computational systems":1.0,"rapid and accurate disaster classification":1.0}},"age_hours":9.3955071525,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"7953575b178adae43fbf7863f232faaf","title_md5":"7cd92c2c4d68264444fd0b401c0f4061","url_normalized":"4e02f4d22e2bfc235ac06392695ffbf0","minhash_signature":["24942206","10475553","14366","2691314","498956","4631126","19721531","2181150","2753670","4729967","9991314","7332789","2343024","1166689","953500","4356152","4551842","3604883","178409","2713066","225209","88593","433252","2113985","16164381","244614","4070773","5225880","5160931","4572792","4883147","11857065","5807442","5849576","1053967","16665534","5971886","6218001","296674","6977948","9515827","3329444","6374721","1350112","14447411","2463753","449615","14540495","1132398","11619362","20134288","843096","6725716","4194521","136855","1914246","7015836","6045037","4656097","1151625","2318646","7144066","6578137","4050641","7601","3789593","3599302","3312365","8082044","5493975","5786666","13189136","984943","3985712","7951082","2595826","5240481","814536","3968445","721726","301782","1544236","7379873","1712457","442394","1240238","4602115","12545075","3320655","1755802","343266","2175767","9012833","6172193","5979754","2999489","1608076","479007","4713810","713149","583082","10538129","3331841","5758998","1405939","3344019","6419416","145406","1815488","44001","1243296","120729","3010402","9732890","3450231","1943155","873230","1144143","4846284","5085385","1131087","4372651","952385","7407467","14744595","809487","1742731","33668"],"title_minhash":["453747998","85077600","262476853","464014220","427360870","730692198","30731995","53868996","552166563","678592121","205703196","1003571768","958275782","79660562","16595777","601957212","283076412","198661410","490685750","223891640","265060075","510933249","262559654","94665471","1628613185","321422689","207946254","284242779","163288084","404694915","870813740","1269249183","946544992","691586291","260375825","86835335","501792253","576947198","408903155","101238272","569568678","186117132","102818059","322026648","1219287146","278512636","447402135","457362640","12067079","181089528","817182539","251973033","103377211","328741033","657587024","47073719","540713772","128863731","463844801","209654940","131338984","244353572","350669301","349388348","233650316","569819422","62753830","1183170552","69641278","252598443","396751353","605457802","199208683","996029547","122999031","89648287","759787980","479114056","400003786","59760279","386668747","441145765","1144327561","24131304","36180185","282661784","201682019","535899397","597376911","199450503","790643768","305974082","391688492","569047259","31924856","959508487","617329501","734736896","41554669","833499973","160443350","40533473","360387169","12692197","174401147","113874261","190660985","228530304","83754983","249633664","263362778","121951088","979445990","222183474","43598489","372458460","254698140","348753544","130456296","32625560","50714747","316250109","212641990","601525446","760715166","363656211","935252682","210900808"],"combined_hash":"d12e24368684cf86939781a291d0e725"},"sentiment_score":7.4695,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4939,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8083,"joy":0.0158,"surprise":0.026,"sadness":0.0209,"fear":0.1125,"anger":0.0099,"disgust":0.0066},"emotion_method":"local"},"uplifting_analysis":{"content_type":"environmental","agency":8,"progress":8,"collective_benefit":9,"connection":6,"innovation":9,"justice":0,"resilience":8,"wonder":6,"reasoning":"The paper introduces a new AI model (ATTBHFA-Net) designed to improve the accuracy of disaster classification from images, which is crucial for efficient rescue operations. This model addresses the challenge of limited and diverse data in disaster contexts by using a novel approach to feature aggregation, ultimately enhancing the ability to analyze critical photographic data and improve disaster response.","key_markers":["disaster classification","rescue operations"],"dimensions":{"agency":8,"progress":8,"collective_benefit":9,"connection":6,"innovation":9,"justice":0,"resilience":8,"wonder":6},"overall_uplift_score":7.84,"tier":"impact","analyzed_at":"2025-10-28T18:58:51.670970Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_03d149a70bb4","title":"Coverage","content":"arXiv:2510.18347v1 Announce Type: new Abstract: This article addresses collaborative 3D map reconstruction using multiple drones. Achieving high-quality reconstruction requires capturing images of keypoints within the target scene from diverse viewing angles, and coverage control offers an effective framework to meet this requirement. Meanwhile, recent advances in real-time 3D reconstruction algorithms make it possible to render an evolving map during flight, enabling immediate feedback to guide drone motion. Building on this, we present Coverage-Recon, a novel coordinated image sampling algorithm that integrates online map feedback to improve reconstruction quality on-the-fly. In Coverage-Recon, the coordinated motion of drones is governed by a Quadratic Programming (QP)-based angle-aware coverage controller, which ensures multi-viewpoint image capture while enforcing safety constraints. The captured images are processed in real time by the NeuralRecon algorithm to generate an evolving 3D mesh. Mesh changes across the scene are interpreted as indicators of reconstruction uncertainty and serve as feedback to update the importance index of the coverage control as the map evolves. The effectiveness of Coverage-Recon is validated through simulation and experiments, demonstrating both qualitatively and quantitatively that incorporating online map feedback yields more complete and accurate 3D reconstructions than conventional methods. Project page: https://htnk-lab.github.io/coverage-recon/","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18347","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.182486","language":"en","tags":["research","preprints","computer-science","eesssy","cssy","mathoc","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":195,"author":"Muhammad Hanif, Reiji Terunuma, Takumi Sumino, Kelvin Cheng, Takeshi Hatanaka","raw_content_length":1511,"priority":7,"update_frequency":1,"reading_time_minutes":0.975,"robust_parsing_used":true,"entities":{"organizations":["Coverage-Recon","NeuralRecon","a Quadratic Programming"],"persons":[],"locations":[],"monetary":[]},"char_count":1510,"language_detected":"en","key_concepts":{"key_phrases":["Coverage","arXiv251018347v1 Announce Type","new Abstract","This article","collaborative 3D map reconstruction","multiple drones","high-quality reconstruction","images","keypoints","the target scene"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"Coverage":2.0,"arXiv251018347v1 Announce Type":1.0,"new Abstract":1.0,"This article":1.0,"collaborative 3D map reconstruction":1.0,"multiple drones":1.0,"high-quality reconstruction":1.0,"images":1.0,"keypoints":1.0,"the target scene":1.0}},"age_hours":9.395750020555555,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"8bb52fca52cdf7c0918772f92325d4bc","title_md5":"9841bdc50c4226cb6ec5db76494249e6","url_normalized":"e6a861a946be171d75d56c6947944ad4","minhash_signature":["9837667","10475553","14366","11617828","1635470","5995015","4333348","2181150","2753670","4729967","3134393","3207698","2343024","656987","953500","4356152","14234784","3345807","2213100","8708371","1196901","88593","433252","2113985","16595181","3480440","4070773","10526923","8420710","3101601","2169913","15804152","5807442","2423718","1053967","2893102","16026051","6218001","309467","8649941","12167310","6176253","6374721","1785966","12507014","5244114","1691750","2416017","1132398","174016","4485723","843096","6725716","7574389","136855","2364710","226173","7750562","4710402","1151625","10643667","1311296","1936","4886012","7601","7689844","2940560","3973728","3843993","15825899","473069","13264497","11204675","4528274","5728012","2595826","3542338","4232416","7656379","721726","10619331","1544236","2296366","23052218","7372952","8605786","4147746","260590","3320655","8661640","7521029","2175767","5133030","436779","13402462","1881726","1608076","479007","15352116","6563146","583082","5380955","3751328","2458004","1405939","3344019","2193094","2946257","6462857","44001","5261392","120729","3010402","15851685","4857321","3298046","120732","1307835","4846284","2936898","12261163","9876597","1129826","363706","15152023","809487","3864915","5084848"],"title_minhash":["1590742319","163305779","336196390","406892654","170518213","752400869","179012528","1158377928","792487093","134695259","728405661","57866152","40944252","758331682","1189380598","229989350","400685016","2781267564","82464397","143750544","838796480","205551619","243539622","829413092","1142857291","77648847","204241718","845157082","423700747","576369272","2061998207","1441607808","5807442","85384897","786866475","118100605","1349152810","448984526","724876562","319304779","398804055","306633119","429916905","691789094","46213406","132610958","393692671","41358520","1621603427","790824418","581428398","74487567","702909094","233304501","839709305","560964046","182681612","379681563","1002697230","363250192","499996843","214142535","445894047","565956187","371291797","2098390614","362220396","8905379","869480171","192197679","51719197","654436452","668484705","326176720","913110528","1239065335","233413772","1413773844","15027250","910438723","726114488","443409858","509915701","210333452","52757854","130904914","244430078","76669091","1415574450","651456139","1720810252","219624751","1074568651","270109753","736168338","128706017","666712928","14034324","92924279","42655223","251962343","1337066788","421035532","408406691","190898233","303716309","291044413","546394016","517809098","149045109","220371495","988884067","559589991","1127094021","918617361","449758265","207248849","211647733","220890904","763259444","356750165","253783352","168271792","2549124698","348746819","1465094645","277315448","74578893"],"combined_hash":"3ccf94f0d045342d9aca7a9a4a5eb714"},"sentiment_score":7.383500000000001,"sentiment_category":"positive","sentiment_confidence":"medium","sentiment_method":"vader","sentiment_raw_score":0.4767,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.8871,"joy":0.0726,"surprise":0.0216,"sadness":0.003,"fear":0.0048,"anger":0.0082,"disgust":0.0027},"emotion_method":"local"},"uplifting_analysis":{"content_type":"environmental","agency":8,"progress":8,"collective_benefit":9,"connection":7,"innovation":9,"justice":0,"resilience":7,"wonder":6,"reasoning":"The article describes a novel algorithm, Coverage-Recon, that uses drones to collaboratively create 3D maps of environments. This technology improves the accuracy and completeness of 3D reconstructions by incorporating real-time feedback, leading to better environmental monitoring and potentially disaster response. The open access project page further enhances collective benefit.","key_markers":["environmental_monitoring","collaborative_mapping"],"dimensions":{"agency":8,"progress":8,"collective_benefit":9,"connection":7,"innovation":9,"justice":0,"resilience":7,"wonder":6},"overall_uplift_score":7.92,"tier":"impact","analyzed_at":"2025-10-28T18:58:53.168498Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_0e8c001d9401","title":"PGTT: Phase","content":"arXiv:2510.18348v1 Announce Type: new Abstract: State-of-the-art perceptive Reinforcement Learning controllers for legged robots either (i) impose oscillator or IK-based gait priors that constrain the action space, add bias to the policy optimization and reduce adaptability across robot morphologies, or (ii) operate \"blind\", which struggle to anticipate hind-leg terrain, and are brittle to noise. In this paper, we propose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL approach that overcomes these limitations by enforcing gait structure purely through reward shaping, thereby reducing inductive bias in policy learning compared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase as a cubic Hermite spline that adapts swing height to local heightmap statistics and adds a swing- phase contact penalty, while the policy acts directly in joint space supporting morphology-agnostic deployment. Trained in MuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and domain randomization, PGTT achieves the highest success under push disturbances (median +7.5% vs. the next best method) and on discrete obstacles (+9%), with comparable velocity tracking, and converging to an effective policy roughly 2x faster than strong end-to-end baselines. We validate PGTT on a Unitree Go2 using a real-time LiDAR elevation-to-heightmap pipeline, and we report preliminary results on ANYmal-C obtained with the same hyperparameters. These findings indicate that terrain-adaptive, phase-guided reward shaping is a simple and general mechanism for robust perceptive locomotion across platforms.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18348","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.182907","language":"en","tags":["research","preprints","computer-science","csai","cslg","csro","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":221,"author":"Alexandros Ntagkas, Chairi Kiourt, Konstantinos Chatzilygeroudis","raw_content_length":1640,"priority":7,"update_frequency":1,"reading_time_minutes":1.105,"robust_parsing_used":true,"entities":{"organizations":["Reinforcement Learning","Phase-Guided Terrain Traversal","MJX"],"persons":[],"locations":[],"monetary":[]},"char_count":1639,"language_detected":"en","key_concepts":{"key_phrases":["PGTT","Phase","arXiv251018348v1 Announce Type","new Abstract","the-art","legged robots","oscillator","IK-based gait priors","the action space","bias"],"filter_categories":{"ai_ml":["IK-based gait priors"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"PGTT":3.0,"Phase":2.0,"arXiv251018348v1 Announce Type":1.0,"new Abstract":1.0,"the-art":1.0,"legged robots":1.0,"oscillator":1.0,"IK-based gait priors":1.0,"the action space":1.0,"bias":1.0}},"age_hours":9.395771645277778,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"ba5c2b2ba88a432b8b0d7e9cfe3b591f","title_md5":"2df504de4a802545980adefdfc5f01bb","url_normalized":"9f1b69c506b647b7109de7207237bb67","minhash_signature":["9837667","8977646","14366","2691314","498956","1444560","4380964","187877","2753670","1929707","9991314","3207698","7668629","1166689","4400448","14833280","11186338","1352173","2213100","113500","1196901","88593","433252","3483270","16595181","14494170","4070773","5929356","8420710","8720670","522500","1613769","2093740","2423718","1053967","880599","17158482","5605276","309467","6977948","12167310","1203294","39838467","1785966","12507014","5244114","449615","6599673","1132398","10675237","5703177","843096","10820278","7574389","136855","1909511","226173","2884394","4430687","1151625","2318646","5351229","6558720","4050641","1886331","7689844","10173455","1563117","7564697","9385305","6267686","827165","984943","4528274","7951082","2595826","3542338","1458913","13987518","15257132","5332671","1969868","3972917","10918364","442394","2319043","4602115","2033222","3320655","8661640","7678043","2175767","1183835","436779","12924892","1881726","631649","479007","23024916","5834464","5622147","3028635","3663773","5251862","1405939","2000720","2193094","4666992","12007630","7559602","9498389","7086138","2086119","76161","756656","1943155","926978","1307835","4846284","2957551","6123858","4372651","952385","1331506","10047967","11374462","1742731","5198470"],"title_minhash":["135880636","100513848","483786488","418116318","437238288","1383110227","599869932","248806348","2753670","1201449078","717444470","90590156","941906896","60892523","939612383","77489830","480255425","394862903","1251381423","56392526","699664439","303648137","71641194","38143436","199116075","410846050","166972358","75995835","87920314","65984160","597112660","189866617","213836056","216998828","378246197","534788420","794833174","567031690","13292079","304539180","226409641","842436574","262836346","156327254","375381139","72123078","534845135","1257359454","584634518","534562426","699485607","1637986029","2313658050","225579129","330793868","58599815","396003263","301247875","165669310","45094459","1888282016","1253287124","33195870","470767465","4698640","1608239915","51544578","544725020","1600218526","191423969","22704879","15702958","43998752","339329123","144697420","414120403","752067855","121337822","126566186","543777855","276712161","39450393","90194461","172789077","173429406","510309678","53810938","124943283","18611933","349873412","37772398","393952056","857248877","293230573","33436299","491121564","442564074","241941432","617975852","155318743","99677409","248652257","162423359","255750870","273057688","212479731","23824645","601518004","1131600603","169472010","284403719","202764009","93865319","441045526","512845464","289175375","580302946","514117492","300143702","39260152","407016026","959434899","736395209","182515295","412596283","2246595905","1742731","287680065"],"combined_hash":"66cbe9d223c73375f83b7c6c33e368aa"},"sentiment_score":1.9379999999999997,"sentiment_category":"negative","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":-0.6124,"is_positive":false,"is_negative":true,"is_neutral":false,"raw_emotions":{"neutral":0.8501,"joy":0.012,"surprise":0.0256,"sadness":0.0104,"fear":0.0354,"anger":0.044,"disgust":0.0226},"emotion_method":"local"},"uplifting_analysis":{"content_type":"environmental","agency":8,"progress":8,"collective_benefit":7,"connection":3,"innovation":9,"justice":2,"resilience":6,"wonder":5,"reasoning":"The article describes a new reinforcement learning approach (PGTT) for legged robots that allows them to navigate complex terrains more effectively and robustly. This innovation has potential for applications in environmental monitoring, disaster response, and other areas where robots can assist humans in challenging environments. The system is validated on real-world robots, demonstrating practical progress.","key_markers":["robotics","environmental_monitoring"],"dimensions":{"agency":8,"progress":8,"collective_benefit":7,"connection":3,"innovation":9,"justice":2,"resilience":6,"wonder":5},"overall_uplift_score":6.77,"tier":"connection","analyzed_at":"2025-10-28T18:58:54.595697Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_cbd5886383fe","title":"Chain-of","content":"arXiv:2510.18434v1 Announce Type: new Abstract: Chain-of-Thought (CoT) is widely applied to improve the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks since there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose another prompt-based paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a concept, then generates the detailed content. The chain of concepts is allowed within the utterance, encouraging the LLM's deep and strategic thinking. We experiment with this paradigm in daily and emotional support conversations where the concept is comprised of emotions, strategies and topics. Automatic, human and model evaluations suggest that CoCT surpasses baselines such as Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective prompt-based paradigm of LLM for a wider scope of tasks.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18434","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.201754","language":"en","tags":["cscl","research","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":136,"author":"Qingqing Gu, Dan Wang, Yue Zhao, Xiaoyu Wang, Zhonglin Jiang, Yong Chen, Hongyan Li, Luo Ji","raw_content_length":938,"priority":7,"update_frequency":1,"reading_time_minutes":0.68,"robust_parsing_used":true,"entities":{"organizations":["CoT","LLM","ToT","CoCT","Chain of Conceptual Thought"],"persons":["Self-Refine","RAG","SoT"],"locations":["Automatic"],"monetary":[]},"char_count":937,"language_detected":"en","key_concepts":{"key_phrases":["Chain","arXiv251018434v1 Announce Type","new Abstract","Thought","CoT","the LLM capability","math","coding and reasoning tasks","its performance","open-domain tasks"],"filter_categories":{"ai_ml":["Chain"]},"extraction_method":"spacy","confidence":"high","concept_scores":{"Chain":4.0,"arXiv251018434v1 Announce Type":1.0,"new Abstract":1.0,"Thought":1.0,"CoT":1.0,"the LLM capability":1.0,"math":1.0,"coding and reasoning tasks":1.0,"its performance":1.0,"open-domain tasks":1.0}},"age_hours":9.396677653333334,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"a06740c4e3baeef0ef1b7f978d95a2d9","title_md5":"9b0bcbb7bacb4cd6f16ad5e13fa1273c","url_normalized":"84e5cd030277af3c34205e497b4af721","minhash_signature":["23332185","25550307","4229889","13560711","25080903","19122126","4626937","187877","2753670","11743625","5968903","7332789","7556069","1166689","953500","14833280","11186338","23825809","4032655","253028","1196901","1457998","1386815","12858916","16595181","14494170","4070773","19091918","9145689","8720670","522500","1613769","25190055","17326044","10644205","2786783","5971886","6836305","4255907","20376078","6154556","2268139","41920148","1785966","12507014","5244114","907311","31283371","1132398","6774644","190238","843096","6725716","43544270","136855","5146736","7015836","6045037","4430687","6066206","10735391","9541652","6578137","2492803","7601","4770883","9932183","4355909","7564697","4625726","7249981","13189136","13012937","3985712","15858453","2595826","10343841","3254368","13987518","22105907","10619331","2330191","7379873","20005884","1207127","2319043","4602115","2033222","2983177","8318141","7521029","2175767","19499623","436779","12406048","5560495","1608076","14235446","24302950","6563146","2610545","3028635","5119106","5292934","1405939","3344019","2193094","4666992","12007630","11984598","10234849","7086138","4549477","76161","23567886","28227271","926978","7842654","1592955","12207048","1131087","4372651","1129826","31710634","9144084","11374462","17858731","5198470"],"title_minhash":["597520670","626516001","52838539","168778675","471324265","793489020","72299679","768541219","1712883204","521185072","2565812410","788784001","7556069","2218838287","1115964629","188975992","141890963","59593206","481128677","873560918","56931098","353521038","590696885","884218225","1665475239","868948741","725256448","19091918","20504899","1751223461","523441405","551714696","726332255","24716973","2200208372","383011603","573138414","1254160531","700088925","555736712","306335124","472483499","1668135136","1501833505","478950246","625998876","903076795","359202091","82140570","1060642731","888677285","17274241","737581692","1006657628","98644559","305944868","108217729","45401407","257263041","380585655","786259478","993601598","12323739","682520525","474971476","1652171038","798155467","1010528589","223063916","707089931","192686709","230451352","433973024","211284407","621273250","271212559","1500599198","784829741","120282712","502000347","165626056","1052376998","908712645","64966267","1207127","33356504","1137550889","455578055","135233826","577931428","663254959","864283802","893717738","503529609","989101803","1077005618","27992026","2145248671","659045699","196819349","537770455","94137638","539123607","589851618","1272748497","1062994659","513767204","35679801","684061551","40079976","557515254","126903015","305075043","15851685","74182526","2109174099","393562854","1939010337","637883423","1551127358","2774861650","126312746","1129826","678022535","1761932272","45457698","341502562","1003734751"],"combined_hash":"5de1d8b4738da4b007b8886e33afd116"},"sentiment_score":5.053999999999999,"sentiment_category":"neutral","sentiment_confidence":"low","sentiment_method":"vader","sentiment_raw_score":0.0108,"is_positive":false,"is_negative":false,"is_neutral":true,"raw_emotions":{"neutral":0.9381,"joy":0.0074,"surprise":0.0191,"sadness":0.0061,"fear":0.0161,"anger":0.0087,"disgust":0.0045},"emotion_method":"local"},"uplifting_analysis":{"content_type":"solutions_story","agency":8,"progress":8,"collective_benefit":7,"connection":3,"innovation":9,"justice":2,"resilience":5,"wonder":6,"reasoning":"The article describes a new AI prompting method (CoCT) that improves LLM performance in tasks like daily and emotional support conversations. This represents progress in making AI more helpful and effective for a wider range of human needs. The innovation lies in the CoCT paradigm itself, which encourages deeper and more strategic thinking in LLMs.","key_markers":["AI assistance","emotional support"],"dimensions":{"agency":8,"progress":8,"collective_benefit":7,"connection":3,"innovation":9,"justice":2,"resilience":5,"wonder":6},"overall_uplift_score":6.8,"tier":"connection","analyzed_at":"2025-10-28T18:59:05.076540Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
{"id":"science_arxiv_cs_24338f73047b","title":"DePass: Unified Feature Attributing by Simple Decomposed Forward Pass","content":"arXiv:2510.18462v1 Announce Type: new Abstract: Attributing the behavior of Transformer models to internal computations is a central challenge in mechanistic interpretability. We introduce DePass, a unified framework for feature attribution based on a single decomposed forward pass. DePass decomposes hidden states into customized additive components, then propagates them with attention scores and MLP's activations fixed. It achieves faithful, fine-grained attribution without requiring auxiliary training. We validate DePass across token-level, model component-level, and subspace-level attribution tasks, demonstrating its effectiveness and fidelity. Our experiments highlight its potential to attribute information flow between arbitrary components of a Transformer model. We hope DePass serves as a foundational tool for broader applications in interpretability.","source":"science_arxiv_cs","source_type":"rss","url":"https://arxiv.org/abs/2510.18462","published_date":"2025-10-22T04:00:00","collected_date":"2025-10-22T12:55:56.207964","language":"en","tags":["cscl","research","preprints","computer-science","science"],"metadata":{"feed_title":"cs updates on arXiv.org","source_category":"science","word_count":109,"author":"Xiangyu Hong, Che Jiang, Kai Tian, Biqing Qi, Youbang Sun, Ning Ding, Bowen Zhou","raw_content_length":870,"priority":7,"update_frequency":1,"reading_time_minutes":0.545,"robust_parsing_used":true,"entities":{"organizations":["DePass","Simple Decomposed Forward Pass arXiv:2510.18462v1 Announce Type","Transformer","MLP"],"persons":[],"locations":[],"monetary":[]},"char_count":869,"language_detected":"en","key_concepts":{"key_phrases":["DePass"," Unified Feature","Simple Decomposed Forward Pass","arXiv251018462v1 Announce Type","new Abstract","the behavior","Transformer models","internal computations","a central challenge","mechanistic interpretability"],"filter_categories":{},"extraction_method":"spacy","confidence":"high","concept_scores":{"DePass":3.0," Unified Feature":2.0,"Simple Decomposed Forward Pass":2.0,"arXiv251018462v1 Announce Type":1.0,"new Abstract":1.0,"the behavior":1.0,"Transformer models":1.0,"internal computations":1.0,"a central challenge":1.0,"mechanistic interpretability":1.0}},"age_hours":9.39696542638889,"is_recent":true,"quality_score":1.0,"hashes":{"content_md5":"3dab9bc3f267ae910726f052ec5dc12e","title_md5":"3f03f679b2ba0234a8237733b8b59f48","url_normalized":"aa7b0011e818d23bf20b47414a549f56","minhash_signature":["19678434","33092728","14366","13560711","6697520","19580689","4380964","13756142","2753670","10835656","5968903","3207698","9587540","1166689","953500","22205146","2138553","4282388","4032655","8708371","1196901","1457998","433252","2113985","16595181","13340951","4070773","19091918","14898113","6873188","12590646","6606854","35901395","19786918","1090859","16665534","1313735","6836305","309467","11762562","3082913","3329444","34073408","1785966","7966961","10151165","9217577","4152219","1132398","16941830","19771076","843096","5647154","7663755","14522170","1909511","226173","7750562","4430687","1151625","11267373","7144066","14326557","2381879","7601","7689844","9932183","3779347","17016683","4625726","5786666","827165","13012937","4528274","4991972","2595826","17362876","4232416","8466869","721726","301782","4072191","7379873","21936519","442394","2319043","4602115","9222483","5524105","8318141","7521029","2175767","19499623","6172193","12406048","10440400","631649","479007","32777169","5834464","5622147","23615957","12264772","5883140","1405939","3344019","1707315","145406","10621501","44001","22507947","120729","2086119","6829739","9652365","13776844","873230","13762129","4846284","21280434","1131087","4372651","952385","1331506","14744595","1376388","15147703","5198470"],"title_minhash":["42893312","51458411","99326028","11617828","204368645","75198439","50234708","17862952","11890316","178613326","77151084","3207698","64882359","39130036","13500562","173418530","11518608","152669875","2213100","160229303","26274608","130873670","232374037","60516281","44720605","102857668","7695165","34131197","29576415","244041689","16281514","120273654","84460271","28392040","1090859","16665534","99151114","158640387","182160355","101238272","362372719","39707560","336276329","196607237","7966961","10151165","9217577","46767638","1132398","89121664","69544403","64939931","69647037","49755262","107176645","21321771","226173","28402388","60335376","18531727","100899178","219700561","46629561","257372575","73765893","18039960","112578877","81643396","249278223","33413450","65114329","112472421","19937992","123371117","66182437","83014494","24516636","14914829","176502829","24325924","21139769","82838450","172950305","167979996","15871323","108331809","21303204","9222483","31750667","38811632","96376865","76711016","23401976","6172193","160690533","314699740","11304047","74418657","141772291","82379273","20165302","79870237","22173503","44451757","31064882","222805587","1707315","145406","166427302","39525534","66286913","66685843","244830053","118880462","214279416","48800787","22942695","112026425","87410233","20025063","7805551","67476139","20674921","57589434","22709825","119506485","17858731","212847652"],"combined_hash":"cbba82f22dccf6df926022917c536a4a"},"sentiment_score":8.352500000000001,"sentiment_category":"positive","sentiment_confidence":"high","sentiment_method":"vader","sentiment_raw_score":0.6705,"is_positive":true,"is_negative":false,"is_neutral":false,"raw_emotions":{"neutral":0.947,"joy":0.0096,"surprise":0.0143,"sadness":0.0026,"fear":0.0048,"anger":0.0119,"disgust":0.0099},"emotion_method":"local"},"uplifting_analysis":{"content_type":"solutions_story","agency":8,"progress":9,"collective_benefit":8,"connection":6,"innovation":9,"justice":7,"resilience":5,"wonder":7,"reasoning":"The article describes a new method, DePass, for understanding how AI models work, specifically Transformer models. This method allows for attributing the behavior of these models to internal computations, which can lead to better understanding and potentially more reliable and beneficial AI systems. The method is validated across multiple tasks, demonstrating its effectiveness and potential for broader applications in interpretability.","key_markers":["interpretability","AI understanding"],"dimensions":{"agency":8,"progress":9,"collective_benefit":8,"connection":6,"innovation":9,"justice":7,"resilience":5,"wonder":7},"overall_uplift_score":7.92,"tier":"impact","analyzed_at":"2025-10-28T18:59:06.669369Z","analyzed_by":"gemini-flash-api-batch","filter_name":"uplifting"}}
